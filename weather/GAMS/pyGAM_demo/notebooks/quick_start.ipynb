{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This quick start will show how to do the following:\n",
    "\n",
    "- `Install` everything needed to use pyGAM.\n",
    "- `fit a regression model` with custom terms\n",
    "- search for the `best smoothing parameters`\n",
    "- plot `partial dependence` functions\n",
    "\n",
    "\n",
    "## Install pyGAM\n",
    "#### Pip\n",
    "\n",
    "    pip install pygam\n",
    "\n",
    "\n",
    "#### Conda\n",
    "pyGAM is on conda-forge, however this is typically less up-to-date:\n",
    "\n",
    "    conda install -c conda-forge pygam\n",
    "    \n",
    "\n",
    "#### Bleeding edge\n",
    "You can install the bleeding edge from github using `poetry`.\n",
    "First clone the repo, ``cd`` into the main directory and do:\n",
    "\n",
    "    pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get `pandas` and `matplotlib`\n",
    "\n",
    "    pip install pandas matplotlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Model\n",
    "\n",
    "Let's get to it. First we need some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam.datasets import wage\n",
    "\n",
    "X, y = wage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import a GAM that's made for regression problems.\n",
    "\n",
    "Let's fit a spline term to the first 2 features, and a factor term to the 3rd feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpygam\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearGAM, f, s\n\u001b[0;32m----> 3\u001b[0m gam \u001b[38;5;241m=\u001b[39m \u001b[43mLinearGAM\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/pygam/pygam.py:913\u001b[0m, in \u001b[0;36mGAM.fit\u001b[0;34m(self, X, y, weights)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatistics_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# optimize\u001b[39;00m\n\u001b[0;32m--> 913\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pirls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# if self._opt == 0:\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m#     self._pirls(X, y, weights)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;66;03m# if self._opt == 1:\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m#     self._pirls_naive(X, y)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/pygam/pygam.py:751\u001b[0m, in \u001b[0;36mGAM._pirls\u001b[0;34m(self, X, Y, weights)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# S += self._H # add any user-chosen minumum penalty to the diagonal\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# if we dont have any constraints, then do cholesky now\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterms\u001b[38;5;241m.\u001b[39mhasconstraint:\n\u001b[0;32m--> 751\u001b[0m     E \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m min_n_m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin([m, n])\n\u001b[1;32m    754\u001b[0m Dinv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((min_n_m \u001b[38;5;241m+\u001b[39m m, m))\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/pygam/pygam.py:518\u001b[0m, in \u001b[0;36mGAM._cholesky\u001b[0;34m(self, A, **kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m constraint_l2 \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint_l2_max:\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m         L \u001b[38;5;241m=\u001b[39m \u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint_l2 \u001b[38;5;241m=\u001b[39m constraint_l2\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m L\n",
      "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/pygam/utils.py:81\u001b[0m, in \u001b[0;36mcholesky\u001b[0;34m(A, sparse, verbose)\u001b[0m\n\u001b[1;32m     78\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39missparse(A):\n\u001b[0;32m---> 81\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     L \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky(A, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'A'"
     ]
    }
   ],
   "source": [
    "from pygam import LinearGAM, f, s\n",
    "\n",
    "gam = LinearGAM(s(0) + s(1) + f(2)).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the model fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have 3 terms with a total of `(20 + 20 + 5) = 45` free variables, the default smoothing penalty (`lam=0.6`) reduces the effective degrees of freedom to just ~25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the spline terms, `s(...)`, use 20 basis functions. This is a good starting point. The rule of thumb is to use a fairly large amount of flexibility, and then let the smoothing penalty regularize the model.\n",
    "\n",
    "However, we can always use our expert knowledge to add flexibility where it is needed, or remove basis functions, and make fitting easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam = LinearGAM(s(0, n_splines=5) + s(1) + f(2)).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, spline terms, `s()` have a penalty on their 2nd derivative, which encourages the functions to be smoother, while factor terms, `f()` and linear terms `l()`, have a l2, ie ridge penalty, which encourages them to take on smaller values.\n",
    "\n",
    "`lam`, short for $\\lambda$, controls the strength of the regularization penalty on each term. Terms can have multiple penalties, and therefore multiple `lam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gam.lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has 3 `lam` parameters, currently just one per term.\n",
    "\n",
    "Let's perform a grid-search over multiple `lam` values to see if we can improve our model.  \n",
    "We will seek the model with the lowest generalized cross-validation (GCV) score.\n",
    "\n",
    "Our search space is 3-dimensional, so we have to be conservative with the number of points we consider per dimension.\n",
    "\n",
    "Let's try 5 values for each smoothing parameter, resulting in a total of `5*5*5 = 125` points in our grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lam = np.logspace(-3, 5, 5)\n",
    "lams = [lam] * 3\n",
    "\n",
    "gam.gridsearch(X, y, lam=lams)\n",
    "gam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a bit better. Even though the in-sample $R^2$ value is lower, we can expect our model to generalize better because the GCV error is lower.\n",
    "\n",
    "We could be more rigorous by using a train/test split, and checking our model's error on the test set. We were also quite lazy and only tried 125 values in our hyperopt. We might find a better model if we spent more time searching across more points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high-dimensional search-spaces, it is sometimes a good idea to try a **randomized search**.  \n",
    "We can achieve this by using numpy's `random` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = np.random.rand(100, 3)  # random points on [0, 1], with shape (100, 3)\n",
    "lams = lams * 6 - 3  # shift values to -3, 3\n",
    "lams = 10**lams  # transforms values to 1e-3, 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_gam = LinearGAM(s(0) + s(1) + f(2)).gridsearch(X, y, lam=lams)\n",
    "random_gam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, our deterministic search found a better model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam.statistics_[\"GCV\"] < random_gam.statistics_[\"GCV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statistics_` attribute is populated after the model has been fitted.\n",
    "There are lots of interesting model statistics to check out, although many are automatically reported in the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gam.statistics_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependence Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most attractive properties of GAMs is that we can decompose and inspect the contribution of each feature to the overall prediction. \n",
    "\n",
    "This is done via **partial dependence** functions.\n",
    "\n",
    "Let's plot the partial dependence for each term in our model, along with a 95% confidence interval for the estimated function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, term in enumerate(gam.terms):\n",
    "    if term.isintercept:\n",
    "        continue\n",
    "\n",
    "    XX = gam.generate_X_grid(term=i)\n",
    "    pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(XX[:, term.feature], pdep)\n",
    "    plt.plot(XX[:, term.feature], confi, c=\"r\", ls=\"--\")\n",
    "    plt.title(repr(term))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we skip the intercept term because it has nothing interesting to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
