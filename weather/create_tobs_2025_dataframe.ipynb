{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create TOBS 2025 DataFrame\n",
        "\n",
        "This notebook creates a wide-format dataframe for TOBS (Temperature at observation time) data for 2025, where:\n",
        "- Each row represents a station\n",
        "- Columns include station ID, year, and 365 columns for each day of the year\n",
        "- Values are TOBS measurements (NaN if not defined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import s3fs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('✓ Libraries imported successfully!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to S3\n",
        "print('Connecting to S3...')\n",
        "s3 = s3fs.S3FileSystem(anon=True)\n",
        "bucket_name = 'noaa-ghcn-pds'\n",
        "print(f'✓ Connected to S3 bucket: {bucket_name}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read TOBS data for 2025\n",
        "tobs_path = f's3://{bucket_name}/parquet/by_year/YEAR=2025/ELEMENT=TOBS/'\n",
        "print(f'Reading TOBS data for 2025 from: {tobs_path}')\n",
        "\n",
        "# Read all parquet files for TOBS 2025\n",
        "df_long = pd.read_parquet(tobs_path, storage_options={'anon': True})\n",
        "\n",
        "print(f'\\nData loaded:')\n",
        "print(f'  - Shape: {df_long.shape[0]:,} rows × {df_long.shape[1]} columns')\n",
        "print(f'  - Unique stations: {df_long[\"ID\"].nunique():,}')\n",
        "print(f'  - Date range: {df_long[\"DATE\"].min()} to {df_long[\"DATE\"].max()}')\n",
        "print(f'\\nFirst few rows:')\n",
        "print(df_long.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert DATE column to datetime and extract day of year\n",
        "df_long['date_dt'] = pd.to_datetime(df_long['DATE'], format='%Y%m%d')\n",
        "df_long['day_of_year'] = df_long['date_dt'].dt.dayofyear\n",
        "df_long['year'] = df_long['date_dt'].dt.year\n",
        "\n",
        "# Check if 2025 is a leap year (it's not)\n",
        "days_in_2025 = 365\n",
        "print(f'Days in 2025: {days_in_2025}')\n",
        "print(f'Day of year range: {df_long[\"day_of_year\"].min()} to {df_long[\"day_of_year\"].max()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep only necessary columns and rename for clarity\n",
        "df_pivot = df_long[['ID', 'year', 'day_of_year', 'DATA_VALUE']].copy()\n",
        "\n",
        "# Pivot the data: stations as rows, days as columns\n",
        "print('Pivoting data to wide format...')\n",
        "df_wide = df_pivot.pivot_table(\n",
        "    index=['ID', 'year'],\n",
        "    columns='day_of_year',\n",
        "    values='DATA_VALUE',\n",
        "    aggfunc='first'  # Use first value if multiple observations per day\n",
        ")\n",
        "\n",
        "# Reset index to make ID and year regular columns\n",
        "df_wide = df_wide.reset_index()\n",
        "\n",
        "print(f'\\nWide format dataframe created:')\n",
        "print(f'  - Shape: {df_wide.shape[0]:,} rows × {df_wide.shape[1]} columns')\n",
        "print(f'  - Columns: station_id, year, day_1, day_2, ..., day_{days_in_2025}')\n",
        "print(f'\\nFirst few rows and columns:')\n",
        "print(df_wide.iloc[:5, :10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rename columns to be more descriptive\n",
        "# Ensure all days 1-365 exist (add missing columns with NaN)\n",
        "for day in range(1, days_in_2025 + 1):\n",
        "    if day not in df_wide.columns:\n",
        "        df_wide[day] = np.nan\n",
        "\n",
        "# Sort columns: ID, year, then days 1-365 in order\n",
        "day_columns = sorted([col for col in df_wide.columns if isinstance(col, int)])\n",
        "df_wide = df_wide[['ID', 'year'] + day_columns]\n",
        "\n",
        "# Rename columns\n",
        "df_wide.columns = ['station_id', 'year'] + [f'day_{i}' for i in range(1, days_in_2025 + 1)]\n",
        "\n",
        "print(f'\\n✓ Final dataframe structure:')\n",
        "print(f'  - Shape: {df_wide.shape[0]:,} rows × {df_wide.shape[1]} columns')\n",
        "print(f'  - Columns: {list(df_wide.columns[:5])} ... {list(df_wide.columns[-3:])}')\n",
        "print(f'\\nFirst 3 rows, first 8 columns:')\n",
        "print(df_wide.iloc[:3, :8])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display data quality statistics\n",
        "print('DATA QUALITY STATISTICS')\n",
        "print('='*80)\n",
        "\n",
        "# Calculate completeness per station\n",
        "day_cols = [col for col in df_wide.columns if col.startswith('day_')]\n",
        "df_wide['completeness'] = df_wide[day_cols].notna().sum(axis=1) / len(day_cols) * 100\n",
        "\n",
        "print(f'\\nTotal stations: {len(df_wide):,}')\n",
        "print(f'\\nCompleteness statistics (% of days with data):')\n",
        "print(f'  - Mean: {df_wide[\"completeness\"].mean():.1f}%')\n",
        "print(f'  - Median: {df_wide[\"completeness\"].median():.1f}%')\n",
        "print(f'  - Min: {df_wide[\"completeness\"].min():.1f}%')\n",
        "print(f'  - Max: {df_wide[\"completeness\"].max():.1f}%')\n",
        "\n",
        "# Show distribution of completeness\n",
        "print(f'\\nCompleteness distribution:')\n",
        "print(f'  - 100% complete: {(df_wide[\"completeness\"] == 100).sum():,} stations')\n",
        "print(f'  - >= 90% complete: {(df_wide[\"completeness\"] >= 90).sum():,} stations')\n",
        "print(f'  - >= 50% complete: {(df_wide[\"completeness\"] >= 50).sum():,} stations')\n",
        "print(f'  - < 50% complete: {(df_wide[\"completeness\"] < 50).sum():,} stations')\n",
        "\n",
        "# Remove the completeness column from final output\n",
        "df_final = df_wide.drop('completeness', axis=1)\n",
        "\n",
        "print(f'\\n✓ Final dataframe ready: {df_final.shape[0]:,} rows × {df_final.shape[1]} columns')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample of the final dataframe\n",
        "print('SAMPLE OF FINAL DATAFRAME')\n",
        "print('='*80)\n",
        "print('\\nFirst 5 stations, first 10 columns:')\n",
        "print(df_final.iloc[:5, :10])\n",
        "\n",
        "print('\\nFirst 5 stations, last 10 columns:')\n",
        "print(df_final.iloc[:5, -10:])\n",
        "\n",
        "# Show a station with good coverage\n",
        "good_station_idx = df_wide.sort_values('completeness', ascending=False).index[0]\n",
        "print(f'\\nExample of well-covered station: {df_wide.loc[good_station_idx, \"station_id\"]}')\n",
        "print(f'Completeness: {df_wide.loc[good_station_idx, \"completeness\"]:.1f}%')\n",
        "print('First 15 days of data:')\n",
        "print(df_final.loc[good_station_idx, ['station_id', 'year'] + [f'day_{i}' for i in range(1, 16)]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to Parquet and CSV\n",
        "import os\n",
        "\n",
        "output_file = 'tobs_2025_wide.parquet'\n",
        "\n",
        "print(f'Saving dataframe to {output_file}...')\n",
        "df_final.to_parquet(output_file, index=False, compression='snappy')\n",
        "print(f'✓ Saved to {output_file}')\n",
        "\n",
        "# Also save a CSV sample (first 1000 rows) for easy viewing\n",
        "csv_sample = 'tobs_2025_wide_sample.csv'\n",
        "df_final.head(1000).to_csv(csv_sample, index=False)\n",
        "print(f'✓ Saved sample (first 1000 rows) to {csv_sample}')\n",
        "\n",
        "print(f'\\nFile sizes:')\n",
        "print(f'  - {output_file}: {os.path.getsize(output_file) / (1024*1024):.2f} MB')\n",
        "print(f'  - {csv_sample}: {os.path.getsize(csv_sample) / (1024*1024):.2f} MB')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "The final dataframe `df_final` contains:\n",
        "- **Columns**: `station_id`, `year`, `day_1`, `day_2`, ..., `day_365`\n",
        "- **Rows**: One per station with TOBS data in 2025\n",
        "- **Values**: TOBS temperature observations (in tenths of degrees C)\n",
        "- **Missing data**: Represented as NaN\n",
        "\n",
        "### Notes:\n",
        "- TOBS values are in tenths of degrees Celsius (divide by 10 to get °C)\n",
        "- 2025 is not a leap year, so it has 365 days\n",
        "- Some stations may have incomplete data coverage\n",
        "- The dataframe is saved as both Parquet (full data) and CSV (sample)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
