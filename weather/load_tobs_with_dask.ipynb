{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load TOBS 2025 Data with Dask\n",
        "\n",
        "This notebook demonstrates how to load and work with the TOBS 2025 wide-format parquet file using Dask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Libraries imported\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print('âœ“ Libraries imported')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 1: Basic Loading with Dask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dask DataFrame Info:\n",
            "  Type: <class 'dask.dataframe.core.DataFrame'>\n",
            "  Columns: 367 (['station_id', 'year', 'day_1', 'day_2', 'day_3'] ... ['day_363', 'day_364', 'day_365'])\n",
            "  Number of partitions: 1\n",
            "  Partitions: [Delayed(('read-parquet-90f11a70ecde0657d83abd445984c36f', 0))]\n",
            "\n",
            "Dataframe structure (lazy):\n",
            "Dask DataFrame Structure:\n",
            "              station_id   year    day_1    day_2    day_3    day_4    day_5    day_6    day_7    day_8    day_9   day_10   day_11   day_12   day_13   day_14   day_15   day_16   day_17   day_18   day_19   day_20   day_21   day_22   day_23   day_24   day_25   day_26   day_27   day_28   day_29   day_30   day_31   day_32   day_33   day_34   day_35   day_36   day_37   day_38   day_39   day_40   day_41   day_42   day_43   day_44   day_45   day_46   day_47   day_48   day_49   day_50   day_51   day_52   day_53   day_54   day_55   day_56   day_57   day_58   day_59   day_60   day_61   day_62   day_63   day_64   day_65   day_66   day_67   day_68   day_69   day_70   day_71   day_72   day_73   day_74   day_75   day_76   day_77   day_78   day_79   day_80   day_81   day_82   day_83   day_84   day_85   day_86   day_87   day_88   day_89   day_90   day_91   day_92   day_93   day_94   day_95   day_96   day_97   day_98   day_99  day_100  day_101  day_102  day_103  day_104  day_105  day_106  day_107  day_108  day_109  day_110  day_111  day_112  day_113  day_114  day_115  day_116  day_117  day_118  day_119  day_120  day_121  day_122  day_123  day_124  day_125  day_126  day_127  day_128  day_129  day_130  day_131  day_132  day_133  day_134  day_135  day_136  day_137  day_138  day_139  day_140  day_141  day_142  day_143  day_144  day_145  day_146  day_147  day_148  day_149  day_150  day_151  day_152  day_153  day_154  day_155  day_156  day_157  day_158  day_159  day_160  day_161  day_162  day_163  day_164  day_165  day_166  day_167  day_168  day_169  day_170  day_171  day_172  day_173  day_174  day_175  day_176  day_177  day_178  day_179  day_180  day_181  day_182  day_183  day_184  day_185  day_186  day_187  day_188  day_189  day_190  day_191  day_192  day_193  day_194  day_195  day_196  day_197  day_198  day_199  day_200  day_201  day_202  day_203  day_204  day_205  day_206  day_207  day_208  day_209  day_210  day_211  day_212  day_213  day_214  day_215  day_216  day_217  day_218  day_219  day_220  day_221  day_222  day_223  day_224  day_225  day_226  day_227  day_228  day_229  day_230  day_231  day_232  day_233  day_234  day_235  day_236  day_237  day_238  day_239  day_240  day_241  day_242  day_243  day_244  day_245  day_246  day_247  day_248  day_249  day_250  day_251  day_252  day_253  day_254  day_255  day_256  day_257  day_258  day_259  day_260  day_261  day_262  day_263  day_264  day_265  day_266  day_267  day_268  day_269  day_270  day_271  day_272  day_273  day_274  day_275  day_276  day_277  day_278  day_279  day_280  day_281  day_282  day_283  day_284  day_285  day_286  day_287  day_288  day_289  day_290  day_291  day_292  day_293  day_294  day_295  day_296  day_297  day_298  day_299  day_300  day_301  day_302  day_303  day_304  day_305  day_306  day_307  day_308  day_309  day_310  day_311  day_312  day_313  day_314  day_315  day_316  day_317  day_318  day_319  day_320  day_321  day_322  day_323  day_324  day_325  day_326  day_327  day_328  day_329  day_330  day_331  day_332  day_333  day_334  day_335  day_336  day_337  day_338  day_339  day_340  day_341  day_342  day_343  day_344  day_345  day_346  day_347  day_348  day_349  day_350  day_351  day_352  day_353  day_354  day_355  day_356  day_357  day_358  day_359  day_360  day_361  day_362  day_363  day_364  day_365\n",
            "npartitions=1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
            "                  object  int64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64  float64\n",
            "                     ...    ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
            "Dask Name: read-parquet, 1 graph layer\n"
          ]
        }
      ],
      "source": [
        "# Read the parquet file with Dask\n",
        "df_dask = dd.read_parquet('tobs_2025_wide.parquet')\n",
        "\n",
        "# Dask operations are lazy - this doesn't load the data yet\n",
        "print('Dask DataFrame Info:')\n",
        "print(f'  Type: {type(df_dask)}')\n",
        "print(f'  Columns: {len(df_dask.columns)} ({list(df_dask.columns[:5])} ... {list(df_dask.columns[-3:])})')\n",
        "print(f'  Number of partitions: {df_dask.npartitions}')\n",
        "print(f'  Partitions: {df_dask.to_delayed()}')\n",
        "\n",
        "# To see the structure without computing\n",
        "print(f'\\nDataframe structure (lazy):')\n",
        "print(df_dask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing Data (Triggers Computation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows, first 10 columns:\n",
            "    station_id  year  day_1  day_2  day_3  day_4  day_5  day_6  day_7  day_8\n",
            "0  CQC00914801  2025  250.0  244.0  250.0  250.0  256.0  250.0  256.0  244.0\n",
            "1  FMC00914325  2025    NaN  294.0  283.0  283.0  256.0  294.0  294.0  272.0\n",
            "2  FMC00914395  2025  300.0  278.0  306.0  294.0  283.0  311.0  300.0  306.0\n",
            "3  FMC00914590  2025  211.0  250.0  300.0  228.0  289.0  294.0  278.0  239.0\n",
            "4  FMC00914720  2025    NaN  283.0  289.0  278.0  289.0  289.0  283.0  289.0\n",
            "\n",
            "\n",
            "Basic statistics:\n",
            "Number of rows (computed): 4706\n",
            "Unique stations: 4706\n"
          ]
        }
      ],
      "source": [
        "# View first few rows (only loads necessary partitions)\n",
        "print('First 5 rows, first 10 columns:')\n",
        "first_10_cols = list(df_dask.columns[:10])\n",
        "print(df_dask[first_10_cols].head())\n",
        "\n",
        "print('\\n\\nBasic statistics:')\n",
        "print(f'Number of rows (computed): {len(df_dask)}')\n",
        "print(f'Unique stations: {df_dask[\"station_id\"].nunique().compute()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dask Operations (Lazy Evaluation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of day columns: 365\n",
            "\n",
            "Mean computation (lazy): Dask Series Structure:\n",
            "npartitions=1\n",
            "day_1    float64\n",
            "day_9        ...\n",
            "dtype: float64\n",
            "Dask Name: dataframe-mean, 7 graph layers\n",
            "\n",
            "Mean temperatures for first 10 days (in tenths of Â°C):\n",
            "  day_1: -11.4 (= -1.1Â°C)\n",
            "  day_2: -16.6 (= -1.7Â°C)\n",
            "  day_3: -22.9 (= -2.3Â°C)\n",
            "  day_4: -42.4 (= -4.2Â°C)\n",
            "  day_5: -40.4 (= -4.0Â°C)\n",
            "  day_6: -50.5 (= -5.1Â°C)\n",
            "  day_7: -62.8 (= -6.3Â°C)\n",
            "  day_8: -62.0 (= -6.2Â°C)\n",
            "  day_9: -59.0 (= -5.9Â°C)\n",
            "  day_10: -37.5 (= -3.8Â°C)\n"
          ]
        }
      ],
      "source": [
        "# Example: Calculate mean temperature for each day across all stations\n",
        "# This is lazy - builds the computation graph but doesn't execute yet\n",
        "\n",
        "day_cols = [col for col in df_dask.columns if col.startswith('day_')]\n",
        "print(f'Number of day columns: {len(day_cols)}')\n",
        "\n",
        "# Calculate mean for first 10 days (lazy)\n",
        "means_lazy = df_dask[day_cols[:10]].mean()\n",
        "print(f'\\nMean computation (lazy): {means_lazy}')\n",
        "\n",
        "# Now compute the result\n",
        "means = means_lazy.compute()\n",
        "print(f'\\nMean temperatures for first 10 days (in tenths of Â°C):')\n",
        "for col, val in means.items():\n",
        "    print(f'  {col}: {val:.1f} (= {val/10:.1f}Â°C)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with Specific Stations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total stations: 4706\n",
            "First 10 stations: ['CQC00914801', 'FMC00914325', 'FMC00914395', 'FMC00914590', 'FMC00914720', 'GQW00041406', 'RQC00660053', 'RQC00660061', 'RQC00660152', 'RQC00660158']\n",
            "\n",
            "Data for station CQC00914801:\n",
            "    station_id  year  day_1  day_2  day_3  day_4  day_5  day_6  day_7  day_8\n",
            "0  CQC00914801  2025  250.0  244.0  250.0  250.0  256.0  250.0  256.0  244.0\n"
          ]
        }
      ],
      "source": [
        "# Filter for specific stations\n",
        "# Get all station IDs first\n",
        "all_stations = df_dask['station_id'].compute()\n",
        "print(f'Total stations: {len(all_stations)}')\n",
        "print(f'First 10 stations: {all_stations.head(10).tolist()}')\n",
        "\n",
        "# Filter for a specific station (lazy)\n",
        "first_station = all_stations.iloc[0]\n",
        "station_filter = df_dask['station_id'] == first_station\n",
        "single_station = df_dask[station_filter]\n",
        "\n",
        "# Compute to get the data\n",
        "station_data = single_station.compute()\n",
        "print(f'\\nData for station {first_station}:')\n",
        "# Now we can use iloc on the pandas dataframe\n",
        "print(station_data.iloc[:, :10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting to Pandas (if data fits in memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted to Pandas DataFrame:\n",
            "  Type: <class 'pandas.core.frame.DataFrame'>\n",
            "  Shape: (4706, 367)\n",
            "  Memory usage: 13.45 MB\n",
            "\n",
            "First 3 rows, first 8 columns:\n",
            "    station_id  year  day_1  day_2  day_3  day_4  day_5  day_6\n",
            "0  CQC00914801  2025  250.0  244.0  250.0  250.0  256.0  250.0\n",
            "1  FMC00914325  2025    NaN  294.0  283.0  283.0  256.0  294.0\n",
            "2  FMC00914395  2025  300.0  278.0  306.0  294.0  283.0  311.0\n"
          ]
        }
      ],
      "source": [
        "# Since this dataset is relatively small (~1.7 MB), we can load it all into pandas\n",
        "df_pandas = df_dask.compute()\n",
        "\n",
        "print(f'Converted to Pandas DataFrame:')\n",
        "print(f'  Type: {type(df_pandas)}')\n",
        "print(f'  Shape: {df_pandas.shape}')\n",
        "print(f'  Memory usage: {df_pandas.memory_usage(deep=True).sum() / (1024**2):.2f} MB')\n",
        "\n",
        "print(f'\\nFirst 3 rows, first 8 columns:')\n",
        "# Now we can use iloc since it's pandas\n",
        "print(df_pandas.iloc[:3, :8])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Differences: Dask vs Pandas\n",
        "\n",
        "| Operation | Pandas | Dask |\n",
        "|-----------|--------|------|\n",
        "| **Reading** | `pd.read_parquet()` | `dd.read_parquet()` |\n",
        "| **Execution** | Immediate (eager) | Lazy (builds computation graph) |\n",
        "| **Compute** | Automatic | Call `.compute()` to execute |\n",
        "| **Memory** | Loads entire dataset | Can work with data larger than memory |\n",
        "| **Partitions** | Single dataframe | Multiple partitions |\n",
        "\n",
        "### Common Dask Operations\n",
        "\n",
        "```python\n",
        "# Reading\n",
        "df = dd.read_parquet('file.parquet')\n",
        "\n",
        "# Viewing (triggers computation)\n",
        "df.head()           # First few rows\n",
        "df.tail()           # Last few rows\n",
        "df.compute()        # Convert to pandas (full dataset)\n",
        "\n",
        "# Lazy operations (no computation yet)\n",
        "df.mean()           # Calculate means\n",
        "df[df['col'] > 10]  # Filter data\n",
        "df.groupby('col').mean()  # Group and aggregate\n",
        "\n",
        "# Execute computation\n",
        "result = df.mean().compute()  # Now it actually runs\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting TOBS Data for Multiple Stations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "print('âœ“ Plotting libraries imported')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a few stations with good data coverage\n",
        "# First, let's find stations with the most complete data\n",
        "day_cols = [col for col in df_pandas.columns if col.startswith('day_')]\n",
        "df_pandas['completeness'] = df_pandas[day_cols].notna().sum(axis=1)\n",
        "\n",
        "# Get top 5 stations with most complete data\n",
        "top_stations = df_pandas.nlargest(5, 'completeness')\n",
        "print('Top 5 stations by data completeness:')\n",
        "print(top_stations[['station_id', 'completeness']])\n",
        "print(f'\\nPlotting data for these stations...')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot TOBS for the top 5 stations\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "days = list(range(1, 366))  # Days 1-365\n",
        "\n",
        "for idx, row in top_stations.iterrows():\n",
        "    station_id = row['station_id']\n",
        "    # Extract temperature values for all days (convert from tenths to degrees C)\n",
        "    temps = [row[f'day_{d}'] / 10 if pd.notna(row[f'day_{d}']) else np.nan for d in days]\n",
        "    \n",
        "    # Plot with gaps where data is missing\n",
        "    ax.plot(days, temps, marker='o', markersize=2, linewidth=1.5, label=station_id, alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Day of Year (2025)', fontsize=12)\n",
        "ax.set_ylabel('Temperature (Â°C)', fontsize=12)\n",
        "ax.set_title('TOBS (Temperature at Observation Time) for Top 5 Stations in 2025', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(1, 365)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('tobs_top5_stations.png', dpi=150, bbox_inches='tight')\n",
        "print('âœ“ Plot saved as tobs_top5_stations.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot for Specific Regions\n",
        "\n",
        "Let's also look at stations from different geographic regions to see variation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select stations from different regions based on station ID prefixes\n",
        "# US stations start with 'US', Canada with 'CA', etc.\n",
        "regions = {\n",
        "    'US': df_pandas[df_pandas['station_id'].str.startswith('US')],\n",
        "    'CA': df_pandas[df_pandas['station_id'].str.startswith('CA')],\n",
        "    'GM': df_pandas[df_pandas['station_id'].str.startswith('GM')],  # Germany\n",
        "    'JA': df_pandas[df_pandas['station_id'].str.startswith('JA')],  # Japan\n",
        "    'AS': df_pandas[df_pandas['station_id'].str.startswith('AS')],  # Australia\n",
        "}\n",
        "\n",
        "print('Stations by region:')\n",
        "selected_stations = []\n",
        "for region, stations_df in regions.items():\n",
        "    if len(stations_df) > 0:\n",
        "        # Get the station with best coverage from this region\n",
        "        best = stations_df.nlargest(1, 'completeness')\n",
        "        if len(best) > 0:\n",
        "            station_id = best.iloc[0]['station_id']\n",
        "            completeness = best.iloc[0]['completeness']\n",
        "            selected_stations.append(station_id)\n",
        "            print(f'  {region}: {station_id} ({completeness:.0f} days)')\n",
        "\n",
        "print(f'\\nTotal selected: {len(selected_stations)} stations')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot TOBS for stations from different regions\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "days = list(range(1, 366))\n",
        "colors = plt.cm.tab10(range(len(selected_stations)))\n",
        "\n",
        "for i, station_id in enumerate(selected_stations):\n",
        "    station_row = df_pandas[df_pandas['station_id'] == station_id].iloc[0]\n",
        "    \n",
        "    # Extract temperature values (convert from tenths to degrees C)\n",
        "    temps = [station_row[f'day_{d}'] / 10 if pd.notna(station_row[f'day_{d}']) else np.nan for d in days]\n",
        "    \n",
        "    ax.plot(days, temps, marker='o', markersize=2, linewidth=1.5, \n",
        "            label=f'{station_id}', alpha=0.8, color=colors[i])\n",
        "\n",
        "ax.set_xlabel('Day of Year (2025)', fontsize=12)\n",
        "ax.set_ylabel('Temperature (Â°C)', fontsize=12)\n",
        "ax.set_title('TOBS Comparison: Stations from Different Regions', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(1, 365)\n",
        "\n",
        "# Add month labels\n",
        "month_starts = [1, 32, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
        "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "ax.set_xticks(month_starts)\n",
        "ax.set_xticklabels(month_names)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('tobs_regional_comparison.png', dpi=150, bbox_inches='tight')\n",
        "print('âœ“ Plot saved as tobs_regional_comparison.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Individual Station Plots\n",
        "\n",
        "Create separate subplots for better visibility of individual patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a grid of subplots\n",
        "n_stations = min(6, len(selected_stations))\n",
        "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "days = list(range(1, 366))\n",
        "month_starts = [1, 32, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
        "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "\n",
        "for i in range(n_stations):\n",
        "    if i < len(selected_stations):\n",
        "        station_id = selected_stations[i]\n",
        "        station_row = df_pandas[df_pandas['station_id'] == station_id].iloc[0]\n",
        "        \n",
        "        # Extract temperature values (use np.nan instead of None)\n",
        "        temps = np.array([station_row[f'day_{d}'] / 10 if pd.notna(station_row[f'day_{d}']) else np.nan for d in days])\n",
        "        \n",
        "        # Calculate completeness\n",
        "        valid_temps = temps[~np.isnan(temps)]\n",
        "        completeness = len(valid_temps) / 365 * 100\n",
        "        \n",
        "        # Plot\n",
        "        axes[i].plot(days, temps, color='steelblue', linewidth=1.5, alpha=0.8)\n",
        "        axes[i].fill_between(days, temps, alpha=0.3, color='steelblue')\n",
        "        axes[i].set_title(f'{station_id}\\n({len(valid_temps)}/365 days, {completeness:.1f}% complete)', \n",
        "                         fontsize=11, fontweight='bold')\n",
        "        axes[i].set_xlabel('Day of Year')\n",
        "        axes[i].set_ylabel('Temperature (Â°C)')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "        axes[i].set_xlim(1, 365)\n",
        "        axes[i].set_xticks(month_starts)\n",
        "        axes[i].set_xticklabels(month_names, rotation=45)\n",
        "    else:\n",
        "        # Hide unused subplots\n",
        "        axes[i].set_visible(False)\n",
        "\n",
        "plt.suptitle('TOBS by Station - 2025', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('tobs_individual_stations.png', dpi=150, bbox_inches='tight')\n",
        "print('âœ“ Plot saved as tobs_individual_stations.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary Statistics\n",
        "\n",
        "Show overall temperature patterns across all stations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate daily statistics across all stations\n",
        "day_cols = [f'day_{d}' for d in range(1, 366)]\n",
        "daily_stats = df_pandas[day_cols].describe().T\n",
        "\n",
        "# Convert to degrees C\n",
        "daily_stats_celsius = daily_stats / 10\n",
        "\n",
        "# Extract statistics\n",
        "days = list(range(1, 366))\n",
        "mean_temps = daily_stats_celsius['mean'].values\n",
        "min_temps = daily_stats_celsius['min'].values\n",
        "max_temps = daily_stats_celsius['max'].values\n",
        "q25_temps = daily_stats_celsius['25%'].values\n",
        "q75_temps = daily_stats_celsius['75%'].values\n",
        "\n",
        "# Create plot\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Plot ranges and means\n",
        "ax.fill_between(days, min_temps, max_temps, alpha=0.15, color='gray', label='Min-Max Range')\n",
        "ax.fill_between(days, q25_temps, q75_temps, alpha=0.3, color='steelblue', label='25th-75th Percentile')\n",
        "ax.plot(days, mean_temps, color='darkred', linewidth=2.5, label='Mean', alpha=0.9)\n",
        "\n",
        "ax.set_xlabel('Day of Year (2025)', fontsize=12)\n",
        "ax.set_ylabel('Temperature (Â°C)', fontsize=12)\n",
        "ax.set_title('Daily Temperature Statistics Across All Stations - 2025', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(1, 365)\n",
        "\n",
        "# Add month labels\n",
        "month_starts = [1, 32, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
        "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "ax.set_xticks(month_starts)\n",
        "ax.set_xticklabels(month_names)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('tobs_daily_statistics.png', dpi=150, bbox_inches='tight')\n",
        "print('âœ“ Plot saved as tobs_daily_statistics.png')\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nðŸ“Š Overall Statistics:')\n",
        "print(f'   Mean temperature: {mean_temps[~np.isnan(mean_temps)].mean():.1f}Â°C')\n",
        "print(f'   Overall min: {np.nanmin(min_temps):.1f}Â°C')\n",
        "print(f'   Overall max: {np.nanmax(max_temps):.1f}Â°C')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dask-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
