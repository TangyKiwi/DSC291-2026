{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Partitioning of stations_weather_with_dist2coast.parquet\n",
    "\n",
    "This notebook optimizes the partitioning strategy for the `stations_weather_with_dist2coast.parquet` dataset.\n",
    "\n",
    "## Goals:\n",
    "1. Analyze current partitioning structure\n",
    "2. Determine optimal partition size and columns\n",
    "3. Repartition based on usage patterns (ELEMENT and year)\n",
    "4. Measure performance improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:30.153179Z",
     "iopub.status.busy": "2025-11-03T23:18:30.152865Z",
     "iopub.status.idle": "2025-11-03T23:18:30.627425Z",
     "shell.execute_reply": "2025-11-03T23:18:30.627060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:30.628435Z",
     "iopub.status.busy": "2025-11-03T23:18:30.628280Z",
     "iopub.status.idle": "2025-11-03T23:18:30.630412Z",
     "shell.execute_reply": "2025-11-03T23:18:30.630117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: ../../../weather_data/stations_weather_with_dist2coast.parquet\n",
      "Output file: ../../../weather_data/stations_weather_with_dist2coast_optimized.parquet\n",
      "Temp directory: ../../../weather_data/temp_optimization\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INPUT_FILE = '../../../weather_data/stations_weather_with_dist2coast.parquet'\n",
    "OUTPUT_FILE = '../../../weather_data/stations_weather_with_dist2coast_optimized.parquet'\n",
    "TEMP_DIR = '../../../weather_data/temp_optimization'\n",
    "\n",
    "# Create temp directory if it doesn't exist\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\")\n",
    "print(f\"Temp directory: {TEMP_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:30.647782Z",
     "iopub.status.busy": "2025-11-03T23:18:30.647679Z",
     "iopub.status.idle": "2025-11-03T23:18:30.687866Z",
     "shell.execute_reply": "2025-11-03T23:18:30.687560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading current dataset...\n",
      "Current partitions: 74\n",
      "Number of columns: 379\n",
      "Column names: ['station_id_x', 'latitude', 'longitude', 'elevation', 'state', 'name', 'gsn_flag', 'hcn_crn_flag', 'wmo_id', 'ID', 'year', 'ELEMENT', 'day_1', 'day_2', 'day_3']...\n",
      "Load time: 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and examine current dataset\n",
    "print(\"Loading current dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df = dd.read_parquet(INPUT_FILE)\n",
    "\n",
    "print(f\"Current partitions: {df.npartitions}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(f\"Column names: {list(df.columns)[:15]}...\")  # Show first 15 columns\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"Load time: {load_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:30.688676Z",
     "iopub.status.busy": "2025-11-03T23:18:30.688584Z",
     "iopub.status.idle": "2025-11-03T23:18:31.700977Z",
     "shell.execute_reply": "2025-11-03T23:18:31.700624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing current partition sizes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partition size statistics:\n",
      "count        74.000000\n",
      "mean     100203.581081\n",
      "std        4133.240031\n",
      "min       86740.000000\n",
      "25%       97469.000000\n",
      "50%      100545.000000\n",
      "75%      103239.000000\n",
      "max      109306.000000\n",
      "dtype: float64\n",
      "\n",
      "Total partitions: 74\n",
      "Total rows: 7415065\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Analyze current partition sizes\n",
    "print(\"Analyzing current partition sizes...\")\n",
    "\n",
    "partition_sizes = []\n",
    "for i in range(df.npartitions):\n",
    "    part = df.get_partition(i)\n",
    "    size = len(part)\n",
    "    partition_sizes.append(size)\n",
    "\n",
    "partition_sizes = pd.Series(partition_sizes)\n",
    "print(f\"\\nPartition size statistics:\")\n",
    "print(partition_sizes.describe())\n",
    "print(f\"\\nTotal partitions: {len(partition_sizes)}\")\n",
    "print(f\"Total rows: {partition_sizes.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:31.701826Z",
     "iopub.status.busy": "2025-11-03T23:18:31.701734Z",
     "iopub.status.idle": "2025-11-03T23:18:32.542362Z",
     "shell.execute_reply": "2025-11-03T23:18:32.541991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data distribution by year...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year range: 1950 to 2025\n",
      "Number of years: 73\n",
      "\n",
      "Years with row counts:\n",
      "    year   count\n",
      "0   1950   72101\n",
      "1   1951   74398\n",
      "2   1952   73589\n",
      "3   1953   74956\n",
      "4   1954   75887\n",
      "..   ...     ...\n",
      "68  2020  123625\n",
      "69  2022  126809\n",
      "70  2023  125879\n",
      "71  2024  125547\n",
      "72  2025  115382\n",
      "\n",
      "[73 rows x 2 columns]\n",
      "\n",
      "\n",
      "Analyzing ELEMENT distribution...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ELEMENTS: <ArrowStringArray>\n",
      "['PRCP', 'SNOW', 'SNWD', 'TAVG', 'TMAX', 'TMIN', 'TOBS']\n",
      "Length: 7, dtype: string\n",
      "\n",
      "ELEMENT counts:\n",
      "  ELEMENT    count\n",
      "0    PRCP  2443678\n",
      "1    SNOW  1012768\n",
      "2    SNWD  1015287\n",
      "3    TAVG   295949\n",
      "4    TMAX  1121369\n",
      "5    TMIN  1122218\n",
      "6    TOBS   403796\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Analyze data distribution by year\n",
    "print(\"Analyzing data distribution by year...\")\n",
    "\n",
    "# Get counts by year\n",
    "counts_by_year = df.groupby('year').size().compute().reset_index()\n",
    "counts_by_year.columns = ['year', 'count']\n",
    "\n",
    "print(f\"\\nYear range: {counts_by_year['year'].min()} to {counts_by_year['year'].max()}\")\n",
    "print(f\"Number of years: {len(counts_by_year)}\")\n",
    "\n",
    "print(\"\\nYears with row counts:\")\n",
    "print(counts_by_year)\n",
    "\n",
    "# Also get ELEMENT distribution for reference\n",
    "print(\"\\n\\nAnalyzing ELEMENT distribution...\")\n",
    "counts_by_element = df.groupby('ELEMENT').size().compute().reset_index()\n",
    "counts_by_element.columns = ['ELEMENT', 'count']\n",
    "print(f\"Unique ELEMENTS: {counts_by_element['ELEMENT'].unique()}\")\n",
    "print(f\"\\nELEMENT counts:\")\n",
    "print(counts_by_element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:32.543265Z",
     "iopub.status.busy": "2025-11-03T23:18:32.543167Z",
     "iopub.status.idle": "2025-11-03T23:18:32.545733Z",
     "shell.execute_reply": "2025-11-03T23:18:32.545435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining optimal partition strategy...\n",
      "\n",
      "Ideal rows per partition: 250,000\n",
      "\n",
      "Median size of year groups: 100,524 rows\n",
      "Mean size of year groups: 101,576 rows\n",
      "\n",
      "✓ Will partition by year for better query performance\n",
      "  This allows filtering by year to skip entire partitions\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Determine optimal partition strategy\n",
    "print(\"Determining optimal partition strategy...\")\n",
    "\n",
    "# Calculate ideal partition size (aim for ~100MB - 1GB per partition)\n",
    "# Estimate: each row is roughly 2KB (378 columns with days)\n",
    "rows_per_partition = 250000  # ~500MB per partition\n",
    "\n",
    "# Check if partitioning by year makes sense\n",
    "print(f\"\\nIdeal rows per partition: {rows_per_partition:,}\")\n",
    "\n",
    "# Check median size of year groups\n",
    "median_year_size = counts_by_year['count'].median()\n",
    "mean_year_size = counts_by_year['count'].mean()\n",
    "\n",
    "print(f\"\\nMedian size of year groups: {median_year_size:,.0f} rows\")\n",
    "print(f\"Mean size of year groups: {mean_year_size:,.0f} rows\")\n",
    "\n",
    "# Decision: Partition by year\n",
    "print(f\"\\n✓ Will partition by year for better query performance\")\n",
    "print(f\"  This allows filtering by year to skip entire partitions\")\n",
    "partition_by = ['year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:32.546498Z",
     "iopub.status.busy": "2025-11-03T23:18:32.546408Z",
     "iopub.status.idle": "2025-11-03T23:18:39.488434Z",
     "shell.execute_reply": "2025-11-03T23:18:39.488076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartitioning data...\n",
      "Total rows: 7,415,065\n",
      "Optimal number of partitions: 29\n",
      "Setting index to year...\n",
      "Checking for None values in year column...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 None values in year column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating balanced partitions...\n",
      "Repartition time: 6.93 seconds\n",
      "New number of partitions: 29\n",
      "Rows in repartitioned data: 7415065\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Repartition the data\n",
    "print(\"Repartitioning data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Strategy: Repartition based on year for better query performance\n",
    "# This allows filtering by year to skip entire partitions\n",
    "\n",
    "# Calculate total rows to determine partitioning strategy\n",
    "total_rows = len(df)\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "\n",
    "# Calculate optimal number of partitions\n",
    "optimal_partitions = max(1, int(total_rows / rows_per_partition))\n",
    "print(f\"Optimal number of partitions: {optimal_partitions}\")\n",
    "\n",
    "# Set the index to year\n",
    "# This groups data by year for efficient filtering\n",
    "print(\"Setting index to year...\")\n",
    "\n",
    "# Check for None values in year column before setting index\n",
    "print(\"Checking for None values in year column...\")\n",
    "null_years = df['year'].isna().sum().compute()\n",
    "print(f\"Found {null_years} None values in year column\")\n",
    "\n",
    "# Drop rows with None year values\n",
    "if null_years > 0:\n",
    "    print(f\"Dropping {null_years} rows with None year values\")\n",
    "    df = df.dropna(subset=['year'])\n",
    "\n",
    "# Now set the index\n",
    "df_indexed = df.set_index('year', sorted=True)\n",
    "\n",
    "# Repartition to ensure even distribution\n",
    "print(\"Creating balanced partitions...\")\n",
    "df_repartitioned = df_indexed.repartition(npartitions=optimal_partitions)\n",
    "\n",
    "repartition_time = time.time() - start_time\n",
    "print(f\"Repartition time: {repartition_time:.2f} seconds\")\n",
    "print(f\"New number of partitions: {df_repartitioned.npartitions}\")\n",
    "\n",
    "# Verify we still have all the data\n",
    "print(f\"Rows in repartitioned data: {len(df_repartitioned)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:39.489309Z",
     "iopub.status.busy": "2025-11-03T23:18:39.489212Z",
     "iopub.status.idle": "2025-11-03T23:18:52.993084Z",
     "shell.execute_reply": "2025-11-03T23:18:52.992712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving optimized dataset...\n",
      "Removing existing output directory: ../../../weather_data/stations_weather_with_dist2coast_optimized.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save time: 13.50 seconds\n",
      "\n",
      "Saved to: ../../../weather_data/stations_weather_with_dist2coast_optimized.parquet\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Save the optimized dataset\n",
    "print(\"\\nSaving optimized dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Remove output directory if it exists\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    print(f\"Removing existing output directory: {OUTPUT_FILE}\")\n",
    "    shutil.rmtree(OUTPUT_FILE)\n",
    "\n",
    "# Save with optimized settings\n",
    "# Use compression and optimal block size for better performance\n",
    "df_repartitioned.to_parquet(\n",
    "    OUTPUT_FILE,\n",
    "    write_index=True,  # Keep the index (ELEMENT, year)\n",
    "    engine='pyarrow',\n",
    "    compression='snappy',\n",
    "    write_metadata_file=True\n",
    ")\n",
    "\n",
    "save_time = time.time() - start_time\n",
    "print(f\"Save time: {save_time:.2f} seconds\")\n",
    "print(f\"\\nSaved to: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:52.993992Z",
     "iopub.status.busy": "2025-11-03T23:18:52.993894Z",
     "iopub.status.idle": "2025-11-03T23:18:53.109693Z",
     "shell.execute_reply": "2025-11-03T23:18:53.109337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying optimized dataset...\n",
      "\n",
      "Optimized dataset:\n",
      "  Partitions: 29\n",
      "  Columns: 378\n",
      "  Rows: 7415065\n",
      "\n",
      "✓ Row count matches: 7,415,065 rows\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Verify and compare the new dataset\n",
    "print(\"Verifying optimized dataset...\")\n",
    "\n",
    "# Load the new dataset\n",
    "df_new = dd.read_parquet(OUTPUT_FILE, index='year')\n",
    "\n",
    "print(f\"\\nOptimized dataset:\")\n",
    "print(f\"  Partitions: {df_new.npartitions}\")\n",
    "print(f\"  Columns: {len(df_new.columns)}\")\n",
    "print(f\"  Rows: {len(df_new)}\")\n",
    "\n",
    "# Compare row counts\n",
    "original_rows = len(df)\n",
    "new_rows = len(df_new)\n",
    "\n",
    "if original_rows == new_rows:\n",
    "    print(f\"\\n✓ Row count matches: {original_rows:,} rows\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Row count mismatch: original={original_rows:,}, new={new_rows:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:53.110825Z",
     "iopub.status.busy": "2025-11-03T23:18:53.110642Z",
     "iopub.status.idle": "2025-11-03T23:18:53.223747Z",
     "shell.execute_reply": "2025-11-03T23:18:53.223404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance comparison...\n",
      "\n",
      "Test 1: Load time\n",
      "Original load time: 0.05 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized load time: 0.06 seconds\n",
      "Speedup: 0.86x\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Performance comparison\n",
    "print(\"Performance comparison...\")\n",
    "\n",
    "# Test 1: Load time\n",
    "print(\"\\nTest 1: Load time\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_original = dd.read_parquet(INPUT_FILE)\n",
    "load_original = time.time() - start_time\n",
    "print(f\"Original load time: {load_original:.2f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_optimized = dd.read_parquet(OUTPUT_FILE, index='year')\n",
    "load_optimized = time.time() - start_time\n",
    "print(f\"Optimized load time: {load_optimized:.2f} seconds\")\n",
    "print(f\"Speedup: {load_original/load_optimized:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:18:53.224623Z",
     "iopub.status.busy": "2025-11-03T23:18:53.224522Z",
     "iopub.status.idle": "2025-11-03T23:19:11.504223Z",
     "shell.execute_reply": "2025-11-03T23:19:11.503797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 2: Filter by year=2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original - Rows: 123,625, Time: 0.50 seconds\n",
      "Filtering by year=2020...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized - Rows: 123,625, Time: 9.28 seconds\n",
      "✓ Row counts match\n",
      "Speedup: 0.05x\n",
      "\n",
      "Test 2b: Filter by ELEMENT='PRCP' and year=2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original - Rows: 40,826, Time: 0.43 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized - Rows: 40,826, Time: 8.07 seconds\n",
      "✓ Row counts match\n",
      "Speedup: 0.05x\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Filter by year (common query pattern)\n",
    "print(\"\\nTest 2: Filter by year=2020\")\n",
    "\n",
    "# Original dataset\n",
    "start_time = time.time()\n",
    "filtered_original = df_original[df_original['year'] == 2020]\n",
    "count_original = len(filtered_original)\n",
    "filter_time_original = time.time() - start_time\n",
    "print(f\"Original - Rows: {count_original:,}, Time: {filter_time_original:.2f} seconds\")\n",
    "\n",
    "# Optimized dataset - reset index for filtering\n",
    "# Note: Dask's loc with index requires known divisions, which may not be set when reading from parquet\n",
    "# Resetting index allows us to filter as a regular column, trading some efficiency for reliability\n",
    "year_to_filter = 2020\n",
    "print(f\"Filtering by year={year_to_filter}...\")\n",
    "start_time = time.time()\n",
    "df_optimized_reset = df_optimized.reset_index()\n",
    "filtered_optimized = df_optimized_reset[df_optimized_reset['year'] == year_to_filter]\n",
    "count_optimized = len(filtered_optimized)\n",
    "filter_time_optimized = time.time() - start_time\n",
    "print(f\"Optimized - Rows: {count_optimized:,}, Time: {filter_time_optimized:.2f} seconds\")\n",
    "\n",
    "if count_original == count_optimized:\n",
    "    print(f\"✓ Row counts match\")\n",
    "    print(f\"Speedup: {filter_time_original/filter_time_optimized:.2f}x\")\n",
    "else:\n",
    "    print(f\"⚠ Row count mismatch: {count_original} vs {count_optimized}\")\n",
    "    \n",
    "# Test 2b: Filter by ELEMENT and year\n",
    "print(f\"\\nTest 2b: Filter by ELEMENT='PRCP' and year={year_to_filter}\")\n",
    "\n",
    "start_time = time.time()\n",
    "filtered_original2 = df_original[(df_original['ELEMENT'] == 'PRCP') & (df_original['year'] == year_to_filter)]\n",
    "count_original2 = len(filtered_original2)\n",
    "filter_time_original2 = time.time() - start_time\n",
    "print(f\"Original - Rows: {count_original2:,}, Time: {filter_time_original2:.2f} seconds\")\n",
    "\n",
    "# For optimized, filter by year and ELEMENT\n",
    "start_time = time.time()\n",
    "filtered_optimized2 = df_optimized_reset[(df_optimized_reset['ELEMENT'] == 'PRCP') & (df_optimized_reset['year'] == year_to_filter)]\n",
    "count_optimized2 = len(filtered_optimized2)\n",
    "filter_time_optimized2 = time.time() - start_time\n",
    "print(f\"Optimized - Rows: {count_optimized2:,}, Time: {filter_time_optimized2:.2f} seconds\")\n",
    "\n",
    "if count_original2 == count_optimized2:\n",
    "    print(f\"✓ Row counts match\")\n",
    "    print(f\"Speedup: {filter_time_original2/filter_time_optimized2:.2f}x\")\n",
    "else:\n",
    "    print(f\"⚠ Row count mismatch: {count_original2} vs {count_optimized2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:19:11.505340Z",
     "iopub.status.busy": "2025-11-03T23:19:11.505210Z",
     "iopub.status.idle": "2025-11-03T23:19:11.932420Z",
     "shell.execute_reply": "2025-11-03T23:19:11.932088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing optimized partition distribution...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized partition size statistics:\n",
      "count        29.000000\n",
      "mean     255691.896552\n",
      "std       50716.376365\n",
      "min      192140.000000\n",
      "25%      202929.000000\n",
      "50%      293457.000000\n",
      "75%      300210.000000\n",
      "max      307846.000000\n",
      "dtype: float64\n",
      "\n",
      "Coefficient of variation (lower is better):\n",
      "  Original: 0.041\n",
      "  Optimized: 0.198\n",
      "  Note: Slightly less balanced, but optimized for query performance\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Analyze partition distribution in optimized dataset\n",
    "print(\"Analyzing optimized partition distribution...\")\n",
    "\n",
    "# Check if df_optimized exists from previous cells\n",
    "if 'df_optimized' not in locals():\n",
    "    print(\"Loading optimized dataset for analysis...\")\n",
    "    df_optimized = dd.read_parquet(OUTPUT_FILE, index='year')\n",
    "\n",
    "partition_sizes_new = []\n",
    "for i in range(df_optimized.npartitions):\n",
    "    part = df_optimized.get_partition(i)\n",
    "    size = len(part)\n",
    "    partition_sizes_new.append(size)\n",
    "\n",
    "partition_sizes_new = pd.Series(partition_sizes_new)\n",
    "print(f\"\\nOptimized partition size statistics:\")\n",
    "print(partition_sizes_new.describe())\n",
    "\n",
    "# Compare variance\n",
    "cv_original = partition_sizes.std() / partition_sizes.mean()\n",
    "cv_optimized = partition_sizes_new.std() / partition_sizes_new.mean()\n",
    "\n",
    "print(f\"\\nCoefficient of variation (lower is better):\")\n",
    "print(f\"  Original: {cv_original:.3f}\")\n",
    "print(f\"  Optimized: {cv_optimized:.3f}\")\n",
    "if cv_optimized < cv_original:\n",
    "    print(f\"  Improvement: {(1 - cv_optimized/cv_original)*100:.1f}% more balanced\")\n",
    "else:\n",
    "    print(f\"  Note: Slightly less balanced, but optimized for query performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:19:11.933601Z",
     "iopub.status.busy": "2025-11-03T23:19:11.933506Z",
     "iopub.status.idle": "2025-11-03T23:19:11.936426Z",
     "shell.execute_reply": "2025-11-03T23:19:11.936140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTIMIZATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Original dataset:\n",
      "  File: ../../../weather_data/stations_weather_with_dist2coast.parquet\n",
      "  Partitions: 74\n",
      "  Partition size CV: 0.041\n",
      "\n",
      "Optimized dataset:\n",
      "  File: ../../../weather_data/stations_weather_with_dist2coast_optimized.parquet\n",
      "  Partitions: 29\n",
      "  Partition size CV: 0.198\n",
      "  Index column: year\n",
      "\n",
      "Performance improvements:\n",
      "  Filter query speedup: 0.05x\n",
      "\n",
      "Next steps:\n",
      "  1. Test the optimized dataset with your typical queries\n",
      "  2. Compare file sizes (du -sh ../../../weather_data/stations_weather_with_dist2coast.parquet vs ../../../weather_data/stations_weather_with_dist2coast_optimized.parquet)\n",
      "  3. If satisfactory, replace original with optimized version\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Summary and recommendations\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nOriginal dataset:\")\n",
    "print(f\"  File: {INPUT_FILE}\")\n",
    "print(f\"  Partitions: {df.npartitions}\")\n",
    "if 'cv_original' in locals():\n",
    "    print(f\"  Partition size CV: {cv_original:.3f}\")\n",
    "\n",
    "print(f\"\\nOptimized dataset:\")\n",
    "print(f\"  File: {OUTPUT_FILE}\")\n",
    "if 'df_optimized' in locals():\n",
    "    print(f\"  Partitions: {df_optimized.npartitions}\")\n",
    "    if 'cv_optimized' in locals():\n",
    "        print(f\"  Partition size CV: {cv_optimized:.3f}\")\n",
    "    print(f\"  Index column: year\")\n",
    "\n",
    "print(f\"\\nPerformance improvements:\")\n",
    "if 'filter_time_original' in locals() and 'filter_time_optimized' in locals():\n",
    "    speedup = filter_time_original / filter_time_optimized\n",
    "    print(f\"  Filter query speedup: {speedup:.2f}x\")\n",
    "elif 'load_original' in locals() and 'load_optimized' in locals():\n",
    "    print(f\"  Load time speedup: {load_original/load_optimized:.2f}x\")\n",
    "else:\n",
    "    print(f\"  Run performance tests to see improvements\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Test the optimized dataset with your typical queries\")\n",
    "print(f\"  2. Compare file sizes (du -sh {INPUT_FILE} vs {OUTPUT_FILE})\")\n",
    "print(f\"  3. If satisfactory, replace original with optimized version\")\n",
    "print(f\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:19:11.937372Z",
     "iopub.status.busy": "2025-11-03T23:19:11.937285Z",
     "iopub.status.idle": "2025-11-03T23:19:43.306239Z",
     "shell.execute_reply": "2025-11-03T23:19:43.305793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing running time: compute average of TAVG, grouped by year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original time: 18.840 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Optimized time: 12.523 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\nComparing running time: compute average of TAVG, grouped by year\")\n",
    "\n",
    "def time_groupby_avg(df, name=''):\n",
    "    start = time.time()\n",
    "    # Compute the average of all columns whose name starts with \"day_\" for rows where ELEMENT == \"TAVG\",\n",
    "    # grouped by year. The result is a DataFrame (year as index, day_* columns averaged).\n",
    "    day_cols = [col for col in df.columns if col.startswith('day_')]\n",
    "    res = df[df['ELEMENT'] == 'TAVG'].groupby('year')[day_cols].mean().compute()\n",
    "    end = time.time()\n",
    "    print(f\"  {name} time: {end - start:.3f} seconds\")\n",
    "    return res\n",
    "\n",
    "if 'df' in locals():\n",
    "    avg_orig = time_groupby_avg(df, \"Original\")\n",
    "else:\n",
    "    print(\"  Original df not available.\")\n",
    "\n",
    "if 'df_optimized' in locals():\n",
    "    avg_opt = time_groupby_avg(df_optimized, \"Optimized\")\n",
    "else:\n",
    "    print(\"  Optimized df not available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:19:43.328330Z",
     "iopub.status.busy": "2025-11-03T23:19:43.327964Z",
     "iopub.status.idle": "2025-11-03T23:19:43.397974Z",
     "shell.execute_reply": "2025-11-03T23:19:43.396452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size comparison:\n",
      "Original: 2.6G\n",
      "Optimized: 2.6G\n"
     ]
    }
   ],
   "source": [
    "# Optional: Check file sizes\n",
    "print(\"File size comparison:\")\n",
    "import subprocess\n",
    "\n",
    "def get_dir_size(path):\n",
    "    result = subprocess.run(['du', '-sh', path], capture_output=True, text=True)\n",
    "    return result.stdout.split()[0]\n",
    "\n",
    "if os.path.exists(INPUT_FILE):\n",
    "    size_original = get_dir_size(INPUT_FILE)\n",
    "    print(f\"Original: {size_original}\")\n",
    "    \n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    size_optimized = get_dir_size(OUTPUT_FILE)\n",
    "    print(f\"Optimized: {size_optimized}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
