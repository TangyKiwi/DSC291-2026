2025-10-19 17:17:54,011 - distributed.scheduler - INFO - State start
2025-10-19 17:17:54,013 - distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:45371
2025-10-19 17:17:54,013 - distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8790
2025-10-19 17:17:54,040 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46835'
2025-10-19 17:17:54,041 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33923'
Weather Data Multi-Year Processor
========================================
Processing years: 1950 to 2025
Data directory: ../../../weather_data/
Dask cluster: 20 workers, 4GB per worker
Started at: 2025-10-19 17:17:51

==================================================
Setting up Dask cluster
==================================================
Checking for existing Dask clusters...
No current client found
All existing clusters closed.
Setting up new cluster with 20 workers...
2025-10-19 17:17:54,042 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42495'
2025-10-19 17:17:54,043 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42487'
2025-10-19 17:17:54,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37553'
2025-10-19 17:17:54,045 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43919'
2025-10-19 17:17:54,046 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33941'
2025-10-19 17:17:54,047 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39025'
2025-10-19 17:17:54,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35029'
2025-10-19 17:17:54,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35677'
2025-10-19 17:17:54,051 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38765'
2025-10-19 17:17:54,053 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43159'
2025-10-19 17:17:54,055 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44605'
2025-10-19 17:17:54,056 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37019'
2025-10-19 17:17:54,057 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34679'
2025-10-19 17:17:54,059 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32833'
2025-10-19 17:17:54,060 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40519'
2025-10-19 17:17:54,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44899'
2025-10-19 17:17:54,064 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36485'
2025-10-19 17:17:54,065 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43475'
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34669 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41787 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37857 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39645 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41043 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38215 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39291 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34323 instead
  warnings.warn(
2025-10-19 17:17:54,738 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45899
2025-10-19 17:17:54,738 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45899
2025-10-19 17:17:54,738 - distributed.worker - INFO -           Worker name:                          0
2025-10-19 17:17:54,738 - distributed.worker - INFO -          dashboard at:             127.0.0.1:8791
2025-10-19 17:17:54,738 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,738 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,738 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,738 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,738 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-871x5uff
2025-10-19 17:17:54,738 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,738 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44229
2025-10-19 17:17:54,738 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44229
2025-10-19 17:17:54,738 - distributed.worker - INFO -           Worker name:                          5
2025-10-19 17:17:54,738 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34669
2025-10-19 17:17:54,738 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,738 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,738 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,738 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,738 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p6iuzwjw
2025-10-19 17:17:54,738 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,740 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45899', name: 0, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,742 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36919
2025-10-19 17:17:54,742 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36919
2025-10-19 17:17:54,742 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45899
2025-10-19 17:17:54,742 - distributed.worker - INFO -           Worker name:                          1
2025-10-19 17:17:54,742 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57864
2025-10-19 17:17:54,742 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41787
2025-10-19 17:17:54,742 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,742 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,742 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,742 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,742 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-smitvvy2
2025-10-19 17:17:54,742 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,742 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,742 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,743 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44229', name: 5, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,743 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44229
2025-10-19 17:17:54,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57880
2025-10-19 17:17:54,743 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,743 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,745 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36919', name: 1, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,745 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36919
2025-10-19 17:17:54,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57888
2025-10-19 17:17:54,745 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,745 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,746 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42425
2025-10-19 17:17:54,753 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42425
2025-10-19 17:17:54,753 - distributed.worker - INFO -           Worker name:                         11
2025-10-19 17:17:54,753 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39645
2025-10-19 17:17:54,753 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,753 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,753 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,753 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b9yudv7o
2025-10-19 17:17:54,753 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,756 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42425', name: 11, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,756 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42425
2025-10-19 17:17:54,756 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57900
2025-10-19 17:17:54,757 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,757 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38361 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39475 instead
  warnings.warn(
2025-10-19 17:17:54,797 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46073
2025-10-19 17:17:54,797 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46073
2025-10-19 17:17:54,797 - distributed.worker - INFO -           Worker name:                          6
2025-10-19 17:17:54,797 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37857
2025-10-19 17:17:54,797 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,797 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,798 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,798 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aadwhfl_
2025-10-19 17:17:54,798 - distributed.worker - INFO - -------------------------------------------------
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34275 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34073 instead
  warnings.warn(
2025-10-19 17:17:54,801 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46073', name: 6, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,801 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46073
2025-10-19 17:17:54,801 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57906
2025-10-19 17:17:54,801 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,801 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,802 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46619
2025-10-19 17:17:54,808 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46619
2025-10-19 17:17:54,808 - distributed.worker - INFO -           Worker name:                         19
2025-10-19 17:17:54,808 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41043
2025-10-19 17:17:54,808 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,808 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,808 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,808 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,808 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p4b_jkdh
2025-10-19 17:17:54,808 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,811 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46619', name: 19, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,812 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46619
2025-10-19 17:17:54,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57908
2025-10-19 17:17:54,812 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43321
2025-10-19 17:17:54,812 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43321
2025-10-19 17:17:54,812 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,812 - distributed.worker - INFO -           Worker name:                         15
2025-10-19 17:17:54,812 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,812 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38215
2025-10-19 17:17:54,812 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,812 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,812 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,812 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,812 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j5ca5hpa
2025-10-19 17:17:54,812 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,814 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43321', name: 15, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,814 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43321
2025-10-19 17:17:54,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57922
2025-10-19 17:17:54,814 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,815 - distributed.worker - INFO - -------------------------------------------------
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41121 instead
  warnings.warn(
2025-10-19 17:17:54,815 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45575 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41215 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44079 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41271 instead
  warnings.warn(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42409 instead
  warnings.warn(
2025-10-19 17:17:54,853 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46857
2025-10-19 17:17:54,853 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46857
2025-10-19 17:17:54,853 - distributed.worker - INFO -           Worker name:                          2
2025-10-19 17:17:54,853 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34323
2025-10-19 17:17:54,853 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,853 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,853 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,853 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,853 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iz4ty7m6
2025-10-19 17:17:54,853 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,856 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46857', name: 2, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,856 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46857
2025-10-19 17:17:54,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57936
2025-10-19 17:17:54,856 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,856 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40153 instead
  warnings.warn(
2025-10-19 17:17:54,888 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44957
2025-10-19 17:17:54,888 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44957
2025-10-19 17:17:54,888 - distributed.worker - INFO -           Worker name:                          4
2025-10-19 17:17:54,888 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38361
2025-10-19 17:17:54,888 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,888 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,888 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,888 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,888 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-clv29muu
2025-10-19 17:17:54,888 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,891 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44957', name: 4, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,891 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44957
2025-10-19 17:17:54,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57948
2025-10-19 17:17:54,891 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36035
2025-10-19 17:17:54,891 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36035
2025-10-19 17:17:54,891 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,891 - distributed.worker - INFO -           Worker name:                          3
2025-10-19 17:17:54,891 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,892 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39291
2025-10-19 17:17:54,892 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,892 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,892 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,892 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,892 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e8s6hn74
2025-10-19 17:17:54,892 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,894 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36035', name: 3, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,895 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36035
2025-10-19 17:17:54,895 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57950
2025-10-19 17:17:54,895 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,895 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,897 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32831
2025-10-19 17:17:54,897 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32831
2025-10-19 17:17:54,897 - distributed.worker - INFO -           Worker name:                         17
2025-10-19 17:17:54,897 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39475
2025-10-19 17:17:54,897 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,897 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,897 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,897 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,897 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h19l7bpk
2025-10-19 17:17:54,897 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,899 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32831', name: 17, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32831
2025-10-19 17:17:54,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57966
2025-10-19 17:17:54,900 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,900 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,922 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39495
2025-10-19 17:17:54,922 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39495
2025-10-19 17:17:54,922 - distributed.worker - INFO -           Worker name:                          8
2025-10-19 17:17:54,922 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34275
2025-10-19 17:17:54,922 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,922 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,922 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,922 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,922 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-le3cerjb
2025-10-19 17:17:54,922 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,925 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39495', name: 8, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,925 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39495
2025-10-19 17:17:54,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57980
2025-10-19 17:17:54,925 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,925 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,947 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34505
2025-10-19 17:17:54,947 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34505
2025-10-19 17:17:54,947 - distributed.worker - INFO -           Worker name:                         12
2025-10-19 17:17:54,947 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45575
2025-10-19 17:17:54,947 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,947 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,947 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,947 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,947 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-upjhszah
2025-10-19 17:17:54,947 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,950 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34505', name: 12, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,950 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34505
2025-10-19 17:17:54,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57994
2025-10-19 17:17:54,950 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,950 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,951 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,956 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45905
2025-10-19 17:17:54,956 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45905
2025-10-19 17:17:54,956 - distributed.worker - INFO -           Worker name:                          7
2025-10-19 17:17:54,956 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41121
2025-10-19 17:17:54,956 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,956 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,956 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,956 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,956 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gext_8ln
2025-10-19 17:17:54,956 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,958 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45905', name: 7, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,959 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45905
2025-10-19 17:17:54,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58006
2025-10-19 17:17:54,959 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,959 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44085
2025-10-19 17:17:54,959 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,959 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44085
2025-10-19 17:17:54,959 - distributed.worker - INFO -           Worker name:                         18
2025-10-19 17:17:54,959 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41271
2025-10-19 17:17:54,959 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,959 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,959 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,959 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,959 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y4okrfta
2025-10-19 17:17:54,959 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,959 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32829
2025-10-19 17:17:54,960 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32829
2025-10-19 17:17:54,960 - distributed.worker - INFO -           Worker name:                         16
2025-10-19 17:17:54,960 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44079
2025-10-19 17:17:54,960 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,960 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,960 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,960 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,960 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c45978ow
2025-10-19 17:17:54,960 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,961 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44085', name: 18, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,961 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44085
2025-10-19 17:17:54,961 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58014
2025-10-19 17:17:54,961 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,961 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,961 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32829', name: 16, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,962 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32829
2025-10-19 17:17:54,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58022
2025-10-19 17:17:54,962 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,962 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,962 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32989
2025-10-19 17:17:54,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,962 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32989
2025-10-19 17:17:54,962 - distributed.worker - INFO -           Worker name:                         13
2025-10-19 17:17:54,962 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34073
2025-10-19 17:17:54,962 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,962 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,962 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,962 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,962 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tdam7wmg
2025-10-19 17:17:54,962 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,964 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32989', name: 13, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,965 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32989
2025-10-19 17:17:54,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58034
2025-10-19 17:17:54,965 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,965 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,976 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46037
2025-10-19 17:17:54,977 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46037
2025-10-19 17:17:54,977 - distributed.worker - INFO -           Worker name:                         14
2025-10-19 17:17:54,977 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42409
2025-10-19 17:17:54,977 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,977 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,977 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,977 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,977 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0x7hb7b3
2025-10-19 17:17:54,977 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,979 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46037', name: 14, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,979 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46037
2025-10-19 17:17:54,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58046
2025-10-19 17:17:54,979 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,979 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,983 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38101
2025-10-19 17:17:54,983 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38101
2025-10-19 17:17:54,983 - distributed.worker - INFO -           Worker name:                          9
2025-10-19 17:17:54,983 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40153
2025-10-19 17:17:54,983 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,983 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,983 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,983 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,983 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oc3h7kjg
2025-10-19 17:17:54,983 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,985 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38101', name: 9, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,985 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38101
2025-10-19 17:17:54,985 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58052
2025-10-19 17:17:54,985 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,986 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,986 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:54,986 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35353
2025-10-19 17:17:54,986 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35353
2025-10-19 17:17:54,986 - distributed.worker - INFO -           Worker name:                         10
2025-10-19 17:17:54,986 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41215
2025-10-19 17:17:54,986 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,986 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,986 - distributed.worker - INFO -               Threads:                          1
2025-10-19 17:17:54,986 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 17:17:54,986 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1i4_t5xf
2025-10-19 17:17:54,986 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,988 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35353', name: 10, status: init, memory: 0, processing: 0>
2025-10-19 17:17:54,988 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35353
2025-10-19 17:17:54,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58058
2025-10-19 17:17:54,989 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 17:17:54,989 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 17:17:54,989 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 17:17:55,044 - distributed.scheduler - INFO - Receive client connection: Client-1413f9b9-ad31-11f0-bc67-5811224a71df
2025-10-19 17:17:55,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58074
2025-10-19 17:20:21,316 - distributed.utils_perf - INFO - full garbage collection released 43.44 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:20:54,076 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:21:34,519 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 138 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:23:30,941 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:23:43,840 - distributed.utils_perf - INFO - full garbage collection released 11.44 MiB from 109 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:27:13,178 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 122 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:27:48,109 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:27:52,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:28:10,347 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:28:15,904 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 103 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:30:12,705 - distributed.utils_perf - INFO - full garbage collection released 11.41 MiB from 100 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:31:12,939 - distributed.utils_perf - INFO - full garbage collection released 10.58 MiB from 95 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:32:26,850 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:36:10,966 - distributed.utils_perf - INFO - full garbage collection released 17.61 MiB from 98 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:36:24,122 - distributed.utils_perf - INFO - full garbage collection released 755.88 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:37:04,436 - distributed.utils_perf - INFO - full garbage collection released 756.65 MiB from 62 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:37:12,971 - distributed.utils_perf - INFO - full garbage collection released 12.98 MiB from 261 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:37:40,327 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:38:04,730 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:38:04,780 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:39:32,169 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:39:40,183 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:39:56,517 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.24 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:41:17,570 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:41:42,600 - distributed.utils_perf - INFO - full garbage collection released 9.99 MiB from 165 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:41:51,017 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:41:51,894 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:42:25,200 - distributed.utils_perf - INFO - full garbage collection released 278.87 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:42:28,395 - distributed.utils_perf - INFO - full garbage collection released 276.70 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:43:15,219 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:43:27,663 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:43:39,056 - distributed.utils_perf - INFO - full garbage collection released 286.23 MiB from 76 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:43:49,087 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.55 GiB -- Worker memory limit: 3.73 GiB
✓ Dask cluster created with 20 workers
✓ Dashboard available at: http://127.0.0.1:8790/status
✓ Total workers: 20
✓ Cores per worker: {'tcp://127.0.0.1:32829': 1, 'tcp://127.0.0.1:32831': 1, 'tcp://127.0.0.1:32989': 1, 'tcp://127.0.0.1:34505': 1, 'tcp://127.0.0.1:35353': 1, 'tcp://127.0.0.1:36035': 1, 'tcp://127.0.0.1:36919': 1, 'tcp://127.0.0.1:38101': 1, 'tcp://127.0.0.1:39495': 1, 'tcp://127.0.0.1:42425': 1, 'tcp://127.0.0.1:43321': 1, 'tcp://127.0.0.1:44085': 1, 'tcp://127.0.0.1:44229': 1, 'tcp://127.0.0.1:44957': 1, 'tcp://127.0.0.1:45899': 1, 'tcp://127.0.0.1:45905': 1, 'tcp://127.0.0.1:46037': 1, 'tcp://127.0.0.1:46073': 1, 'tcp://127.0.0.1:46619': 1, 'tcp://127.0.0.1:46857': 1}
✓ Total memory available: 80.0 GB
✓ Cluster test computation successful: 5000.58
Created directory: data

==================================================
Processing year 1950
==================================================
Loading weather data for year 1950...
Found 49 files for year 1950
Loaded data: 23,497,286 rows × 4 columns
Transforming to wide format...
Created wide format: 72,101 station-year-measurement combinations × 368 columns
Saved to data/weather_1950_wide.parquet

⏱️  Year 1950 processed in 73.2 seconds (1.2 minutes)

==================================================
Processing year 1951
==================================================
Loading weather data for year 1951...
Found 52 files for year 1951
Loaded data: 24,272,360 rows × 4 columns
Transforming to wide format...
Created wide format: 74,398 station-year-measurement combinations × 368 columns
Saved to data/weather_1951_wide.parquet

⏱️  Year 1951 processed in 81.2 seconds (1.4 minutes)

==================================================
Processing year 1952
==================================================
Loading weather data for year 1952...
Found 53 files for year 1952
Loaded data: 24,677,053 rows × 4 columns
Transforming to wide format...
Created wide format: 73,589 station-year-measurement combinations × 368 columns
Saved to data/weather_1952_wide.parquet

⏱️  Year 1952 processed in 80.0 seconds (1.3 minutes)

==================================================
Processing year 1953
==================================================
Loading weather data for year 1953...
Found 54 files for year 1953
Loaded data: 25,057,954 rows × 4 columns
Transforming to wide format...
Created wide format: 74,956 station-year-measurement combinations × 368 columns
Saved to data/weather_1953_wide.parquet

⏱️  Year 1953 processed in 80.8 seconds (1.3 minutes)

==================================================
Processing year 1954
==================================================
Loading weather data for year 1954...
Found 54 files for year 1954
Loaded data: 25,531,679 rows × 4 columns
Transforming to wide format...
Created wide format: 75,887 station-year-measurement combinations × 368 columns
Saved to data/weather_1954_wide.parquet

⏱️  Year 1954 processed in 81.1 seconds (1.4 minutes)

==================================================
Processing year 1955
==================================================
Loading weather data for year 1955...
Found 56 files for year 1955
Loaded data: 25,965,832 rows × 4 columns
Transforming to wide format...
Created wide format: 77,430 station-year-measurement combinations × 368 columns
Saved to data/weather_1955_wide.parquet

⏱️  Year 1955 processed in 85.8 seconds (1.4 minutes)

==================================================
Processing year 1956
==================================================
Loading weather data for year 1956...
Found 57 files for year 1956
Loaded data: 26,457,414 rows × 4 columns
Transforming to wide format...
Created wide format: 78,888 station-year-measurement combinations × 368 columns
Saved to data/weather_1956_wide.parquet

⏱️  Year 1956 processed in 85.6 seconds (1.4 minutes)

==================================================
Processing year 1957
==================================================
Loading weather data for year 1957...
Found 58 files for year 1957
Loaded data: 27,076,074 rows × 4 columns
Transforming to wide format...
Created wide format: 81,386 station-year-measurement combinations × 368 columns
Saved to data/weather_1957_wide.parquet

⏱️  Year 1957 processed in 91.1 seconds (1.5 minutes)

==================================================
Processing year 1958
==================================================
Loading weather data for year 1958...
Found 59 files for year 1958
Loaded data: 27,261,495 rows × 4 columns
Transforming to wide format...
Created wide format: 81,666 station-year-measurement combinations × 368 columns
Saved to data/weather_1958_wide.parquet

⏱️  Year 1958 processed in 89.2 seconds (1.5 minutes)

==================================================
Processing year 1959
==================================================
Loading weather data for year 1959...
Found 59 files for year 1959
Loaded data: 27,939,566 rows × 4 columns
Transforming to wide format...
Created wide format: 84,795 station-year-measurement combinations × 368 columns
Saved to data/weather_1959_wide.parquet

⏱️  Year 1959 processed in 96.7 seconds (1.6 minutes)

==================================================
Processing year 1960
==================================================
Loading weather data for year 1960...
Found 61 files for year 1960
Loaded data: 28,375,931 rows × 4 columns
Transforming to wide format...
Created wide format: 85,069 station-year-measurement combinations × 368 columns
Saved to data/weather_1960_wide.parquet

⏱️  Year 1960 processed in 100.1 seconds (1.7 minutes)

==================================================
Processing year 1961
==================================================
Loading weather data for year 1961...
Found 63 files for year 1961
Loaded data: 29,316,345 rows × 4 columns
Transforming to wide format...
Created wide format: 87,983 station-year-measurement combinations × 368 columns
Saved to data/weather_1961_wide.parquet

⏱️  Year 1961 processed in 103.7 seconds (1.7 minutes)

==================================================
Processing year 1962
==================================================
Loading weather data for year 1962...
Found 63 files for year 1962
Loaded data: 29,920,011 rows × 4 columns
Transforming to wide format...
Created wide format: 90,492 station-year-measurement combinations × 368 columns
Saved to data/weather_1962_wide.parquet

⏱️  Year 1962 processed in 103.1 seconds (1.7 minutes)

==================================================
Processing year 1963
==================================================
Loading weather data for year 1963...
Found 66 files for year 1963
Loaded data: 30,593,443 rows × 4 columns
Transforming to wide format...
Created wide format: 91,978 station-year-measurement combinations × 368 columns
Saved to data/weather_1963_wide.parquet

⏱️  Year 1963 processed in 110.0 seconds (1.8 minutes)

==================================================
Processing year 1964
==================================================
Loading weather data for year 1964...
Found 65 files for year 1964
Loaded data: 30,768,665 rows × 4 columns
Transforming to wide format...
Created wide format: 91,186 station-year-measurement combinations × 368 columns
Saved to data/weather_1964_wide.parquet

⏱️  Year 1964 processed in 111.8 seconds (1.9 minutes)

==================================================
Processing year 1965
==================================================
Loading weather data for year 1965...
Found 65 files for year 1965
Loaded data: 31,102,806 rows × 4 columns
Transforming to wide format...
Created wide format: 92,687 station-year-measurement combinations × 368 columns
Saved to data/weather_1965_wide.parquet

⏱️  Year 1965 processed in 118.9 seconds (2.0 minutes)

==================================================
Processing year 1966
==================================================
Loading weather data for year 1966...
Found 65 files for year 1966
Loaded data: 31,551,893 rows × 4 columns
Transforming to wide format...
2025-10-19 17:44:20,155 - distributed.utils_perf - INFO - full garbage collection released 282.42 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:44:21,191 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:44:25,240 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:44:40,132 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.79 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:44:57,682 - distributed.utils_perf - INFO - full garbage collection released 11.87 MiB from 105 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:45:36,433 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:45:40,108 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:45:42,093 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.48 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:45:42,260 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:45:50,325 - distributed.utils_perf - INFO - full garbage collection released 771.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:45:50,372 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:46:13,071 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:46:14,498 - distributed.utils_perf - INFO - full garbage collection released 289.78 MiB from 113 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:46:14,498 - distributed.worker.memory - WARNING - gc.collect() took 1.020s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 17:46:15,046 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:46:30,431 - distributed.utils_perf - INFO - full garbage collection released 812.67 MiB from 44 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:46:31,662 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:46:36,677 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:46:36,685 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.90 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:46:36,786 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:46:36,786 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:46:36,885 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 2.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:47:05,954 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:47:32,544 - distributed.worker.memory - WARNING - gc.collect() took 1.012s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 17:47:32,962 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:47:33,042 - distributed.utils_perf - INFO - full garbage collection released 16.18 MiB from 184 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:47:43,568 - distributed.utils_perf - INFO - full garbage collection released 293.22 MiB from 86 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:47:47,949 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:47:48,769 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:47:52,209 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:47:52,277 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.13 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:47:52,364 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:47:52,496 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:09,244 - distributed.utils_perf - INFO - full garbage collection released 23.12 MiB from 210 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:48:20,578 - distributed.utils_perf - INFO - full garbage collection released 291.48 MiB from 124 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:48:20,579 - distributed.worker.memory - WARNING - gc.collect() took 1.049s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 17:48:26,528 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:26,925 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:27,756 - distributed.utils_perf - INFO - full garbage collection released 769.83 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:48:38,157 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:39,469 - distributed.utils_perf - INFO - full garbage collection released 12.90 MiB from 152 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:48:41,059 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:41,150 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:41,150 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:41,251 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:41,350 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:41,576 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:41,620 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.42 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:48:41,769 - distributed.worker.memory - WARNING - Worker is at 27% memory usage. Resuming worker. Process memory: 1.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:49:15,294 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:49:40,873 - distributed.utils_perf - INFO - full garbage collection released 288.80 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:49:41,096 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 148 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:49:42,901 - distributed.utils_perf - INFO - full garbage collection released 192.32 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:49:43,144 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:49:43,201 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:49:43,301 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:49:43,401 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:49:44,059 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:49:48,516 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:49:54,737 - distributed.utils_perf - INFO - full garbage collection released 292.70 MiB from 44 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:49:56,768 - distributed.utils_perf - INFO - full garbage collection released 634.36 MiB from 37 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:49:56,768 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:49:57,123 - distributed.utils_perf - INFO - full garbage collection released 456.58 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:49:57,214 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:21,695 - distributed.utils_perf - INFO - full garbage collection released 288.80 MiB from 64 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:50:21,696 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:23,849 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:23,915 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:24,014 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:24,116 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:24,214 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:38,041 - distributed.utils_perf - INFO - full garbage collection released 293.46 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:50:45,259 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.90 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:45,357 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:45,357 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:45,457 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.11 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:45,556 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:45,862 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.93 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:50:46,100 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:15,040 - distributed.utils_perf - INFO - full garbage collection released 13.11 MiB from 172 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:51:48,504 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:51:49,303 - distributed.utils_perf - INFO - full garbage collection released 194.00 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:51:52,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:52,986 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:53,086 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:53,186 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:53,286 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:53,387 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:53,486 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:53,586 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:53,686 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:53,786 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:58,959 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:51:59,010 - distributed.utils_perf - INFO - full garbage collection released 295.00 MiB from 56 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:51:59,058 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:03,318 - distributed.utils_perf - INFO - full garbage collection released 297.29 MiB from 112 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:52:03,760 - distributed.utils_perf - INFO - full garbage collection released 195.82 MiB from 37 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:52:05,285 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:05,285 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:05,294 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:05,394 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:05,669 - distributed.utils_perf - INFO - full garbage collection released 434.39 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:52:36,949 - distributed.utils_perf - INFO - full garbage collection released 289.95 MiB from 57 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:52:41,082 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:42,721 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.44 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:44,400 - distributed.utils_perf - INFO - full garbage collection released 834.94 MiB from 114 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:52:50,558 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:55,421 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:55,947 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:56,007 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.13 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:56,188 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.59 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:52:57,719 - distributed.utils_perf - INFO - full garbage collection released 690.03 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:53:29,261 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:53:40,580 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:54:01,091 - distributed.utils_perf - INFO - full garbage collection released 183.45 MiB from 51 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:54:02,276 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:54:04,869 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:54:08,299 - distributed.utils_perf - INFO - full garbage collection released 273.07 MiB from 37 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:54:09,816 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.88 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:54:09,850 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:54:36,459 - distributed.utils_perf - INFO - full garbage collection released 273.44 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:54:44,894 - distributed.utils_perf - INFO - full garbage collection released 67.84 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:55:28,147 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:55:54,531 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:55:55,061 - distributed.worker.memory - WARNING - gc.collect() took 1.122s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 17:55:55,281 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:55:55,299 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:55:55,399 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:55:55,500 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:56:02,637 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.36 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:56:02,908 - distributed.utils_perf - INFO - full garbage collection released 276.36 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:56:04,198 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.36 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:56:07,195 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.79 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:56:07,241 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.86 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:56:31,818 - distributed.utils_perf - INFO - full garbage collection released 274.62 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:56:31,818 - distributed.worker.memory - WARNING - gc.collect() took 1.007s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 17:56:32,621 - distributed.utils_perf - INFO - full garbage collection released 276.51 MiB from 133 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:56:40,796 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.50 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:56:51,447 - distributed.utils_perf - INFO - full garbage collection released 772.13 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:57:38,794 - distributed.utils_perf - INFO - full garbage collection released 12.16 MiB from 226 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:57:52,213 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:57:52,286 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:57:53,649 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:58:02,882 - distributed.utils_perf - INFO - full garbage collection released 12.86 MiB from 177 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:58:05,879 - distributed.utils_perf - INFO - full garbage collection released 283.68 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:58:06,026 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:58:06,395 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:58:08,127 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.98 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:58:08,142 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.98 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:58:35,420 - distributed.utils_perf - INFO - full garbage collection released 279.61 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:58:36,214 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:58:36,383 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:58:36,443 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:58:38,792 - distributed.utils_perf - INFO - full garbage collection released 188.64 MiB from 44 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:58:39,850 - distributed.utils_perf - INFO - full garbage collection released 191.64 MiB from 25 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:58:49,335 - distributed.utils_perf - INFO - full garbage collection released 283.68 MiB from 31 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:58:49,336 - distributed.worker.memory - WARNING - gc.collect() took 1.144s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 17:58:55,388 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 17:59:38,732 - distributed.utils_perf - INFO - full garbage collection released 11.40 MiB from 139 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:59:50,024 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:59:51,339 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 17:59:55,990 - distributed.utils_perf - INFO - full garbage collection released 190.02 MiB from 51 reference cycles (threshold: 9.54 MiB)
2025-10-19 17:59:55,990 - distributed.worker.memory - WARNING - gc.collect() took 1.169s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:00:00,014 - distributed.utils_perf - INFO - full garbage collection released 10.74 MiB from 159 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:00:00,242 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:00,292 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:00,391 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:00,491 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:00,590 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:00,691 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:04,699 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:00:10,665 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:10,691 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:10,791 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:10,891 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:10,991 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:11,091 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:11,668 - distributed.utils_perf - INFO - full garbage collection released 15.27 MiB from 209 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:00:41,422 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:41,492 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:41,591 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:41,692 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:41,792 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:41,890 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:41,990 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:42,092 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:42,191 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:42,292 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:00:45,523 - distributed.utils_perf - INFO - full garbage collection released 191.34 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:00:56,042 - distributed.worker.memory - WARNING - gc.collect() took 1.212s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:00:59,071 - distributed.utils_perf - INFO - full garbage collection released 282.28 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:01:01,007 - distributed.utils_perf - INFO - full garbage collection released 768.08 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:01:44,040 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:02:07,951 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:08,049 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:08,150 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:08,251 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:08,350 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:08,449 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:08,551 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:08,651 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:08,751 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:08,920 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:08,950 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:09,051 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:09,149 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:09,249 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:09,349 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:09,374 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:09,450 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:09,571 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:09,575 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.75 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:09,659 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:09,758 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:10,556 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.59 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:11,237 - distributed.utils_perf - INFO - full garbage collection released 276.16 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:02:12,129 - distributed.utils_perf - INFO - full garbage collection released 189.38 MiB from 93 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:02:12,763 - distributed.utils_perf - INFO - full garbage collection released 278.22 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:02:14,749 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.94 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:14,755 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:14,855 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:14,855 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:14,955 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 2.32 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:39,655 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:39,741 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:39,841 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:39,943 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:40,043 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:40,143 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:45,432 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:45,636 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:45,657 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:45,756 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:45,857 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:45,957 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:50,342 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.48 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:57,648 - distributed.utils_perf - INFO - full garbage collection released 276.01 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:02:57,649 - distributed.worker.memory - WARNING - gc.collect() took 1.195s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:02:57,649 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 3.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:02:57,821 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 2.25 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:03:02,488 - distributed.utils_perf - INFO - full garbage collection released 649.66 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:03:02,512 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.33 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:03:03,115 - distributed.utils_perf - INFO - full garbage collection released 840.37 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:03:36,437 - distributed.utils_perf - INFO - full garbage collection released 24.38 MiB from 52 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:03:58,954 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.58 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:04:14,620 - distributed.utils_perf - INFO - full garbage collection released 281.62 MiB from 192 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:04:15,287 - distributed.utils_perf - INFO - full garbage collection released 284.97 MiB from 94 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:04:15,348 - distributed.utils_perf - INFO - full garbage collection released 188.89 MiB from 57 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:04:17,308 - distributed.utils_perf - INFO - full garbage collection released 796.16 MiB from 61 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:04:43,694 - distributed.utils_perf - INFO - full garbage collection released 280.86 MiB from 66 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:04:43,695 - distributed.worker.memory - WARNING - gc.collect() took 1.364s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:04:44,354 - distributed.utils_perf - INFO - full garbage collection released 12.56 MiB from 176 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:04:47,125 - distributed.utils_perf - INFO - full garbage collection released 276.34 MiB from 51 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:04:47,807 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:05:05,122 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.93 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:05:05,143 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.97 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:05:05,242 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:05:05,242 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:05:05,342 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 2.36 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:05:33,289 - distributed.utils_perf - INFO - full garbage collection released 9.61 MiB from 145 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:06:05,210 - distributed.worker.memory - WARNING - gc.collect() took 1.053s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:06:05,248 - distributed.utils_perf - INFO - full garbage collection released 185.26 MiB from 76 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:06:05,248 - distributed.worker.memory - WARNING - gc.collect() took 1.067s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:06:05,861 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:05,901 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:06,000 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:06,101 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:06,200 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:14,692 - distributed.utils_perf - INFO - full garbage collection released 285.86 MiB from 100 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:06:14,965 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:14,997 - distributed.utils_perf - INFO - full garbage collection released 740.62 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:06:17,786 - distributed.utils_perf - INFO - full garbage collection released 751.79 MiB from 63 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:06:46,601 - distributed.worker.memory - WARNING - gc.collect() took 1.022s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:06:46,740 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:46,955 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:46,990 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:47,090 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:51,402 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:54,436 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:54,457 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:54,555 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:54,656 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:54,756 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:54,855 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:54,955 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:55,057 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:55,156 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:55,255 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:55,355 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:55,422 - distributed.utils_perf - INFO - full garbage collection released 186.56 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:06:55,422 - distributed.worker.memory - WARNING - gc.collect() took 1.107s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:06:55,457 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:55,557 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:55,655 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:06:55,757 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:07:02,882 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 141 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:07:06,694 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:07:06,694 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:07:06,785 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:07:06,885 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:07:07,610 - distributed.utils_perf - INFO - full garbage collection released 185.42 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:08:01,108 - distributed.worker.memory - WARNING - gc.collect() took 1.003s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:08:03,326 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:08:16,322 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:16,325 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:16,424 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:16,525 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:16,625 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:16,725 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:16,825 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:16,925 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:17,025 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:17,125 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:17,224 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:18,192 - distributed.utils_perf - INFO - full garbage collection released 279.90 MiB from 89 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:08:18,217 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.50 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:18,855 - distributed.utils_perf - INFO - full garbage collection released 187.52 MiB from 98 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:08:19,609 - distributed.utils_perf - INFO - full garbage collection released 279.85 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:08:19,651 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:21,650 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:21,660 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:21,660 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:21,761 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:21,859 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:21,958 - distributed.utils_perf - INFO - full garbage collection released 738.20 MiB from 80 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:08:21,958 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:22,010 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:08:41,012 - distributed.utils_perf - INFO - full garbage collection released 20.21 MiB from 182 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:08:47,102 - distributed.utils_perf - INFO - full garbage collection released 279.59 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:08:47,102 - distributed.worker.memory - WARNING - gc.collect() took 1.408s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:08:47,632 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:08:48,172 - distributed.utils_perf - INFO - full garbage collection released 279.55 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:08:48,173 - distributed.worker.memory - WARNING - gc.collect() took 1.494s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:08:56,638 - distributed.utils_perf - INFO - full garbage collection released 185.73 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:08:56,639 - distributed.worker.memory - WARNING - gc.collect() took 1.031s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:09:02,407 - distributed.worker.memory - WARNING - gc.collect() took 1.002s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:09:02,407 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:09:02,575 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:09:07,788 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:09:09,134 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:09:09,155 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:09:09,359 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:10:09,047 - distributed.worker.memory - WARNING - gc.collect() took 1.053s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:10:10,304 - distributed.utils_perf - INFO - full garbage collection released 185.15 MiB from 133 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:10:10,305 - distributed.worker.memory - WARNING - gc.collect() took 1.446s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:10:21,140 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:10:21,482 - distributed.utils_perf - INFO - full garbage collection released 282.78 MiB from 100 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:10:23,277 - distributed.utils_perf - INFO - full garbage collection released 185.75 MiB from 76 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:10:23,359 - distributed.utils_perf - INFO - full garbage collection released 278.58 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:10:24,933 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.93 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:10:24,941 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.94 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:10:25,042 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:10:25,042 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:10:25,141 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 2.35 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:10:53,036 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:10:54,733 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:10:55,181 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:10:55,959 - distributed.utils_perf - INFO - full garbage collection released 195.07 MiB from 149 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:10:59,295 - distributed.utils_perf - INFO - full garbage collection released 809.62 MiB from 172 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:11:06,336 - distributed.utils_perf - INFO - full garbage collection released 198.56 MiB from 220 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:11:06,337 - distributed.worker.memory - WARNING - gc.collect() took 1.187s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:11:10,559 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:11:10,624 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:11:12,568 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.90 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:11:12,577 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.92 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:11:12,679 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:11:12,679 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:11:12,778 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 2.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:11:56,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:12:06,830 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:12:09,989 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 138 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:12:10,644 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:12:11,710 - distributed.utils_perf - INFO - full garbage collection released 279.77 MiB from 100 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:12:24,378 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:26,891 - distributed.utils_perf - INFO - full garbage collection released 284.00 MiB from 124 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:12:27,297 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:27,360 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:27,462 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:27,561 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:29,631 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:29,641 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.92 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:29,741 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:29,741 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:29,854 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 2.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:30,181 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.44 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:12:47,406 - distributed.utils_perf - INFO - full garbage collection released 17.63 MiB from 179 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:12:50,038 - distributed.utils_perf - INFO - full garbage collection released 137.67 MiB from 57 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:12:55,074 - distributed.utils_perf - INFO - full garbage collection released 713.27 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:12:55,075 - distributed.worker.memory - WARNING - gc.collect() took 1.492s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:12:55,326 - distributed.utils_perf - INFO - full garbage collection released 279.77 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:12:55,326 - distributed.worker.memory - WARNING - gc.collect() took 1.227s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:12:59,985 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:00,197 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:00,285 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:00,386 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:00,486 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:00,584 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:00,685 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:00,785 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:00,885 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:01,583 - distributed.utils_perf - INFO - full garbage collection released 187.66 MiB from 44 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:13:01,584 - distributed.worker.memory - WARNING - gc.collect() took 1.283s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:13:02,332 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:02,382 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:02,571 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:11,693 - distributed.utils_perf - INFO - full garbage collection released 281.54 MiB from 63 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:13:11,694 - distributed.worker.memory - WARNING - gc.collect() took 1.477s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:13:11,694 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:11,805 - distributed.worker.memory - WARNING - Worker is at 55% memory usage. Resuming worker. Process memory: 2.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:14,902 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:15,542 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:18,210 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:18,570 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.98 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:18,584 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:18,585 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:18,683 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:13:18,785 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.41 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:14:08,737 - distributed.utils_perf - INFO - full garbage collection released 218.82 MiB from 112 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:14:08,737 - distributed.worker.memory - WARNING - gc.collect() took 1.414s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:14:09,232 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:14:09,683 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:14:23,474 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:14:24,005 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:14:26,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:14:28,812 - distributed.utils_perf - INFO - full garbage collection released 284.55 MiB from 94 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:14:30,974 - distributed.utils_perf - INFO - full garbage collection released 285.85 MiB from 112 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:14:30,974 - distributed.worker.memory - WARNING - gc.collect() took 1.006s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:14:31,904 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:14:32,000 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:14:32,101 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.75 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:14:32,173 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:14:32,214 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:14:32,866 - distributed.utils_perf - INFO - full garbage collection released 187.33 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:14:35,558 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:14:35,658 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:14:35,658 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:14:35,758 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:14:35,987 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:04,631 - distributed.utils_perf - INFO - full garbage collection released 286.58 MiB from 176 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:15:04,631 - distributed.worker.memory - WARNING - gc.collect() took 1.671s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:15:05,755 - distributed.utils_perf - INFO - full garbage collection released 282.93 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:15:05,755 - distributed.worker.memory - WARNING - gc.collect() took 1.535s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:15:05,895 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:15:08,867 - distributed.utils_perf - INFO - full garbage collection released 189.02 MiB from 74 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:15:10,624 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:10,730 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:11,485 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:17,751 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:17,906 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:17,907 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:18,535 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:18,656 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:18,657 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:18,940 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:18,958 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:19,058 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:23,278 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:24,391 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.02 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:25,243 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:25,360 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:25,360 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:15:55,151 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:16:02,704 - distributed.utils_perf - INFO - full garbage collection released 16.37 MiB from 185 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:16:12,884 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:12,900 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:13,001 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:13,101 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:13,200 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:23,726 - distributed.worker.memory - WARNING - gc.collect() took 1.205s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:16:24,518 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:24,560 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:16:35,288 - distributed.utils_perf - INFO - full garbage collection released 289.30 MiB from 50 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:16:35,288 - distributed.worker.memory - WARNING - gc.collect() took 1.024s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:16:37,488 - distributed.utils_perf - INFO - full garbage collection released 193.55 MiB from 122 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:16:39,741 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:39,785 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:39,885 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:39,985 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:40,083 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:41,091 - distributed.utils_perf - INFO - full garbage collection released 190.39 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:16:42,773 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:42,783 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:42,783 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:42,885 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:42,984 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:16:43,388 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:11,984 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.84 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:12,207 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.93 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:12,278 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.93 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:16,709 - distributed.utils_perf - INFO - full garbage collection released 192.05 MiB from 25 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:17:16,709 - distributed.worker.memory - WARNING - gc.collect() took 1.127s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:17:17,354 - distributed.worker.memory - WARNING - gc.collect() took 1.136s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:17:17,642 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.88 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:21,519 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:21,781 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:21,861 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:21,961 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:22,060 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:22,161 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:22,262 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:22,361 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:22,460 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:25,336 - distributed.utils_perf - INFO - full garbage collection released 295.85 MiB from 70 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:17:26,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:17:30,910 - distributed.utils_perf - INFO - full garbage collection released 101.61 MiB from 82 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:17:32,778 - distributed.utils_perf - INFO - full garbage collection released 285.14 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:17:32,835 - distributed.utils_perf - INFO - full garbage collection released 190.20 MiB from 108 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:17:34,348 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:34,354 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:34,456 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:34,456 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.13 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:34,558 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 2.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:17:34,895 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:34,384 - distributed.utils_perf - INFO - full garbage collection released 287.22 MiB from 82 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:18:34,384 - distributed.worker.memory - WARNING - gc.collect() took 1.553s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:18:34,714 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:18:35,621 - distributed.utils_perf - INFO - full garbage collection released 190.70 MiB from 70 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:18:36,956 - distributed.utils_perf - INFO - full garbage collection released 754.25 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:18:36,958 - distributed.worker.memory - WARNING - gc.collect() took 1.307s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:18:36,995 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:18:40,549 - distributed.utils_perf - INFO - full garbage collection released 178.99 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:18:40,549 - distributed.worker.memory - WARNING - gc.collect() took 1.487s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:18:42,051 - distributed.worker.memory - WARNING - gc.collect() took 1.045s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:18:42,235 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:42,311 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:42,410 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:42,511 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:42,611 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:42,710 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:42,810 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.75 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:43,937 - distributed.utils_perf - INFO - full garbage collection released 290.25 MiB from 267 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:18:43,937 - distributed.worker.memory - WARNING - gc.collect() took 1.344s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:18:47,897 - distributed.utils_perf - INFO - full garbage collection released 286.09 MiB from 57 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:18:47,897 - distributed.worker.memory - WARNING - gc.collect() took 1.381s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:18:52,547 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.93 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:52,558 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:52,657 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:52,657 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:52,756 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 2.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:18:53,175 - distributed.utils_perf - INFO - full garbage collection released 772.39 MiB from 56 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:19:15,402 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:19:15,491 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:19:15,591 - distributed.utils_perf - INFO - full garbage collection released 32.47 MiB from 139 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:19:15,591 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:19:15,690 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:19:15,790 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:19:15,891 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:19:16,472 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:19:17,594 - distributed.utils_perf - INFO - full garbage collection released 9.66 MiB from 174 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:19:21,288 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:19:21,824 - distributed.utils_perf - INFO - full garbage collection released 286.42 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:19:21,824 - distributed.worker.memory - WARNING - gc.collect() took 1.799s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:19:22,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:19:24,323 - distributed.utils_perf - INFO - full garbage collection released 190.07 MiB from 44 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:19:27,397 - distributed.utils_perf - INFO - full garbage collection released 190.73 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:19:35,878 - distributed.utils_perf - INFO - full garbage collection released 13.07 MiB from 106 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:19:35,878 - distributed.worker.memory - WARNING - gc.collect() took 1.716s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:19:36,758 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:19:37,027 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.88 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:19:38,883 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:19:40,139 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.48 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:19:43,502 - distributed.utils_perf - INFO - full garbage collection released 631.46 MiB from 7 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:19:43,503 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:30,254 - distributed.utils_perf - INFO - full garbage collection released 276.29 MiB from 105 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:20:30,254 - distributed.worker.memory - WARNING - gc.collect() took 1.227s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:20:30,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:20:43,838 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:43,878 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:43,978 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:44,079 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:44,177 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:44,279 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:44,379 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:44,477 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:44,579 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:44,677 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:44,777 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:44,878 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:44,978 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:50,386 - distributed.utils_perf - INFO - full garbage collection released 182.71 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:20:55,365 - distributed.utils_perf - INFO - full garbage collection released 11.63 MiB from 218 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:20:56,608 - distributed.utils_perf - INFO - full garbage collection released 357.57 MiB from 132 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:20:56,646 - distributed.utils_perf - INFO - full garbage collection released 235.15 MiB from 51 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:20:59,062 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:59,062 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:59,144 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:59,244 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:59,344 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.48 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:20:59,621 - distributed.utils_perf - INFO - full garbage collection released 543.65 MiB from 194 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:20:59,696 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:21:23,736 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:21:24,650 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:21:31,309 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:21:33,919 - distributed.utils_perf - INFO - full garbage collection released 184.39 MiB from 44 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:21:33,920 - distributed.worker.memory - WARNING - gc.collect() took 1.619s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:21:34,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:21:46,254 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:21:46,630 - distributed.utils_perf - INFO - full garbage collection released 807.33 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:21:46,630 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.19 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:21:46,631 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.19 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:21:46,641 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:21:47,836 - distributed.utils_perf - INFO - full garbage collection released 449.38 MiB from 19 reference cycles (threshold: 9.54 MiB)
Created wide format: 93,403 station-year-measurement combinations × 368 columns
Saved to data/weather_1966_wide.parquet

⏱️  Year 1966 processed in 115.5 seconds (1.9 minutes)

==================================================
Processing year 1967
==================================================
Loading weather data for year 1967...
Found 66 files for year 1967
Loaded data: 31,809,106 rows × 4 columns
Transforming to wide format...
Created wide format: 94,035 station-year-measurement combinations × 368 columns
Saved to data/weather_1967_wide.parquet

⏱️  Year 1967 processed in 117.4 seconds (2.0 minutes)

==================================================
Processing year 1968
==================================================
Loading weather data for year 1968...
Found 66 files for year 1968
Loaded data: 31,835,416 rows × 4 columns
Transforming to wide format...
Created wide format: 94,147 station-year-measurement combinations × 368 columns
Saved to data/weather_1968_wide.parquet

⏱️  Year 1968 processed in 124.8 seconds (2.1 minutes)

==================================================
Processing year 1969
==================================================
Loading weather data for year 1969...
Found 68 files for year 1969
Loaded data: 32,210,729 rows × 4 columns
Transforming to wide format...
Created wide format: 96,213 station-year-measurement combinations × 368 columns
Saved to data/weather_1969_wide.parquet

⏱️  Year 1969 processed in 124.2 seconds (2.1 minutes)

==================================================
Processing year 1970
==================================================
Loading weather data for year 1970...
Found 69 files for year 1970
Loaded data: 32,400,359 rows × 4 columns
Transforming to wide format...
Created wide format: 96,830 station-year-measurement combinations × 368 columns
Saved to data/weather_1970_wide.parquet

⏱️  Year 1970 processed in 131.5 seconds (2.2 minutes)

==================================================
Processing year 1971
==================================================
Loading weather data for year 1971...
Found 65 files for year 1971
Loaded data: 31,221,091 rows × 4 columns
Transforming to wide format...
Created wide format: 93,880 station-year-measurement combinations × 368 columns
Saved to data/weather_1971_wide.parquet

⏱️  Year 1971 processed in 117.6 seconds (2.0 minutes)

==================================================
Processing year 1972
==================================================
Loading weather data for year 1972...
Found 65 files for year 1972
Loaded data: 31,124,123 rows × 4 columns
Transforming to wide format...
Created wide format: 92,329 station-year-measurement combinations × 368 columns
Saved to data/weather_1972_wide.parquet

⏱️  Year 1972 processed in 115.9 seconds (1.9 minutes)

==================================================
Processing year 1973
==================================================
Loading weather data for year 1973...
Found 68 files for year 1973
Loaded data: 32,233,744 rows × 4 columns
Transforming to wide format...
Created wide format: 101,328 station-year-measurement combinations × 368 columns
Saved to data/weather_1973_wide.parquet

⏱️  Year 1973 processed in 124.7 seconds (2.1 minutes)

==================================================
Processing year 1974
==================================================
Loading weather data for year 1974...
Found 68 files for year 1974
Loaded data: 32,504,963 rows × 4 columns
Transforming to wide format...
Created wide format: 100,996 station-year-measurement combinations × 368 columns
Saved to data/weather_1974_wide.parquet

⏱️  Year 1974 processed in 125.8 seconds (2.1 minutes)

==================================================
Processing year 1975
==================================================
Loading weather data for year 1975...
Found 67 files for year 1975
Loaded data: 32,305,003 rows × 4 columns
Transforming to wide format...
Created wide format: 100,627 station-year-measurement combinations × 368 columns
Saved to data/weather_1975_wide.parquet

⏱️  Year 1975 processed in 121.6 seconds (2.0 minutes)

==================================================
Processing year 1976
==================================================
Loading weather data for year 1976...
Found 69 files for year 1976
Loaded data: 32,381,851 rows × 4 columns
Transforming to wide format...
Created wide format: 100,227 station-year-measurement combinations × 368 columns
Saved to data/weather_1976_wide.parquet

⏱️  Year 1976 processed in 122.8 seconds (2.0 minutes)

==================================================
Processing year 1977
==================================================
Loading weather data for year 1977...
Found 68 files for year 1977
Loaded data: 32,207,180 rows × 4 columns
Transforming to wide format...
Created wide format: 100,165 station-year-measurement combinations × 368 columns
Saved to data/weather_1977_wide.parquet

⏱️  Year 1977 processed in 123.5 seconds (2.1 minutes)

==================================================
Processing year 1978
==================================================
Loading weather data for year 1978...
Found 68 files for year 1978
Loaded data: 32,130,624 rows × 4 columns
Transforming to wide format...
Created wide format: 100,321 station-year-measurement combinations × 368 columns
Saved to data/weather_1978_wide.parquet

⏱️  Year 1978 processed in 120.6 seconds (2.0 minutes)

==================================================
Processing year 1979
==================================================
Loading weather data for year 1979...
Found 68 files for year 1979
Loaded data: 32,186,442 rows × 4 columns
Transforming to wide format...
Created wide format: 100,060 station-year-measurement combinations × 368 columns
Saved to data/weather_1979_wide.parquet

⏱️  Year 1979 processed in 123.2 seconds (2.1 minutes)

==================================================
Processing year 1980
==================================================
Loading weather data for year 1980...
Found 69 files for year 1980
Loaded data: 32,450,665 rows × 4 columns
Transforming to wide format...
Created wide format: 102,440 station-year-measurement combinations × 368 columns
Saved to data/weather_1980_wide.parquet

⏱️  Year 1980 processed in 126.3 seconds (2.1 minutes)

==================================================
Processing year 1981
==================================================
Loading weather data for year 1981...
Found 69 files for year 1981
Loaded data: 33,183,917 rows × 4 columns
Transforming to wide format...
Created wide format: 102,749 station-year-measurement combinations × 368 columns
Saved to data/weather_1981_wide.parquet

⏱️  Year 1981 processed in 126.5 seconds (2.1 minutes)

==================================================
Processing year 1982
==================================================
Loading weather data for year 1982...
Found 68 files for year 1982
Loaded data: 32,835,727 rows × 4 columns
Transforming to wide format...
Created wide format: 103,265 station-year-measurement combinations × 368 columns
Saved to data/weather_1982_wide.parquet

⏱️  Year 1982 processed in 128.9 seconds (2.1 minutes)

==================================================
Processing year 1983
==================================================
Loading weather data for year 1983...
Found 70 files for year 1983
Loaded data: 33,136,044 rows × 4 columns
Transforming to wide format...
Created wide format: 103,340 station-year-measurement combinations × 368 columns
Saved to data/weather_1983_wide.parquet

⏱️  Year 1983 processed in 129.2 seconds (2.2 minutes)

==================================================
Processing year 1984
==================================================
Loading weather data for year 1984...
Found 69 files for year 1984
Loaded data: 32,515,346 rows × 4 columns
Transforming to wide format...
Created wide format: 101,880 station-year-measurement combinations × 368 columns
Saved to data/weather_1984_wide.parquet
2025-10-19 18:22:19,897 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:22:27,805 - distributed.utils_perf - INFO - full garbage collection released 11.27 MiB from 207 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:22:37,517 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:22:41,515 - distributed.worker.memory - WARNING - gc.collect() took 1.114s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:22:41,647 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:22:41,960 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:22:47,575 - distributed.worker.memory - WARNING - gc.collect() took 1.136s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:22:47,799 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:22:47,860 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:22:52,012 - distributed.utils_perf - INFO - full garbage collection released 274.44 MiB from 50 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:22:52,012 - distributed.worker.memory - WARNING - gc.collect() took 1.485s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:22:52,729 - distributed.worker.memory - WARNING - gc.collect() took 1.127s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:22:57,568 - distributed.utils_perf - INFO - full garbage collection released 346.55 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:22:57,568 - distributed.worker.memory - WARNING - gc.collect() took 1.138s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:22:58,122 - distributed.utils_perf - INFO - full garbage collection released 234.15 MiB from 88 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:23:00,345 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:00,345 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:00,424 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.13 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:00,525 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:00,595 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:00,623 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:00,729 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:00,749 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:22,844 - distributed.worker.memory - WARNING - gc.collect() took 1.317s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:23:23,370 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:27,783 - distributed.worker.memory - WARNING - gc.collect() took 1.100s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:23:27,940 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:28,187 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:28,255 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:31,808 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:31,898 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:33,949 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.42 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:42,272 - distributed.worker.memory - WARNING - gc.collect() took 1.411s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:23:44,495 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:46,465 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:46,465 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:46,541 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:46,641 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.24 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:46,699 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.07 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:23:46,740 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:24:38,742 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 179 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:24:42,183 - distributed.utils_perf - INFO - full garbage collection released 264.94 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:24:42,184 - distributed.worker.memory - WARNING - gc.collect() took 1.159s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:24:59,252 - distributed.utils_perf - INFO - full garbage collection released 271.71 MiB from 75 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:24:59,309 - distributed.utils_perf - INFO - full garbage collection released 178.78 MiB from 127 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:25:00,665 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:00,788 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:00,863 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:00,963 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:01,063 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:01,162 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:01,283 - distributed.utils_perf - INFO - full garbage collection released 545.93 MiB from 133 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:25:01,283 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:01,320 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.59 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:01,376 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:01,393 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:26,471 - distributed.worker.memory - WARNING - gc.collect() took 1.184s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:25:26,763 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:33,609 - distributed.utils_perf - INFO - full garbage collection released 265.20 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:25:33,609 - distributed.worker.memory - WARNING - gc.collect() took 1.423s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:25:33,609 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:33,678 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:34,514 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:35,438 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:35,500 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:35,729 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:35,799 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:36,133 - distributed.utils_perf - INFO - full garbage collection released 178.30 MiB from 88 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:25:36,134 - distributed.worker.memory - WARNING - gc.collect() took 1.220s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:25:47,869 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:47,974 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:48,056 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.25 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:48,174 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:48,256 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:48,333 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:48,428 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:48,471 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 2.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:25:48,562 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:26:45,643 - distributed.utils_perf - INFO - full garbage collection released 176.34 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:26:47,989 - distributed.utils_perf - INFO - full garbage collection released 264.45 MiB from 70 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:26:47,989 - distributed.worker.memory - WARNING - gc.collect() took 1.461s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:26:48,312 - distributed.utils_perf - INFO - full garbage collection released 175.80 MiB from 89 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:26:48,313 - distributed.worker.memory - WARNING - gc.collect() took 1.420s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:26:58,831 - distributed.utils_perf - INFO - full garbage collection released 473.48 MiB from 136 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:26:58,831 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:26:58,868 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:26:59,106 - distributed.utils_perf - INFO - full garbage collection released 554.11 MiB from 132 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:26:59,127 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:22,318 - distributed.utils_perf - INFO - full garbage collection released 271.79 MiB from 125 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:27:22,318 - distributed.worker.memory - WARNING - gc.collect() took 1.258s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:27:24,801 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.48 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:29,090 - distributed.utils_perf - INFO - full garbage collection released 177.91 MiB from 44 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:27:32,944 - distributed.utils_perf - INFO - full garbage collection released 180.31 MiB from 49 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:27:32,945 - distributed.worker.memory - WARNING - gc.collect() took 1.180s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:27:44,080 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:44,080 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:44,155 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:44,254 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:44,367 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:44,454 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:44,648 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:44,708 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:44,743 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:27:44,843 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.58 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:28:11,955 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:28:18,828 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:28:21,524 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:28:27,021 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:28:37,913 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:28:41,898 - distributed.utils_perf - INFO - full garbage collection released 175.68 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:28:41,899 - distributed.worker.memory - WARNING - gc.collect() took 1.658s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:28:42,777 - distributed.utils_perf - INFO - full garbage collection released 347.20 MiB from 103 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:28:42,777 - distributed.worker.memory - WARNING - gc.collect() took 1.627s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:28:44,387 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:28:58,337 - distributed.utils_perf - INFO - full garbage collection released 178.27 MiB from 67 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:28:59,191 - distributed.utils_perf - INFO - full garbage collection released 274.68 MiB from 174 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:29:01,833 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:01,939 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:02,010 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.27 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:02,110 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.32 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:02,210 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:02,611 - distributed.utils_perf - INFO - full garbage collection released 540.85 MiB from 83 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:29:02,674 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:25,922 - distributed.worker.memory - WARNING - gc.collect() took 1.299s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:29:26,686 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:27,385 - distributed.utils_perf - INFO - full garbage collection released 264.31 MiB from 139 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:29:27,385 - distributed.worker.memory - WARNING - gc.collect() took 1.260s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:29:32,980 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:33,055 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:33,156 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:33,255 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:34,590 - distributed.utils_perf - INFO - full garbage collection released 178.29 MiB from 100 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:29:34,591 - distributed.worker.memory - WARNING - gc.collect() took 1.080s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:29:43,586 - distributed.utils_perf - INFO - full garbage collection released 210.38 MiB from 62 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:29:43,586 - distributed.worker.memory - WARNING - gc.collect() took 1.083s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:29:44,989 - distributed.utils_perf - INFO - full garbage collection released 15.64 MiB from 162 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:29:47,403 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:47,427 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:47,456 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:47,456 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:47,508 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.11 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:47,540 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.25 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:47,557 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.24 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:47,641 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:47,641 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:29:47,690 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:18,887 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:30:19,160 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:30:20,737 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:30:29,477 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:30:37,870 - distributed.utils_perf - INFO - full garbage collection released 261.75 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:30:37,871 - distributed.worker.memory - WARNING - gc.collect() took 1.603s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:30:40,012 - distributed.utils_perf - INFO - full garbage collection released 265.75 MiB from 81 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:30:40,012 - distributed.worker.memory - WARNING - gc.collect() took 1.401s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:30:42,837 - distributed.utils_perf - INFO - full garbage collection released 175.01 MiB from 88 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:30:43,939 - distributed.utils_perf - INFO - full garbage collection released 686.58 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:30:43,939 - distributed.worker.memory - WARNING - gc.collect() took 1.072s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:30:44,338 - distributed.utils_perf - INFO - full garbage collection released 177.99 MiB from 47 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:30:44,338 - distributed.worker.memory - WARNING - gc.collect() took 1.092s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:30:51,683 - distributed.utils_perf - INFO - full garbage collection released 340.43 MiB from 114 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:30:56,412 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:56,492 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:56,492 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:56,592 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:56,644 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:56,692 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.50 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:56,721 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:56,748 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:56,794 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:56,847 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 2.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:30:56,949 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.79 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:23,275 - distributed.utils_perf - INFO - full garbage collection released 260.89 MiB from 76 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:31:23,275 - distributed.worker.memory - WARNING - gc.collect() took 1.615s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:31:29,289 - distributed.utils_perf - INFO - full garbage collection released 194.16 MiB from 42 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:31:29,289 - distributed.worker.memory - WARNING - gc.collect() took 1.203s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:31:30,253 - distributed.utils_perf - INFO - full garbage collection released 22.06 MiB from 101 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:31:30,253 - distributed.worker.memory - WARNING - gc.collect() took 1.519s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:31:30,833 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:31:31,043 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:37,118 - distributed.utils_perf - INFO - full garbage collection released 341.82 MiB from 89 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:31:42,011 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:42,092 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:42,092 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:42,192 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.27 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:42,303 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:42,393 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:42,783 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:42,855 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.18 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:42,919 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:31:42,990 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:14,530 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:32:20,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:32:32,350 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:32:49,173 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:49,219 - distributed.utils_perf - INFO - full garbage collection released 173.41 MiB from 64 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:32:49,220 - distributed.worker.memory - WARNING - gc.collect() took 1.438s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:32:49,239 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:49,366 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:49,440 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:49,541 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:49,743 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.75 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:50,875 - distributed.utils_perf - INFO - full garbage collection released 338.29 MiB from 113 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:32:54,377 - distributed.utils_perf - INFO - full garbage collection released 260.07 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:32:54,408 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:55,015 - distributed.utils_perf - INFO - full garbage collection released 496.23 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:32:55,015 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:55,073 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:55,110 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:56,152 - distributed.worker.memory - WARNING - Worker is at 90% memory usage. Pausing worker.  Process memory: 3.36 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:56,226 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:56,259 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:32:56,259 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:33:20,529 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:33:20,586 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:33:23,765 - distributed.utils_perf - INFO - full garbage collection released 261.20 MiB from 31 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:33:26,602 - distributed.utils_perf - INFO - full garbage collection released 246.58 MiB from 91 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:33:26,602 - distributed.worker.memory - WARNING - gc.collect() took 1.026s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:33:39,268 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 132 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:33:41,009 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 3.27 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:33:41,084 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.29 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:33:41,154 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:33:41,466 - distributed.utils_perf - INFO - full garbage collection released 487.62 MiB from 121 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:33:41,467 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:33:41,525 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:33:41,561 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.79 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:34:10,928 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:34:17,559 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:34:18,874 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:34:40,719 - distributed.utils_perf - INFO - full garbage collection released 257.39 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:34:40,719 - distributed.worker.memory - WARNING - gc.collect() took 1.096s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:34:41,805 - distributed.utils_perf - INFO - full garbage collection released 176.57 MiB from 68 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:34:42,236 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:34:42,565 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:34:45,661 - distributed.utils_perf - INFO - full garbage collection released 182.23 MiB from 173 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:34:52,739 - distributed.utils_perf - INFO - full garbage collection released 530.98 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:34:52,762 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:34:53,024 - distributed.utils_perf - INFO - full garbage collection released 521.71 MiB from 89 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:34:53,024 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:34:53,088 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:19,641 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:19,692 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:19,793 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:20,224 - distributed.utils_perf - INFO - full garbage collection released 262.27 MiB from 144 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:35:21,353 - distributed.utils_perf - INFO - full garbage collection released 174.16 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:35:23,651 - distributed.utils_perf - INFO - full garbage collection released 174.33 MiB from 56 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:35:25,146 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:35:38,564 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:38,589 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.07 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:38,648 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:38,748 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.33 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:38,849 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:38,947 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:39,006 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:39,161 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:35:39,196 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.58 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:36:17,069 - distributed.utils_perf - INFO - full garbage collection released 10.25 MiB from 155 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:36:32,884 - distributed.utils_perf - INFO - full garbage collection released 170.02 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:36:32,885 - distributed.worker.memory - WARNING - gc.collect() took 1.183s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:36:38,800 - distributed.utils_perf - INFO - full garbage collection released 255.62 MiB from 51 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:36:38,800 - distributed.worker.memory - WARNING - gc.collect() took 1.303s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:36:42,336 - distributed.utils_perf - INFO - full garbage collection released 32.56 MiB from 173 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:36:43,909 - distributed.worker.memory - WARNING - gc.collect() took 1.193s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:36:44,078 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:36:44,747 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 148 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:36:51,149 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:36:51,296 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.11 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:36:51,343 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.19 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:36:51,442 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.28 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:36:51,543 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:36:52,144 - distributed.utils_perf - INFO - full garbage collection released 349.76 MiB from 83 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:36:52,463 - distributed.utils_perf - INFO - full garbage collection released 689.76 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:37:18,363 - distributed.utils_perf - INFO - full garbage collection released 259.43 MiB from 56 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:37:18,364 - distributed.worker.memory - WARNING - gc.collect() took 1.042s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:37:18,438 - distributed.utils_perf - INFO - full garbage collection released 255.38 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:37:18,438 - distributed.worker.memory - WARNING - gc.collect() took 1.379s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:37:26,508 - distributed.utils_perf - INFO - full garbage collection released 171.67 MiB from 88 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:37:26,508 - distributed.worker.memory - WARNING - gc.collect() took 1.655s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:37:26,762 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:37:36,136 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:37:36,162 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:37:36,259 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:37:36,297 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.23 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:37:36,396 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:37:36,498 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.59 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:37:36,915 - distributed.utils_perf - INFO - full garbage collection released 500.89 MiB from 57 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:37:36,993 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:37:37,009 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:38:45,483 - distributed.utils_perf - INFO - full garbage collection released 168.66 MiB from 107 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:38:48,348 - distributed.utils_perf - INFO - full garbage collection released 253.16 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:38:50,414 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 3.28 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:38:50,476 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.48 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:38:50,498 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:38:50,945 - distributed.utils_perf - INFO - full garbage collection released 529.75 MiB from 64 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:38:50,982 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:38:50,999 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:39:15,785 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:39:15,793 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:39:22,818 - distributed.utils_perf - INFO - full garbage collection released 168.61 MiB from 101 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:39:22,819 - distributed.worker.memory - WARNING - gc.collect() took 1.106s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:39:23,599 - distributed.utils_perf - INFO - full garbage collection released 168.66 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:39:23,601 - distributed.worker.memory - WARNING - gc.collect() took 1.307s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:39:34,600 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:39:34,753 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:39:34,797 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:39:34,896 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.29 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:39:34,996 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.48 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:39:35,096 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:39:35,588 - distributed.utils_perf - INFO - full garbage collection released 528.41 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:39:35,588 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:39:35,646 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:40:19,326 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 153 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:40:27,403 - distributed.utils_perf - INFO - full garbage collection released 251.53 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:40:27,404 - distributed.worker.memory - WARNING - gc.collect() took 1.215s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:40:29,926 - distributed.utils_perf - INFO - full garbage collection released 650.95 MiB from 83 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:40:29,927 - distributed.worker.memory - WARNING - gc.collect() took 1.113s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:40:30,227 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:40:32,438 - distributed.utils_perf - INFO - full garbage collection released 252.78 MiB from 119 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:40:32,438 - distributed.worker.memory - WARNING - gc.collect() took 1.006s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:40:34,382 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:40:37,887 - distributed.utils_perf - INFO - full garbage collection released 56.28 MiB from 43 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:40:43,163 - distributed.utils_perf - INFO - full garbage collection released 168.17 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:40:46,789 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:40:46,977 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.24 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:40:46,981 - distributed.utils_perf - INFO - full garbage collection released 485.36 MiB from 101 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:40:47,033 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:40:47,066 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:40:47,066 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:40:47,089 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.83 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:41:13,739 - distributed.utils_perf - INFO - full garbage collection released 254.90 MiB from 72 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:41:13,739 - distributed.worker.memory - WARNING - gc.collect() took 1.450s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:41:20,773 - distributed.utils_perf - INFO - full garbage collection released 330.79 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:41:20,773 - distributed.worker.memory - WARNING - gc.collect() took 1.410s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:41:24,380 - distributed.utils_perf - INFO - full garbage collection released 168.48 MiB from 95 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:41:25,798 - distributed.utils_perf - INFO - full garbage collection released 659.41 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:41:25,798 - distributed.worker.memory - WARNING - gc.collect() took 1.389s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:41:25,799 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:41:30,431 - distributed.worker.memory - WARNING - Worker is at 91% memory usage. Pausing worker.  Process memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:41:30,526 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.59 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:41:30,599 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:41:30,599 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:41:31,652 - distributed.utils_perf - INFO - full garbage collection released 505.12 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:41:31,652 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:41:31,704 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:41:58,604 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:42:25,485 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:42:26,593 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:42:28,446 - distributed.utils_perf - INFO - full garbage collection released 248.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:42:28,446 - distributed.worker.memory - WARNING - gc.collect() took 1.076s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:42:41,162 - distributed.utils_perf - INFO - full garbage collection released 519.90 MiB from 70 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:42:41,186 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:42:41,209 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:42:42,390 - distributed.utils_perf - INFO - full garbage collection released 543.29 MiB from 107 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:42:42,390 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:42:42,426 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:42:42,457 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:42:42,483 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:07,560 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:07,648 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:07,747 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:07,848 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:07,948 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:09,302 - distributed.utils_perf - INFO - full garbage collection released 249.09 MiB from 101 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:43:09,303 - distributed.worker.memory - WARNING - gc.collect() took 1.136s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:43:21,811 - distributed.utils_perf - INFO - full garbage collection released 12.97 MiB from 160 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:43:25,088 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.97 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:25,148 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:25,148 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:25,248 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:25,348 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:25,448 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.58 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:25,734 - distributed.utils_perf - INFO - full garbage collection released 507.55 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:43:25,734 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:25,769 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:43:46,243 - distributed.utils_perf - INFO - full garbage collection released 12.33 MiB from 414 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:43:59,672 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:44:11,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:44:21,860 - distributed.utils_perf - INFO - full garbage collection released 247.71 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:44:21,860 - distributed.worker.memory - WARNING - gc.collect() took 1.210s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:44:29,209 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:29,484 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:44:30,763 - distributed.utils_perf - INFO - full garbage collection released 165.54 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:44:30,990 - distributed.utils_perf - INFO - full garbage collection released 252.41 MiB from 113 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:44:37,506 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:37,628 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:37,697 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.26 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:37,797 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.36 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:37,897 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:37,996 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:38,220 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:38,366 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.23 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:38,392 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.36 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:38,492 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.75 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:38,492 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.75 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:44:57,983 - distributed.utils_perf - INFO - full garbage collection released 250.42 MiB from 208 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:45:02,725 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:02,740 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:02,840 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:08,520 - distributed.utils_perf - INFO - full garbage collection released 339.48 MiB from 248 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:45:19,773 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:19,927 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:20,134 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:20,154 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:20,253 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:20,253 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:20,996 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:21,084 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:21,133 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.58 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:45:21,212 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:46:15,584 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:46:22,514 - distributed.utils_perf - INFO - full garbage collection released 245.62 MiB from 51 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:46:22,514 - distributed.worker.memory - WARNING - gc.collect() took 1.108s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:46:28,346 - distributed.utils_perf - INFO - full garbage collection released 164.56 MiB from 101 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:46:28,347 - distributed.worker.memory - WARNING - gc.collect() took 1.290s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:46:31,017 - distributed.utils_perf - INFO - full garbage collection released 244.49 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:46:31,510 - distributed.utils_perf - INFO - full garbage collection released 15.27 MiB from 267 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:46:33,194 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:46:33,361 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:46:33,447 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.27 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:46:33,547 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:46:33,548 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:46:33,792 - distributed.utils_perf - INFO - full garbage collection released 479.74 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:46:33,792 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:46:33,813 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:46:57,365 - distributed.worker.memory - WARNING - gc.collect() took 1.282s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:46:57,519 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:47:06,715 - distributed.worker.memory - WARNING - gc.collect() took 1.271s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:47:07,043 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.58 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:47:07,174 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:47:07,189 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:47:17,694 - distributed.worker.memory - WARNING - Worker is at 92% memory usage. Pausing worker.  Process memory: 3.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:47:17,819 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:47:17,819 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:47:17,854 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:47:18,408 - distributed.utils_perf - INFO - full garbage collection released 426.58 MiB from 101 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:48:15,551 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:48:23,569 - distributed.utils_perf - INFO - full garbage collection released 233.57 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:48:26,806 - distributed.utils_perf - INFO - full garbage collection released 564.34 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:48:26,828 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:48:26,876 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:48:26,922 - distributed.utils_perf - INFO - full garbage collection released 416.76 MiB from 31 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:48:26,922 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:48:26,955 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:48:56,935 - distributed.worker.memory - WARNING - gc.collect() took 1.302s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:49:07,810 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:49:07,876 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.20 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:49:07,882 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.13 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:49:07,983 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.33 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:49:08,158 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:49:08,182 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:49:09,018 - distributed.utils_perf - INFO - full garbage collection released 525.50 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:49:09,018 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:49:09,052 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:49:35,633 - distributed.utils_perf - INFO - full garbage collection released 21.27 MiB from 82 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:49:50,199 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:50:00,788 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:50:14,030 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 259 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:50:17,092 - distributed.utils_perf - INFO - full garbage collection released 488.25 MiB from 149 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:50:17,164 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:50:17,206 - distributed.utils_perf - INFO - full garbage collection released 381.32 MiB from 119 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:50:58,424 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 3.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:50:58,535 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:50:58,536 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:50:58,696 - distributed.utils_perf - INFO - full garbage collection released 524.11 MiB from 87 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:50:58,696 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:50:58,757 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:51:34,744 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:51:53,622 - distributed.utils_perf - INFO - full garbage collection released 645.16 MiB from 57 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:51:53,623 - distributed.worker.memory - WARNING - gc.collect() took 1.116s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:52:04,498 - distributed.worker.memory - WARNING - Worker is at 92% memory usage. Pausing worker.  Process memory: 3.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:04,555 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:04,639 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:04,639 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:04,977 - distributed.utils_perf - INFO - full garbage collection released 402.27 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:52:04,977 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:05,049 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 2.79 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:27,031 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:33,687 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.42 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:44,821 - distributed.utils_perf - INFO - full garbage collection released 15.63 MiB from 173 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:52:45,533 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:45,533 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:45,583 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.18 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:45,682 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.35 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:45,782 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:45,883 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:45,883 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:52:46,408 - distributed.utils_perf - INFO - full garbage collection released 465.85 MiB from 101 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:52:46,487 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:53:41,771 - distributed.utils_perf - INFO - full garbage collection released 12.93 MiB from 182 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:53:44,570 - distributed.utils_perf - INFO - full garbage collection released 229.48 MiB from 81 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:53:44,570 - distributed.worker.memory - WARNING - gc.collect() took 1.563s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:53:52,181 - distributed.utils_perf - INFO - full garbage collection released 224.58 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:53:52,181 - distributed.worker.memory - WARNING - gc.collect() took 1.152s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:53:57,163 - distributed.utils_perf - INFO - full garbage collection released 472.07 MiB from 107 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:53:57,163 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:53:57,190 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:53:57,279 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.83 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:53:57,823 - distributed.utils_perf - INFO - full garbage collection released 474.51 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:54:33,709 - distributed.utils_perf - INFO - full garbage collection released 16.13 MiB from 188 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:54:37,819 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:54:37,880 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:54:37,991 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:54:38,716 - distributed.utils_perf - INFO - full garbage collection released 463.07 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:55:07,845 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:55:15,080 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:55:30,028 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:55:33,579 - distributed.utils_perf - INFO - full garbage collection released 16.73 MiB from 160 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:55:46,383 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:55:46,454 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.59 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:55:46,539 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:55:46,776 - distributed.utils_perf - INFO - full garbage collection released 381.84 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:55:46,776 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:55:46,841 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:56:10,335 - distributed.utils_perf - INFO - full garbage collection released 228.71 MiB from 89 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:56:10,336 - distributed.worker.memory - WARNING - gc.collect() took 1.172s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:56:27,667 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:56:27,726 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.18 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:56:27,739 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:56:27,840 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:56:27,940 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:56:28,020 - distributed.utils_perf - INFO - full garbage collection released 311.33 MiB from 57 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:56:28,111 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:56:28,111 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.85 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:56:28,139 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB

⏱️  Year 1984 processed in 123.7 seconds (2.1 minutes)

==================================================
Processing year 1985
==================================================
Loading weather data for year 1985...
Found 68 files for year 1985
Loaded data: 32,080,054 rows × 4 columns
Transforming to wide format...
Created wide format: 101,567 station-year-measurement combinations × 368 columns
Saved to data/weather_1985_wide.parquet

⏱️  Year 1985 processed in 119.6 seconds (2.0 minutes)

==================================================
Processing year 1986
==================================================
Loading weather data for year 1986...
Found 67 files for year 1986
Loaded data: 31,863,484 rows × 4 columns
Transforming to wide format...
Created wide format: 100,733 station-year-measurement combinations × 368 columns
Saved to data/weather_1986_wide.parquet

⏱️  Year 1986 processed in 121.2 seconds (2.0 minutes)

==================================================
Processing year 1987
==================================================
Loading weather data for year 1987...
Found 67 files for year 1987
Loaded data: 31,907,550 rows × 4 columns
Transforming to wide format...
Created wide format: 100,782 station-year-measurement combinations × 368 columns
Saved to data/weather_1987_wide.parquet

⏱️  Year 1987 processed in 116.5 seconds (1.9 minutes)

==================================================
Processing year 1988
==================================================
Loading weather data for year 1988...
Found 67 files for year 1988
Loaded data: 32,037,772 rows × 4 columns
Transforming to wide format...
Created wide format: 100,635 station-year-measurement combinations × 368 columns
Saved to data/weather_1988_wide.parquet

⏱️  Year 1988 processed in 122.8 seconds (2.0 minutes)

==================================================
Processing year 1989
==================================================
Loading weather data for year 1989...
Found 67 files for year 1989
Loaded data: 32,053,104 rows × 4 columns
Transforming to wide format...
Created wide format: 100,549 station-year-measurement combinations × 368 columns
Saved to data/weather_1989_wide.parquet

⏱️  Year 1989 processed in 115.2 seconds (1.9 minutes)

==================================================
Processing year 1990
==================================================
Loading weather data for year 1990...
Found 67 files for year 1990
Loaded data: 32,099,322 rows × 4 columns
Transforming to wide format...
Created wide format: 100,852 station-year-measurement combinations × 368 columns
Saved to data/weather_1990_wide.parquet

⏱️  Year 1990 processed in 118.4 seconds (2.0 minutes)

==================================================
Processing year 1991
==================================================
Loading weather data for year 1991...
Found 66 files for year 1991
Loaded data: 32,296,429 rows × 4 columns
Transforming to wide format...
Created wide format: 101,122 station-year-measurement combinations × 368 columns
Saved to data/weather_1991_wide.parquet

⏱️  Year 1991 processed in 118.0 seconds (2.0 minutes)

==================================================
Processing year 1992
==================================================
Loading weather data for year 1992...
Found 67 files for year 1992
Loaded data: 32,261,700 rows × 4 columns
Transforming to wide format...
Created wide format: 100,378 station-year-measurement combinations × 368 columns
Saved to data/weather_1992_wide.parquet

⏱️  Year 1992 processed in 117.9 seconds (2.0 minutes)

==================================================
Processing year 1993
==================================================
Loading weather data for year 1993...
Found 66 files for year 1993
Loaded data: 31,943,598 rows × 4 columns
Transforming to wide format...
Created wide format: 100,013 station-year-measurement combinations × 368 columns
Saved to data/weather_1993_wide.parquet

⏱️  Year 1993 processed in 119.2 seconds (2.0 minutes)

==================================================
Processing year 1994
==================================================
Loading weather data for year 1994...
Found 65 files for year 1994
Loaded data: 31,767,767 rows × 4 columns
Transforming to wide format...
Created wide format: 99,546 station-year-measurement combinations × 368 columns
Saved to data/weather_1994_wide.parquet

⏱️  Year 1994 processed in 115.2 seconds (1.9 minutes)

==================================================
Processing year 1995
==================================================
Loading weather data for year 1995...
Found 66 files for year 1995
Loaded data: 31,601,823 rows × 4 columns
Transforming to wide format...
Created wide format: 98,955 station-year-measurement combinations × 368 columns
Saved to data/weather_1995_wide.parquet

⏱️  Year 1995 processed in 113.9 seconds (1.9 minutes)

==================================================
Processing year 1996
==================================================
Loading weather data for year 1996...
Found 66 files for year 1996
Loaded data: 31,667,991 rows × 4 columns
Transforming to wide format...
Created wide format: 98,617 station-year-measurement combinations × 368 columns
Saved to data/weather_1996_wide.parquet

⏱️  Year 1996 processed in 116.1 seconds (1.9 minutes)

==================================================
Processing year 1997
==================================================
Loading weather data for year 1997...
Found 67 files for year 1997
Loaded data: 31,523,476 rows × 4 columns
Transforming to wide format...
Created wide format: 98,957 station-year-measurement combinations × 368 columns
Saved to data/weather_1997_wide.parquet

⏱️  Year 1997 processed in 117.1 seconds (2.0 minutes)

==================================================
Processing year 1998
==================================================
Loading weather data for year 1998...
Found 66 files for year 1998
Loaded data: 31,302,678 rows × 4 columns
Transforming to wide format...
Created wide format: 99,929 station-year-measurement combinations × 368 columns
Saved to data/weather_1998_wide.parquet

⏱️  Year 1998 processed in 110.6 seconds (1.8 minutes)

==================================================
Processing year 1999
==================================================
Loading weather data for year 1999...
Found 66 files for year 1999
Loaded data: 31,480,282 rows × 4 columns
Transforming to wide format...
Created wide format: 99,070 station-year-measurement combinations × 368 columns
Saved to data/weather_1999_wide.parquet

⏱️  Year 1999 processed in 109.4 seconds (1.8 minutes)

==================================================
Processing year 2000
==================================================
Loading weather data for year 2000...
Found 65 files for year 2000
Loaded data: 31,668,064 rows × 4 columns
Transforming to wide format...
Created wide format: 100,117 station-year-measurement combinations × 368 columns
Saved to data/weather_2000_wide.parquet

⏱️  Year 2000 processed in 108.0 seconds (1.8 minutes)

==================================================
Processing year 2001
==================================================
Loading weather data for year 2001...
Found 65 files for year 2001
Loaded data: 31,793,042 rows × 4 columns
Transforming to wide format...
Created wide format: 100,524 station-year-measurement combinations × 368 columns
Saved to data/weather_2001_wide.parquet

⏱️  Year 2001 processed in 111.8 seconds (1.9 minutes)

==================================================
Processing year 2002
==================================================
Loading weather data for year 2002...
Found 67 files for year 2002
Loaded data: 32,130,595 rows × 4 columns
Transforming to wide format...
Created wide format: 102,029 station-year-measurement combinations × 368 columns
Saved to data/weather_2002_wide.parquet

⏱️  Year 2002 processed in 110.3 seconds (1.8 minutes)

==================================================
Processing year 2003
2025-10-19 18:57:24,098 - distributed.utils_perf - INFO - full garbage collection released 664.04 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:57:24,100 - distributed.worker.memory - WARNING - gc.collect() took 1.161s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:57:24,101 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 18:57:39,519 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:57:39,642 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.19 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:57:39,698 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:57:39,798 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:57:39,899 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:57:39,899 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:57:40,563 - distributed.utils_perf - INFO - full garbage collection released 525.71 MiB from 100 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:57:40,563 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:57:40,617 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:58:02,895 - distributed.utils_perf - INFO - full garbage collection released 236.15 MiB from 112 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:58:04,089 - distributed.utils_perf - INFO - full garbage collection released 612.01 MiB from 73 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:58:04,090 - distributed.worker.memory - WARNING - gc.collect() took 1.158s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:58:09,362 - distributed.utils_perf - INFO - full garbage collection released 9.56 MiB from 162 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:58:10,084 - distributed.utils_perf - INFO - full garbage collection released 23.34 MiB from 121 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:58:11,056 - distributed.utils_perf - INFO - full garbage collection released 151.55 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:58:11,056 - distributed.worker.memory - WARNING - gc.collect() took 1.210s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:58:24,160 - distributed.utils_perf - INFO - full garbage collection released 414.60 MiB from 56 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:58:24,160 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:58:24,190 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:58:24,197 - distributed.utils_perf - INFO - full garbage collection released 472.50 MiB from 113 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:58:24,197 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:58:24,253 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:59:21,206 - distributed.worker.memory - WARNING - gc.collect() took 1.150s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 18:59:34,098 - distributed.utils_perf - INFO - full garbage collection released 236.78 MiB from 191 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:59:34,189 - distributed.utils_perf - INFO - full garbage collection released 154.38 MiB from 76 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:59:39,306 - distributed.utils_perf - INFO - full garbage collection released 501.63 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:59:39,331 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:59:39,353 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:59:39,447 - distributed.utils_perf - INFO - full garbage collection released 354.28 MiB from 76 reference cycles (threshold: 9.54 MiB)
2025-10-19 18:59:40,829 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 18:59:40,853 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:00:03,667 - distributed.utils_perf - INFO - full garbage collection released 247.19 MiB from 173 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:00:03,667 - distributed.worker.memory - WARNING - gc.collect() took 1.172s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:00:07,060 - distributed.utils_perf - INFO - full garbage collection released 239.15 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:00:24,345 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:00:24,473 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:00:24,501 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:00:24,553 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:00:25,352 - distributed.utils_perf - INFO - full garbage collection released 423.25 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:00:25,352 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:00:25,406 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:01:14,418 - distributed.utils_perf - INFO - full garbage collection released 237.58 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:01:14,418 - distributed.worker.memory - WARNING - gc.collect() took 1.641s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:01:25,898 - distributed.utils_perf - INFO - full garbage collection released 251.95 MiB from 188 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:01:26,533 - distributed.utils_perf - INFO - full garbage collection released 163.11 MiB from 88 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:01:34,753 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:01:34,884 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:01:34,910 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.75 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:01:35,873 - distributed.utils_perf - INFO - full garbage collection released 407.56 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:01:35,873 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:01:35,929 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:01:35,991 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.88 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:01:58,805 - distributed.utils_perf - INFO - full garbage collection released 237.20 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:01:58,805 - distributed.worker.memory - WARNING - gc.collect() took 1.394s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:02:01,850 - distributed.utils_perf - INFO - full garbage collection released 238.41 MiB from 51 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:02:01,850 - distributed.worker.memory - WARNING - gc.collect() took 1.487s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:02:21,457 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.15 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:02:21,488 - distributed.utils_perf - INFO - full garbage collection released 566.67 MiB from 95 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:02:21,515 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:02:21,519 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.23 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:02:21,553 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:02:21,558 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:02:21,653 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:02:21,653 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:02:23,035 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:02:23,057 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:02:56,029 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:03:17,107 - distributed.utils_perf - INFO - full garbage collection released 244.52 MiB from 32 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:03:17,107 - distributed.worker.memory - WARNING - gc.collect() took 1.537s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:03:22,208 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:03:27,903 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:03:31,028 - distributed.utils_perf - INFO - full garbage collection released 165.50 MiB from 145 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:03:36,352 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 205 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:03:36,769 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:03:36,899 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:03:36,939 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:03:37,039 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:03:37,039 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:03:37,287 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:03:37,346 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.59 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:03:37,418 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:03:38,867 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:04:01,291 - distributed.utils_perf - INFO - full garbage collection released 238.04 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:04:01,291 - distributed.worker.memory - WARNING - gc.collect() took 1.643s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:04:05,076 - distributed.utils_perf - INFO - full garbage collection released 244.69 MiB from 50 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:04:05,076 - distributed.worker.memory - WARNING - gc.collect() took 1.078s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:04:10,561 - distributed.utils_perf - INFO - full garbage collection released 341.16 MiB from 18 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:04:10,561 - distributed.worker.memory - WARNING - gc.collect() took 1.536s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:04:22,903 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:04:22,951 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:04:22,988 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:04:23,087 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:04:23,187 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.42 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:04:23,287 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:04:23,287 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:04:23,586 - distributed.utils_perf - INFO - full garbage collection released 418.05 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:04:23,677 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.79 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:04:24,857 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:04:27,868 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:05:05,319 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:05:15,309 - distributed.utils_perf - INFO - full garbage collection released 16.26 MiB from 212 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:05:21,181 - distributed.utils_perf - INFO - full garbage collection released 627.58 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:05:21,181 - distributed.worker.memory - WARNING - gc.collect() took 1.491s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:05:21,458 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:05:31,821 - distributed.utils_perf - INFO - full garbage collection released 265.83 MiB from 97 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:05:31,821 - distributed.worker.memory - WARNING - gc.collect() took 1.067s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:05:36,662 - distributed.utils_perf - INFO - full garbage collection released 168.28 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:05:36,663 - distributed.worker.memory - WARNING - gc.collect() took 1.506s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:05:38,263 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:05:43,777 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44085 (pid=1457383) exceeded 95% memory budget. Restarting...
2025-10-19 19:05:43,883 - distributed.nanny - INFO - Worker process 1457383 was killed by signal 15
2025-10-19 19:05:43,886 - distributed.core - INFO - Connection to tcp://127.0.0.1:58014 has been closed.
2025-10-19 19:05:43,886 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44085', name: 18, status: running, memory: 8, processing: 1>
2025-10-19 19:05:43,886 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44085
2025-10-19 19:05:43,889 - distributed.nanny - WARNING - Restarting worker
==================================================
Loading weather data for year 2003...
Found 69 files for year 2003
Loaded data: 32,775,394 rows × 4 columns
Transforming to wide format...
Created wide format: 103,627 station-year-measurement combinations × 368 columns
Saved to data/weather_2003_wide.parquet

⏱️  Year 2003 processed in 115.7 seconds (1.9 minutes)

==================================================
Processing year 2004
==================================================
Loading weather data for year 2004...
Found 71 files for year 2004
Loaded data: 33,361,686 rows × 4 columns
Transforming to wide format...
Created wide format: 106,801 station-year-measurement combinations × 368 columns
Saved to data/weather_2004_wide.parquet

⏱️  Year 2004 processed in 121.4 seconds (2.0 minutes)

==================================================
Processing year 2005
==================================================
Loading weather data for year 2005...
Found 71 files for year 2005
Loaded data: 33,022,426 rows × 4 columns
Transforming to wide format...
Created wide format: 107,124 station-year-measurement combinations × 368 columns
Saved to data/weather_2005_wide.parquet

⏱️  Year 2005 processed in 116.5 seconds (1.9 minutes)

==================================================
Processing year 2006
==================================================
Loading weather data for year 2006...
Found 70 files for year 2006
Loaded data: 33,254,122 rows × 4 columns
Transforming to wide format...
Created wide format: 108,786 station-year-measurement combinations × 368 columns
Saved to data/weather_2006_wide.parquet

⏱️  Year 2006 processed in 123.2 seconds (2.1 minutes)

==================================================
Processing year 2007
==================================================
Loading weather data for year 2007...
Found 70 files for year 2007
Loaded data: 33,671,174 rows × 4 columns
Transforming to wide format...
2025-10-19 19:05:44,050 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:05:44,076 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:34505 (pid=1457365) exceeded 95% memory budget. Restarting...
2025-10-19 19:05:44,170 - distributed.core - INFO - Connection to tcp://127.0.0.1:57994 has been closed.
2025-10-19 19:05:44,170 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34505', name: 12, status: running, memory: 8, processing: 1>
2025-10-19 19:05:44,170 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34505
2025-10-19 19:05:44,178 - distributed.nanny - INFO - Worker process 1457365 was killed by signal 15
2025-10-19 19:05:44,188 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35521 instead
  warnings.warn(
2025-10-19 19:05:44,545 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44061
2025-10-19 19:05:44,545 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44061
2025-10-19 19:05:44,545 - distributed.worker - INFO -           Worker name:                         18
2025-10-19 19:05:44,545 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35521
2025-10-19 19:05:44,545 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:05:44,545 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:05:44,545 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:05:44,545 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:05:44,545 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-npns7k8m
2025-10-19 19:05:44,545 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:05:44,548 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44061', name: 18, status: init, memory: 0, processing: 0>
2025-10-19 19:05:44,548 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44061
2025-10-19 19:05:44,548 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49772
2025-10-19 19:05:44,548 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:05:44,548 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:05:44,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35981 instead
  warnings.warn(
2025-10-19 19:05:44,849 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41189
2025-10-19 19:05:44,849 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41189
2025-10-19 19:05:44,849 - distributed.worker - INFO -           Worker name:                         12
2025-10-19 19:05:44,849 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35981
2025-10-19 19:05:44,849 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:05:44,849 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:05:44,849 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:05:44,849 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:05:44,849 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y6utuzz5
2025-10-19 19:05:44,849 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:05:44,851 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41189', name: 12, status: init, memory: 0, processing: 0>
2025-10-19 19:05:44,852 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41189
2025-10-19 19:05:44,852 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49784
2025-10-19 19:05:44,852 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:05:44,852 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:05:44,852 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:06:14,237 - distributed.worker.memory - WARNING - gc.collect() took 1.480s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:06:14,238 - distributed.worker.memory - WARNING - Worker is at 94% memory usage. Pausing worker.  Process memory: 3.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:14,277 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45905 (pid=1457349) exceeded 95% memory budget. Restarting...
2025-10-19 19:06:14,478 - distributed.core - INFO - Connection to tcp://127.0.0.1:58006 has been closed.
2025-10-19 19:06:14,478 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45905', name: 7, status: running, memory: 9, processing: 1>
2025-10-19 19:06:14,478 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45905
2025-10-19 19:06:14,492 - distributed.nanny - INFO - Worker process 1457349 was killed by signal 15
2025-10-19 19:06:14,515 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45683 instead
  warnings.warn(
2025-10-19 19:06:15,533 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41407
2025-10-19 19:06:15,533 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41407
2025-10-19 19:06:15,533 - distributed.worker - INFO -           Worker name:                          7
2025-10-19 19:06:15,533 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45683
2025-10-19 19:06:15,533 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:06:15,533 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:06:15,533 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:06:15,533 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:06:15,533 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hv9gji3t
2025-10-19 19:06:15,533 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:06:15,539 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41407', name: 7, status: init, memory: 0, processing: 0>
2025-10-19 19:06:15,539 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41407
2025-10-19 19:06:15,540 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46606
2025-10-19 19:06:15,540 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:06:15,540 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:06:15,541 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:06:40,164 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:40,239 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:40,445 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.19 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:40,469 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:40,539 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.28 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:40,639 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:40,740 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.29 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:40,840 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:40,940 - distributed.worker.memory - WARNING - Worker is at 73% memory usage. Resuming worker. Process memory: 2.75 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:40,940 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.75 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:41,040 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.92 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:41,904 - distributed.utils_perf - INFO - full garbage collection released 258.19 MiB from 76 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:06:42,071 - distributed.utils_perf - INFO - full garbage collection released 243.24 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:06:44,113 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:44,229 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.29 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:44,275 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:44,374 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:44,476 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.59 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:06:44,476 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46037 (pid=1457371) exceeded 95% memory budget. Restarting...
2025-10-19 19:06:44,571 - distributed.nanny - INFO - Worker process 1457371 was killed by signal 15
2025-10-19 19:06:44,575 - distributed.core - INFO - Connection to tcp://127.0.0.1:58046 has been closed.
2025-10-19 19:06:44,575 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46037', name: 14, status: paused, memory: 8, processing: 1>
2025-10-19 19:06:44,575 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46037
2025-10-19 19:06:44,588 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42525 instead
  warnings.warn(
2025-10-19 19:06:45,244 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44469
2025-10-19 19:06:45,245 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44469
2025-10-19 19:06:45,245 - distributed.worker - INFO -           Worker name:                         14
2025-10-19 19:06:45,245 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42525
2025-10-19 19:06:45,245 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:06:45,245 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:06:45,245 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:06:45,245 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:06:45,245 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_dw5ukwr
2025-10-19 19:06:45,245 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:06:45,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44469', name: 14, status: init, memory: 0, processing: 0>
2025-10-19 19:06:45,247 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44469
2025-10-19 19:06:45,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45220
2025-10-19 19:06:45,247 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:06:45,247 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:06:45,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:07:13,286 - distributed.utils_perf - INFO - full garbage collection released 245.73 MiB from 207 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:07:13,336 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:07:13,574 - distributed.utils_perf - INFO - full garbage collection released 254.38 MiB from 63 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:07:15,783 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:07:16,176 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:32829 (pid=1457377) exceeded 95% memory budget. Restarting...
2025-10-19 19:07:16,270 - distributed.core - INFO - Connection to tcp://127.0.0.1:58022 has been closed.
2025-10-19 19:07:16,271 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32829', name: 16, status: running, memory: 8, processing: 1>
2025-10-19 19:07:16,271 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32829
2025-10-19 19:07:16,271 - distributed.scheduler - INFO - Task ('pivot_table_count-combine-35bf6de98385c9a59f0275953fcdc330', 0, 2) marked as failed because 3 workers died while trying to run it
2025-10-19 19:07:16,271 - distributed.nanny - INFO - Worker process 1457377 was killed by signal 15
2025-10-19 19:07:16,276 - distributed.nanny - WARNING - Restarting worker
Error processing year 2007: Attempted to run task ('pivot_table_count-combine-35bf6de98385c9a59f0275953fcdc330', 0, 2) on 3 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:32829. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.
⏱️  Failed after 167.1 seconds (2.8 minutes)

==================================================
Processing year 2008
==================================================
Loading weather data for year 2008...
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33963 instead
  warnings.warn(
2025-10-19 19:07:16,925 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44099
2025-10-19 19:07:16,925 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44099
2025-10-19 19:07:16,925 - distributed.worker - INFO -           Worker name:                         16
2025-10-19 19:07:16,925 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33963
2025-10-19 19:07:16,925 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:07:16,925 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:07:16,925 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:07:16,925 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:07:16,925 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0v79tojd
2025-10-19 19:07:16,925 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:07:16,928 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44099', name: 16, status: init, memory: 0, processing: 0>
2025-10-19 19:07:16,928 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44099
2025-10-19 19:07:16,928 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38046
2025-10-19 19:07:16,928 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:07:16,928 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:07:16,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:08:01,492 - distributed.utils_perf - INFO - full garbage collection released 19.08 MiB from 229 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:08:20,828 - distributed.utils_perf - INFO - full garbage collection released 181.26 MiB from 57 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:08:20,828 - distributed.worker.memory - WARNING - gc.collect() took 1.031s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:08:21,091 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:08:22,264 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:08:25,416 - distributed.utils_perf - INFO - full garbage collection released 409.15 MiB from 134 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:08:25,416 - distributed.worker.memory - WARNING - gc.collect() took 1.312s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:08:28,008 - distributed.utils_perf - INFO - full garbage collection released 186.33 MiB from 37 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:08:32,514 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:08:32,653 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:08:32,697 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.28 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:08:32,797 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.42 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:08:32,898 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:08:32,925 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46073 (pid=1457348) exceeded 95% memory budget. Restarting...
2025-10-19 19:08:33,025 - distributed.nanny - INFO - Worker process 1457348 was killed by signal 15
2025-10-19 19:08:33,031 - distributed.core - INFO - Connection to tcp://127.0.0.1:57906 has been closed.
2025-10-19 19:08:33,031 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46073', name: 6, status: paused, memory: 8, processing: 1>
2025-10-19 19:08:33,031 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46073
2025-10-19 19:08:33,053 - distributed.nanny - WARNING - Restarting worker
Found 73 files for year 2008
Loaded data: 35,153,121 rows × 4 columns
Transforming to wide format...
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33929 instead
  warnings.warn(
2025-10-19 19:08:33,722 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33109
2025-10-19 19:08:33,722 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33109
2025-10-19 19:08:33,722 - distributed.worker - INFO -           Worker name:                          6
2025-10-19 19:08:33,722 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33929
2025-10-19 19:08:33,722 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:08:33,722 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:08:33,722 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:08:33,722 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:08:33,722 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a95vj6y9
2025-10-19 19:08:33,722 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:08:33,725 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33109', name: 6, status: init, memory: 0, processing: 0>
2025-10-19 19:08:33,725 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33109
2025-10-19 19:08:33,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56794
2025-10-19 19:08:33,725 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:08:33,725 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:08:33,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:08:43,030 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:09:00,017 - distributed.utils_perf - INFO - full garbage collection released 14.82 MiB from 201 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:09:01,641 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.98 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:01,649 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:01,649 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:01,758 - distributed.worker.memory - WARNING - Worker is at 55% memory usage. Resuming worker. Process memory: 2.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:27,807 - distributed.utils_perf - INFO - full garbage collection released 271.29 MiB from 79 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:09:27,807 - distributed.worker.memory - WARNING - gc.collect() took 1.807s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:09:28,429 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:09:32,768 - distributed.worker.memory - WARNING - gc.collect() took 1.223s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:09:32,964 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.88 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:32,986 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.79 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:33,086 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.79 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:33,185 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.79 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:39,432 - distributed.utils_perf - INFO - full garbage collection released 410.69 MiB from 217 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:09:39,433 - distributed.worker.memory - WARNING - gc.collect() took 1.302s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:09:41,967 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:09:52,729 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:52,843 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:52,885 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.19 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:52,985 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.33 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:53,085 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:53,184 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:53,285 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:54,723 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:54,723 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:54,784 - distributed.worker.memory - WARNING - Worker is at 58% memory usage. Resuming worker. Process memory: 2.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:09:57,530 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:10:17,480 - distributed.utils_perf - INFO - full garbage collection released 10.78 MiB from 346 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:10:52,906 - distributed.utils_perf - INFO - full garbage collection released 289.59 MiB from 177 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:10:52,906 - distributed.worker.memory - WARNING - gc.collect() took 1.215s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:10:52,906 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:10:53,234 - distributed.worker.memory - WARNING - Worker is at 56% memory usage. Resuming worker. Process memory: 2.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:10:53,980 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:10:57,738 - distributed.worker.memory - WARNING - gc.collect() took 1.306s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:10:57,923 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:11:10,991 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:11:14,398 - distributed.worker.memory - WARNING - gc.collect() took 1.077s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:11:16,189 - distributed.utils_perf - INFO - full garbage collection released 18.88 MiB from 235 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:11:17,992 - distributed.utils_perf - INFO - full garbage collection released 431.08 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:11:18,370 - distributed.utils_perf - INFO - full garbage collection released 193.07 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:11:18,370 - distributed.worker.memory - WARNING - gc.collect() took 1.000s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:11:21,552 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:21,863 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:21,874 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46857 (pid=1457334) exceeded 95% memory budget. Restarting...
2025-10-19 19:11:21,967 - distributed.core - INFO - Connection to tcp://127.0.0.1:57936 has been closed.
2025-10-19 19:11:21,967 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46857', name: 2, status: paused, memory: 8, processing: 1>
2025-10-19 19:11:21,967 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46857
2025-10-19 19:11:21,968 - distributed.nanny - INFO - Worker process 1457334 was killed by signal 15
2025-10-19 19:11:21,974 - distributed.nanny - WARNING - Restarting worker
Created wide format: 121,949 station-year-measurement combinations × 368 columns
Saved to data/weather_2008_wide.parquet

⏱️  Year 2008 processed in 162.7 seconds (2.7 minutes)

==================================================
Processing year 2009
==================================================
Loading weather data for year 2009...
Found 74 files for year 2009
Loaded data: 35,408,923 rows × 4 columns
Transforming to wide format...
2025-10-19 19:11:22,133 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:22,393 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:22,407 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.33 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:22,507 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.48 GiB -- Worker memory limit: 3.73 GiB
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46545 instead
  warnings.warn(
2025-10-19 19:11:22,608 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:22,651 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37387
2025-10-19 19:11:22,651 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37387
2025-10-19 19:11:22,651 - distributed.worker - INFO -           Worker name:                          2
2025-10-19 19:11:22,651 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46545
2025-10-19 19:11:22,651 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:11:22,651 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:22,651 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:11:22,651 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:11:22,651 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6fl12bm7
2025-10-19 19:11:22,651 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:22,653 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37387', name: 2, status: init, memory: 0, processing: 0>
2025-10-19 19:11:22,654 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37387
2025-10-19 19:11:22,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49456
2025-10-19 19:11:22,654 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:11:22,654 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:22,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:11:22,707 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 2.97 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:22,707 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.97 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:25,036 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:28,464 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.86 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:28,495 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.86 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:30,614 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:30,794 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.44 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:30,896 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:30,926 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:32831 (pid=1457380) exceeded 95% memory budget. Restarting...
2025-10-19 19:11:31,017 - distributed.nanny - INFO - Worker process 1457380 was killed by signal 15
2025-10-19 19:11:31,022 - distributed.core - INFO - Connection to tcp://127.0.0.1:57966 has been closed.
2025-10-19 19:11:31,023 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32831', name: 17, status: paused, memory: 8, processing: 1>
2025-10-19 19:11:31,023 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32831
2025-10-19 19:11:31,029 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39149 instead
  warnings.warn(
2025-10-19 19:11:31,681 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35503
2025-10-19 19:11:31,681 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35503
2025-10-19 19:11:31,681 - distributed.worker - INFO -           Worker name:                         17
2025-10-19 19:11:31,681 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39149
2025-10-19 19:11:31,681 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:11:31,681 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:31,681 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:11:31,681 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:11:31,681 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0mqdlflw
2025-10-19 19:11:31,681 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:31,683 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35503', name: 17, status: init, memory: 0, processing: 0>
2025-10-19 19:11:31,683 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35503
2025-10-19 19:11:31,683 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52376
2025-10-19 19:11:31,683 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:11:31,683 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:31,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:11:42,058 - distributed.utils_perf - INFO - full garbage collection released 428.34 MiB from 7 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:11:42,085 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:43,820 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 3.25 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:43,983 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:44,027 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38101 (pid=1457356) exceeded 95% memory budget. Restarting...
2025-10-19 19:11:44,114 - distributed.core - INFO - Connection to tcp://127.0.0.1:58052 has been closed.
2025-10-19 19:11:44,115 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38101', name: 9, status: paused, memory: 8, processing: 1>
2025-10-19 19:11:44,115 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38101
2025-10-19 19:11:44,118 - distributed.nanny - INFO - Worker process 1457356 was killed by signal 15
2025-10-19 19:11:44,125 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40095 instead
  warnings.warn(
2025-10-19 19:11:44,778 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41707
2025-10-19 19:11:44,778 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41707
2025-10-19 19:11:44,778 - distributed.worker - INFO -           Worker name:                          9
2025-10-19 19:11:44,778 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40095
2025-10-19 19:11:44,778 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:11:44,778 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:44,778 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:11:44,778 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:11:44,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f4gyzel0
2025-10-19 19:11:44,779 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:44,782 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41707', name: 9, status: init, memory: 0, processing: 0>
2025-10-19 19:11:44,782 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41707
2025-10-19 19:11:44,782 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36208
2025-10-19 19:11:44,782 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:11:44,782 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:44,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:11:52,310 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.88 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:52,341 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.88 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:54,171 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:54,349 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:54,441 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:11:54,474 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36919 (pid=1457331) exceeded 95% memory budget. Restarting...
2025-10-19 19:11:54,569 - distributed.core - INFO - Connection to tcp://127.0.0.1:57888 has been closed.
2025-10-19 19:11:54,570 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36919', name: 1, status: paused, memory: 8, processing: 1>
2025-10-19 19:11:54,570 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36919
2025-10-19 19:11:54,570 - distributed.scheduler - INFO - Task ('pivot_table_count-combine-f6cd7603734a66f47869ed2a2c712615', 0, 2) marked as failed because 3 workers died while trying to run it
2025-10-19 19:11:54,570 - distributed.nanny - INFO - Worker process 1457331 was killed by signal 15
2025-10-19 19:11:54,576 - distributed.nanny - WARNING - Restarting worker
Error processing year 2009: Attempted to run task ('pivot_table_count-combine-f6cd7603734a66f47869ed2a2c712615', 0, 2) on 3 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:36919. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.
⏱️  Failed after 115.6 seconds (1.9 minutes)

==================================================
Processing year 2010
==================================================
Loading weather data for year 2010...
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43157 instead
  warnings.warn(
2025-10-19 19:11:55,232 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33811
2025-10-19 19:11:55,232 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33811
2025-10-19 19:11:55,232 - distributed.worker - INFO -           Worker name:                          1
2025-10-19 19:11:55,232 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43157
2025-10-19 19:11:55,232 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:11:55,232 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:55,232 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:11:55,232 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:11:55,232 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s6iwhtzk
2025-10-19 19:11:55,232 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:55,234 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33811', name: 1, status: init, memory: 0, processing: 0>
2025-10-19 19:11:55,235 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33811
2025-10-19 19:11:55,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48764
2025-10-19 19:11:55,235 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:11:55,235 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:11:55,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:12:41,326 - distributed.utils_perf - INFO - full garbage collection released 21.04 MiB from 162 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:12:44,237 - distributed.utils_perf - INFO - full garbage collection released 10.79 MiB from 111 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:12:54,810 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 207 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:12:57,158 - distributed.utils_perf - INFO - full garbage collection released 208.84 MiB from 153 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:12:57,159 - distributed.worker.memory - WARNING - gc.collect() took 1.717s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:12:57,355 - distributed.utils_perf - INFO - full garbage collection released 26.34 MiB from 183 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:12:57,550 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:12:58,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:12:59,200 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:12:59,239 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:12:59,339 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:12:59,440 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:12:59,539 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:12:59,640 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:12:59,740 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:12:59,839 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:12:59,940 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:00,039 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.91 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:00,154 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:00,553 - distributed.utils_perf - INFO - full garbage collection released 298.36 MiB from 158 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:13:00,553 - distributed.worker.memory - WARNING - gc.collect() took 1.165s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:13:11,937 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:12,033 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:12,085 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.23 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:12,184 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.29 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:12,285 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 2.35 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:12,412 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:12,452 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:12,535 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:46,106 - distributed.utils_perf - INFO - full garbage collection released 118.88 MiB from 82 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:13:46,106 - distributed.worker.memory - WARNING - gc.collect() took 1.549s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:13:46,469 - distributed.worker.memory - WARNING - gc.collect() took 1.112s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:13:46,721 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:46,842 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:46,853 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.79 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:46,953 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.86 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:47,052 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:47,154 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:47,252 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:47,353 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:47,454 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:47,552 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:47,653 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:47,753 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:47,854 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:47,953 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:13:48,177 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:05,148 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.19 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:05,255 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.41 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:05,291 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:05,326 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36035 (pid=1457337) exceeded 95% memory budget. Restarting...
2025-10-19 19:14:05,415 - distributed.nanny - INFO - Worker process 1457337 was killed by signal 15
2025-10-19 19:14:05,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:57950 has been closed.
2025-10-19 19:14:05,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36035', name: 3, status: paused, memory: 8, processing: 1>
2025-10-19 19:14:05,420 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36035
2025-10-19 19:14:05,423 - distributed.nanny - WARNING - Restarting worker
Found 75 files for year 2010
Loaded data: 35,732,998 rows × 4 columns
Transforming to wide format...
Created wide format: 127,496 station-year-measurement combinations × 368 columns
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43939 instead
  warnings.warn(
2025-10-19 19:14:06,103 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42179
2025-10-19 19:14:06,103 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42179
2025-10-19 19:14:06,103 - distributed.worker - INFO -           Worker name:                          3
2025-10-19 19:14:06,103 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43939
2025-10-19 19:14:06,103 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:14:06,103 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:14:06,103 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:14:06,103 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:14:06,103 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-33yncqz_
2025-10-19 19:14:06,103 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:14:06,106 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42179', name: 3, status: init, memory: 0, processing: 0>
2025-10-19 19:14:06,106 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42179
2025-10-19 19:14:06,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52952
2025-10-19 19:14:06,106 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:14:06,106 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:14:06,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:14:06,196 - distributed.worker.memory - WARNING - Worker is at 93% memory usage. Pausing worker.  Process memory: 3.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:06,273 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44229 (pid=1457343) exceeded 95% memory budget. Restarting...
2025-10-19 19:14:06,373 - distributed.nanny - INFO - Worker process 1457343 was killed by signal 15
2025-10-19 19:14:06,377 - distributed.core - INFO - Connection to tcp://127.0.0.1:57880 has been closed.
2025-10-19 19:14:06,377 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44229', name: 5, status: running, memory: 8, processing: 1>
2025-10-19 19:14:06,377 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44229
2025-10-19 19:14:06,382 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36605 instead
  warnings.warn(
2025-10-19 19:14:07,246 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33015
2025-10-19 19:14:07,246 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33015
2025-10-19 19:14:07,246 - distributed.worker - INFO -           Worker name:                          5
2025-10-19 19:14:07,246 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36605
2025-10-19 19:14:07,246 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:14:07,246 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:14:07,246 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:14:07,246 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:14:07,246 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-itme_sdj
2025-10-19 19:14:07,246 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:14:07,249 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33015', name: 5, status: init, memory: 0, processing: 0>
2025-10-19 19:14:07,250 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33015
2025-10-19 19:14:07,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52966
2025-10-19 19:14:07,250 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:14:07,250 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:14:07,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:14:23,454 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:23,539 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:23,639 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:23,739 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:23,840 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:23,939 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:24,039 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:24,138 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:24,239 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:24,339 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:24,439 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:24,539 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:24,639 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:24,739 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:24,839 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:24,939 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:25,039 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.96 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:25,138 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:29,069 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:29,358 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:29,464 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:29,530 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:29,629 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:29,730 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:29,830 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:29,929 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:30,030 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.18 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:30,171 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:30,230 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:30,329 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.25 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:30,429 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.29 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:30,529 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:30,630 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.32 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:30,730 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.35 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:30,830 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:30,929 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:31,030 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:31,130 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:31,229 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:31,330 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:31,429 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:31,529 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:31,629 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:31,810 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:36,600 - distributed.utils_perf - INFO - full garbage collection released 199.65 MiB from 51 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:14:36,639 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:38,735 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:38,842 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:14:38,936 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 2.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:38,783 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:39,229 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:39,345 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:39,440 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:39,539 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:39,540 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:39,640 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:39,739 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:39,840 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:41,747 - distributed.worker.memory - WARNING - gc.collect() took 1.540s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:15:42,190 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:53,365 - distributed.utils_perf - INFO - full garbage collection released 12.21 MiB from 157 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:15:53,602 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 120 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:15:58,631 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:58,737 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.25 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:58,836 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.32 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:58,935 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 2.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:59,026 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.24 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:59,159 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.50 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:15:59,159 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.50 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:16:25,087 - distributed.utils_perf - INFO - full garbage collection released 684.13 MiB from 217 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:16:25,104 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:16:29,137 - distributed.utils_perf - INFO - full garbage collection released 49.75 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:16:29,137 - distributed.worker.memory - WARNING - gc.collect() took 1.306s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:16:42,992 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:16:43,462 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:16:48,921 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:16:49,080 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:16:49,125 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44957 (pid=1457340) exceeded 95% memory budget. Restarting...
2025-10-19 19:16:49,217 - distributed.core - INFO - Connection to tcp://127.0.0.1:57948 has been closed.
2025-10-19 19:16:49,217 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44957', name: 4, status: paused, memory: 9, processing: 1>
2025-10-19 19:16:49,218 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44957
2025-10-19 19:16:49,219 - distributed.nanny - INFO - Worker process 1457340 was killed by signal 15
2025-10-19 19:16:49,227 - distributed.nanny - WARNING - Restarting worker
Saved to data/weather_2010_wide.parquet

⏱️  Year 2010 processed in 168.6 seconds (2.8 minutes)

==================================================
Processing year 2011
==================================================
Loading weather data for year 2011...
Found 69 files for year 2011
Loaded data: 32,884,921 rows × 4 columns
Transforming to wide format...
Created wide format: 127,581 station-year-measurement combinations × 368 columns
2025-10-19 19:16:49,310 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44957
Traceback (most recent call last):
  File "/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/worker.py", line 2870, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/utils_comm.py", line 402, in retry_operation
    return await retry(
  File "/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/utils_comm.py", line 387, in retry
    return await coro()
  File "/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/worker.py", line 2850, in _get_data
    response = await send_recv(
  File "/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:58076 remote=tcp://127.0.0.1:44957>: Stream is closed
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46727 instead
  warnings.warn(
2025-10-19 19:16:49,877 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41291
2025-10-19 19:16:49,877 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41291
2025-10-19 19:16:49,877 - distributed.worker - INFO -           Worker name:                          4
2025-10-19 19:16:49,877 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46727
2025-10-19 19:16:49,877 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:16:49,877 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:16:49,877 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:16:49,877 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:16:49,877 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bzwbsfeg
2025-10-19 19:16:49,877 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:16:49,880 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41291', name: 4, status: init, memory: 0, processing: 0>
2025-10-19 19:16:49,880 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41291
2025-10-19 19:16:49,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35736
2025-10-19 19:16:49,880 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:16:49,880 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:16:49,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:16:54,404 - distributed.utils_perf - INFO - full garbage collection released 27.95 MiB from 248 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:17:01,694 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:17:01,817 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 2.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:17:01,817 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:17:01,936 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:17:03,885 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:17:03,988 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.18 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:17:04,053 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:17:04,153 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.35 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:17:04,253 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:17:41,922 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:17:53,366 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:18:01,420 - distributed.worker.memory - WARNING - gc.collect() took 1.182s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:18:01,733 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:18:06,304 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:18:16,135 - distributed.utils_perf - INFO - full garbage collection released 203.94 MiB from 134 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:18:16,431 - distributed.utils_perf - INFO - full garbage collection released 46.04 MiB from 84 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:18:17,039 - distributed.utils_perf - INFO - full garbage collection released 711.95 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:18:18,834 - distributed.utils_perf - INFO - full garbage collection released 310.54 MiB from 121 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:18:18,834 - distributed.worker.memory - WARNING - gc.collect() took 1.008s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:18:22,596 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 3.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:18:22,773 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45899 (pid=1457328) exceeded 95% memory budget. Restarting...
2025-10-19 19:18:22,872 - distributed.nanny - INFO - Worker process 1457328 was killed by signal 15
2025-10-19 19:18:22,876 - distributed.core - INFO - Connection to tcp://127.0.0.1:57864 has been closed.
2025-10-19 19:18:22,876 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45899', name: 0, status: running, memory: 8, processing: 1>
2025-10-19 19:18:22,876 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45899
2025-10-19 19:18:22,883 - distributed.nanny - WARNING - Restarting worker
Saved to data/weather_2011_wide.parquet

⏱️  Year 2011 processed in 145.6 seconds (2.4 minutes)

==================================================
Processing year 2012
==================================================
Loading weather data for year 2012...
Found 71 files for year 2012
Loaded data: 32,888,235 rows × 4 columns
Transforming to wide format...
2025-10-19 19:18:23,078 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:18:23,256 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:18:23,257 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:18:23,349 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:18:23,571 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32975
2025-10-19 19:18:23,571 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32975
2025-10-19 19:18:23,571 - distributed.worker - INFO -           Worker name:                          0
2025-10-19 19:18:23,571 - distributed.worker - INFO -          dashboard at:             127.0.0.1:8791
2025-10-19 19:18:23,572 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:18:23,572 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:18:23,572 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:18:23,572 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:18:23,572 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-71wp3odk
2025-10-19 19:18:23,572 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:18:23,574 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32975', name: 0, status: init, memory: 0, processing: 0>
2025-10-19 19:18:23,574 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32975
2025-10-19 19:18:23,574 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37476
2025-10-19 19:18:23,574 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:18:23,574 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:18:23,575 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:18:28,188 - distributed.utils_perf - INFO - full garbage collection released 10.25 MiB from 116 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:18:32,975 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:18:33,064 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.23 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:18:33,081 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.23 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:18:33,183 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 2.24 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:02,540 - distributed.utils_perf - INFO - full garbage collection released 160.31 MiB from 213 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:19:02,540 - distributed.worker.memory - WARNING - gc.collect() took 1.127s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:19:04,635 - distributed.utils_perf - INFO - full garbage collection released 305.71 MiB from 51 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:19:04,635 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:05,146 - distributed.worker.memory - WARNING - Worker is at 50% memory usage. Resuming worker. Process memory: 1.88 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:08,067 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:09,073 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 3.28 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:09,510 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:09,511 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:18,163 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.93 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:18,223 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:18,323 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:18,421 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:18,521 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:18,621 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:18,721 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:18,822 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:18,922 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:24,272 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:24,411 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.35 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:24,466 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.41 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:24,492 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:24,555 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:24,648 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.26 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:24,681 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:19:24,802 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 2.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:23,981 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:25,892 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:29,622 - distributed.utils_perf - INFO - full garbage collection released 299.93 MiB from 89 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:20:29,623 - distributed.worker.memory - WARNING - gc.collect() took 1.084s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:20:38,760 - distributed.utils_perf - INFO - full garbage collection released 468.99 MiB from 59 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:20:42,676 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:42,998 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:43,055 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:43,114 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:43,153 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:43,180 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:43,183 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.26 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:43,282 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.44 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:20:43,382 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:21:32,773 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:21:32,920 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.23 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:21:32,931 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:21:32,976 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.27 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:21:33,073 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:21:33,073 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:21:33,095 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 2.28 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:21:33,153 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:05,761 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:22:07,669 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:22:23,743 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:22:24,841 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 135 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:22:39,216 - distributed.utils_perf - INFO - full garbage collection released 307.70 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:22:45,644 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:46,500 - distributed.utils_perf - INFO - full garbage collection released 298.65 MiB from 119 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:22:46,582 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:48,937 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:49,126 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:49,181 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:49,210 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:49,310 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:49,324 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43321 (pid=1457374) exceeded 95% memory budget. Restarting...
2025-10-19 19:22:49,422 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.41 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:49,431 - distributed.core - INFO - Connection to tcp://127.0.0.1:57922 has been closed.
2025-10-19 19:22:49,431 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43321', name: 15, status: paused, memory: 8, processing: 1>
2025-10-19 19:22:49,431 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43321
2025-10-19 19:22:49,434 - distributed.nanny - INFO - Worker process 1457374 was killed by signal 15
2025-10-19 19:22:49,444 - distributed.nanny - WARNING - Restarting worker
Created wide format: 127,506 station-year-measurement combinations × 368 columns
Saved to data/weather_2012_wide.parquet

⏱️  Year 2012 processed in 140.8 seconds (2.3 minutes)

==================================================
Processing year 2013
==================================================
Loading weather data for year 2013...
Found 68 files for year 2013
Loaded data: 32,024,392 rows × 4 columns
Transforming to wide format...
Created wide format: 127,423 station-year-measurement combinations × 368 columns
Saved to data/weather_2013_wide.parquet

⏱️  Year 2013 processed in 128.0 seconds (2.1 minutes)

==================================================
Processing year 2014
==================================================
Loading weather data for year 2014...
Found 66 files for year 2014
Loaded data: 31,631,162 rows × 4 columns
Transforming to wide format...
2025-10-19 19:22:49,452 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:49,552 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:49,653 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:49,653 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42701 instead
  warnings.warn(
2025-10-19 19:22:50,106 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39151
2025-10-19 19:22:50,106 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39151
2025-10-19 19:22:50,106 - distributed.worker - INFO -           Worker name:                         15
2025-10-19 19:22:50,106 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42701
2025-10-19 19:22:50,106 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:22:50,106 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:22:50,106 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:22:50,106 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:22:50,106 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cdhhc8yl
2025-10-19 19:22:50,106 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:22:50,108 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39151', name: 15, status: init, memory: 0, processing: 0>
2025-10-19 19:22:50,108 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39151
2025-10-19 19:22:50,108 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57018
2025-10-19 19:22:50,108 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:22:50,108 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:22:50,109 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:22:58,802 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:58,864 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.19 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:22:58,940 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 2.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:23:23,501 - distributed.utils_perf - INFO - full garbage collection released 312.11 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:23:25,167 - distributed.utils_perf - INFO - full garbage collection released 9.58 MiB from 125 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:23:46,692 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:23:46,931 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.20 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:23:46,983 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.27 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:23:47,083 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.42 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:23:47,191 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:23:47,282 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:23:47,475 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:23:47,665 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:23:47,767 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:23:47,852 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:24:44,047 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:24:56,992 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:00,640 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:25:02,946 - distributed.utils_perf - INFO - full garbage collection released 306.88 MiB from 75 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:25:02,972 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.58 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:06,284 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:06,416 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:06,560 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.32 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:06,652 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:06,680 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:06,751 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:06,772 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:42425 (pid=1457362) exceeded 95% memory budget. Restarting...
2025-10-19 19:25:06,820 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.41 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:06,881 - distributed.nanny - INFO - Worker process 1457362 was killed by signal 15
2025-10-19 19:25:06,887 - distributed.core - INFO - Connection to tcp://127.0.0.1:57900 has been closed.
2025-10-19 19:25:06,887 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42425', name: 11, status: paused, memory: 8, processing: 1>
2025-10-19 19:25:06,887 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42425
2025-10-19 19:25:06,901 - distributed.nanny - WARNING - Restarting worker
Created wide format: 124,785 station-year-measurement combinations × 368 columns
Saved to data/weather_2014_wide.parquet

⏱️  Year 2014 processed in 134.6 seconds (2.2 minutes)

==================================================
Processing year 2015
==================================================
Loading weather data for year 2015...
Found 67 files for year 2015
Loaded data: 31,806,902 rows × 4 columns
Transforming to wide format...
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33693 instead
  warnings.warn(
2025-10-19 19:25:07,564 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43641
2025-10-19 19:25:07,564 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43641
2025-10-19 19:25:07,564 - distributed.worker - INFO -           Worker name:                         11
2025-10-19 19:25:07,564 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33693
2025-10-19 19:25:07,564 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:25:07,564 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:25:07,564 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:25:07,564 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:25:07,564 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cg6k1zbq
2025-10-19 19:25:07,564 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:25:07,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43641', name: 11, status: init, memory: 0, processing: 0>
2025-10-19 19:25:07,566 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43641
2025-10-19 19:25:07,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42428
2025-10-19 19:25:07,566 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:25:07,566 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:25:07,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:25:41,974 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:42,116 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.23 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:42,151 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.41 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:42,251 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:25:42,251 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:07,666 - distributed.utils_perf - INFO - full garbage collection released 53.25 MiB from 94 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:26:11,022 - distributed.worker.memory - WARNING - gc.collect() took 1.385s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:26:11,628 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:26:11,632 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.93 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:11,721 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:11,722 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:11,822 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:11,922 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:12,022 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:12,121 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:12,221 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:12,321 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:12,421 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:12,520 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:12,621 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:12,722 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:12,822 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:12,822 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:18,893 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:21,243 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:21,548 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 2.95 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:21,864 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:28,316 - distributed.utils_perf - INFO - full garbage collection released 745.32 MiB from 57 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:26:31,020 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:31,282 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:31,382 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.33 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:31,482 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.48 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:31,608 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:31,683 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:31,945 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:32,048 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.42 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:32,149 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.58 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:32,248 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:26:33,621 - distributed.utils_perf - INFO - full garbage collection released 1.02 GiB from 56 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:27:20,245 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:27:28,984 - distributed.utils_perf - INFO - full garbage collection released 205.28 MiB from 149 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:27:28,984 - distributed.worker.memory - WARNING - gc.collect() took 1.869s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:27:38,648 - distributed.utils_perf - INFO - full garbage collection released 464.59 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:27:46,119 - distributed.utils_perf - INFO - full garbage collection released 314.95 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:27:48,021 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:27:48,145 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:27:48,304 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.25 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:27:48,375 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.35 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:27:48,475 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:27:48,575 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:27:48,674 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:16,813 - distributed.utils_perf - INFO - full garbage collection released 314.80 MiB from 95 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:28:21,110 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:22,006 - distributed.worker.memory - WARNING - Worker is at 51% memory usage. Resuming worker. Process memory: 1.90 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:37,309 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:37,345 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:37,436 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.28 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:37,467 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:37,508 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.20 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:37,540 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 2.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:37,609 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:37,636 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.77 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:28:37,743 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 2.35 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:29:12,216 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:29:23,897 - distributed.utils_perf - INFO - full garbage collection released 9.66 MiB from 82 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:29:39,523 - distributed.utils_perf - INFO - full garbage collection released 244.65 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:29:45,933 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:29:48,528 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.41 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:29:49,562 - distributed.utils_perf - INFO - full garbage collection released 15.27 MiB from 130 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:29:54,339 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:29:54,706 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:29:54,735 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:29:54,773 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:33811 (pid=1500626) exceeded 95% memory budget. Restarting...
2025-10-19 19:29:54,871 - distributed.core - INFO - Connection to tcp://127.0.0.1:48764 has been closed.
2025-10-19 19:29:54,871 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33811', name: 1, status: paused, memory: 9, processing: 1>
2025-10-19 19:29:54,871 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33811
2025-10-19 19:29:54,881 - distributed.nanny - INFO - Worker process 1500626 was killed by signal 15
2025-10-19 19:29:54,891 - distributed.nanny - WARNING - Restarting worker
Created wide format: 124,361 station-year-measurement combinations × 368 columns
Saved to data/weather_2015_wide.parquet

⏱️  Year 2015 processed in 164.2 seconds (2.7 minutes)

==================================================
Processing year 2016
==================================================
Loading weather data for year 2016...
Found 67 files for year 2016
Loaded data: 31,955,055 rows × 4 columns
Transforming to wide format...
Created wide format: 123,708 station-year-measurement combinations × 368 columns
Saved to data/weather_2016_wide.parquet

⏱️  Year 2016 processed in 125.7 seconds (2.1 minutes)

==================================================
Processing year 2017
==================================================
Loading weather data for year 2017...
Found 67 files for year 2017
Loaded data: 31,697,032 rows × 4 columns
Transforming to wide format...
2025-10-19 19:29:55,034 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:29:55,213 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:29:55,213 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:29:55,281 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.80 GiB -- Worker memory limit: 3.73 GiB
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41723 instead
  warnings.warn(
2025-10-19 19:29:55,568 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42093
2025-10-19 19:29:55,568 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42093
2025-10-19 19:29:55,569 - distributed.worker - INFO -           Worker name:                          1
2025-10-19 19:29:55,569 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41723
2025-10-19 19:29:55,569 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:29:55,569 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:29:55,569 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:29:55,569 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:29:55,569 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-109mng73
2025-10-19 19:29:55,569 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:29:55,571 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42093', name: 1, status: init, memory: 0, processing: 0>
2025-10-19 19:29:55,571 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42093
2025-10-19 19:29:55,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54394
2025-10-19 19:29:55,571 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:29:55,571 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:29:55,572 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:30:25,758 - distributed.utils_perf - INFO - full garbage collection released 216.25 MiB from 157 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:30:28,150 - distributed.utils_perf - INFO - full garbage collection released 465.84 MiB from 83 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:30:29,770 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:30:29,896 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.29 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:30:29,951 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:30:30,051 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:30:30,073 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:33015 (pid=1501583) exceeded 95% memory budget. Restarting...
2025-10-19 19:30:30,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:52966 has been closed.
2025-10-19 19:30:30,160 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33015', name: 5, status: paused, memory: 8, processing: 1>
2025-10-19 19:30:30,160 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33015
2025-10-19 19:30:30,162 - distributed.nanny - INFO - Worker process 1501583 was killed by signal 15
2025-10-19 19:30:30,168 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34681 instead
  warnings.warn(
2025-10-19 19:30:30,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34667
2025-10-19 19:30:30,808 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34667
2025-10-19 19:30:30,808 - distributed.worker - INFO -           Worker name:                          5
2025-10-19 19:30:30,808 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34681
2025-10-19 19:30:30,808 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:30:30,808 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:30:30,808 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:30:30,808 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:30:30,808 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1orlk7g2
2025-10-19 19:30:30,808 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:30:30,810 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34667', name: 5, status: init, memory: 0, processing: 0>
2025-10-19 19:30:30,810 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34667
2025-10-19 19:30:30,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49494
2025-10-19 19:30:30,810 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:30:30,810 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:30:30,811 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:30:36,580 - distributed.utils_perf - INFO - full garbage collection released 483.32 MiB from 175 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:30:38,268 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.18 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:30:38,397 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.31 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:30:38,452 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:30:38,552 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:30:38,576 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:41189 (pid=1498158) exceeded 95% memory budget. Restarting...
2025-10-19 19:30:38,664 - distributed.core - INFO - Connection to tcp://127.0.0.1:49784 has been closed.
2025-10-19 19:30:38,664 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41189', name: 12, status: paused, memory: 8, processing: 1>
2025-10-19 19:30:38,664 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41189
2025-10-19 19:30:38,673 - distributed.nanny - INFO - Worker process 1498158 was killed by signal 15
2025-10-19 19:30:38,683 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38875 instead
  warnings.warn(
2025-10-19 19:30:39,331 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36151
2025-10-19 19:30:39,331 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36151
2025-10-19 19:30:39,331 - distributed.worker - INFO -           Worker name:                         12
2025-10-19 19:30:39,331 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38875
2025-10-19 19:30:39,331 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:30:39,331 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:30:39,331 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:30:39,331 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:30:39,331 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i5j1q4rc
2025-10-19 19:30:39,331 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:30:39,334 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36151', name: 12, status: init, memory: 0, processing: 0>
2025-10-19 19:30:39,334 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36151
2025-10-19 19:30:39,335 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51432
2025-10-19 19:30:39,335 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:30:39,335 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:30:39,335 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:31:11,269 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 3.26 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:31:11,339 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.36 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:31:11,367 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:31:11,467 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.42 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:31:11,566 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:31:39,823 - distributed.utils_perf - INFO - full garbage collection released 184.03 MiB from 128 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:31:39,824 - distributed.worker.memory - WARNING - gc.collect() took 1.433s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:31:40,035 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:31:44,563 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:31:45,101 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:31:45,314 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:00,259 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:00,379 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:00,379 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:00,388 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.26 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:00,524 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35503 (pid=1500494) exceeded 95% memory budget. Restarting...
2025-10-19 19:32:00,526 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.32 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:00,620 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:00,624 - distributed.core - INFO - Connection to tcp://127.0.0.1:52376 has been closed.
2025-10-19 19:32:00,624 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35503', name: 17, status: paused, memory: 8, processing: 1>
2025-10-19 19:32:00,624 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35503
2025-10-19 19:32:00,626 - distributed.nanny - INFO - Worker process 1500494 was killed by signal 15
2025-10-19 19:32:00,634 - distributed.nanny - WARNING - Restarting worker
Created wide format: 123,740 station-year-measurement combinations × 368 columns
2025-10-19 19:32:00,720 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:00,725 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:39495 (pid=1457353) exceeded 95% memory budget. Restarting...
2025-10-19 19:32:00,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:57980 has been closed.
2025-10-19 19:32:00,825 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39495', name: 8, status: paused, memory: 8, processing: 1>
2025-10-19 19:32:00,825 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39495
2025-10-19 19:32:00,826 - distributed.nanny - INFO - Worker process 1457353 was killed by signal 15
2025-10-19 19:32:00,834 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39603 instead
  warnings.warn(
2025-10-19 19:32:01,296 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46681
2025-10-19 19:32:01,296 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46681
2025-10-19 19:32:01,296 - distributed.worker - INFO -           Worker name:                         17
2025-10-19 19:32:01,296 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39603
2025-10-19 19:32:01,296 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:32:01,296 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:32:01,296 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:32:01,296 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:32:01,296 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tsc3lyxe
2025-10-19 19:32:01,296 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:32:01,298 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46681', name: 17, status: init, memory: 0, processing: 0>
2025-10-19 19:32:01,298 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46681
2025-10-19 19:32:01,298 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49902
2025-10-19 19:32:01,298 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:32:01,298 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:32:01,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44563 instead
  warnings.warn(
2025-10-19 19:32:01,468 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39445
2025-10-19 19:32:01,468 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39445
2025-10-19 19:32:01,468 - distributed.worker - INFO -           Worker name:                          8
2025-10-19 19:32:01,468 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44563
2025-10-19 19:32:01,468 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:32:01,468 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:32:01,468 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:32:01,468 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:32:01,468 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1fb_ihfv
2025-10-19 19:32:01,468 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:32:01,470 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39445', name: 8, status: init, memory: 0, processing: 0>
2025-10-19 19:32:01,471 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39445
2025-10-19 19:32:01,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49906
2025-10-19 19:32:01,471 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:32:01,471 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:32:01,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:32:23,917 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.59 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:28,168 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:32:45,835 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:45,986 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:45,987 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:46,103 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:46,109 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:46,326 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:46,440 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:32:46,526 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.73 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:33:43,373 - distributed.utils_perf - INFO - full garbage collection released 14.22 MiB from 128 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:33:55,994 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:33:58,781 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:33:59,039 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:04,589 - distributed.utils_perf - INFO - full garbage collection released 821.71 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:34:05,303 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:05,431 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:05,435 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:05,536 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.33 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:05,636 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:05,736 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:05,835 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:06,370 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:06,538 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:06,547 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:06,648 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:06,746 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:06,746 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:06,847 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.87 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:30,068 - distributed.utils_perf - INFO - full garbage collection released 275.02 MiB from 91 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:34:33,769 - distributed.utils_perf - INFO - full garbage collection released 303.49 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:34:33,769 - distributed.worker.memory - WARNING - gc.collect() took 1.495s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:34:37,886 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:34:50,117 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:50,603 - distributed.worker.memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 2.97 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:51,223 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.33 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:55,036 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:55,309 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.50 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:55,384 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:55,426 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:35353 (pid=1457358) exceeded 95% memory budget. Restarting...
2025-10-19 19:34:55,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:58058 has been closed.
2025-10-19 19:34:55,521 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35353', name: 10, status: paused, memory: 9, processing: 1>
2025-10-19 19:34:55,521 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35353
2025-10-19 19:34:55,525 - distributed.nanny - INFO - Worker process 1457358 was killed by signal 15
2025-10-19 19:34:55,533 - distributed.nanny - WARNING - Restarting worker
Saved to data/weather_2017_wide.parquet

⏱️  Year 2017 processed in 249.0 seconds (4.2 minutes)

==================================================
Processing year 2018
==================================================
Loading weather data for year 2018...
Found 67 files for year 2018
Loaded data: 31,556,309 rows × 4 columns
Transforming to wide format...
Created wide format: 123,887 station-year-measurement combinations × 368 columns
2025-10-19 19:34:55,786 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:55,974 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:55,981 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.41 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:56,082 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:56,083 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33897 instead
  warnings.warn(
2025-10-19 19:34:56,182 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:34:56,217 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37849
2025-10-19 19:34:56,217 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37849
2025-10-19 19:34:56,217 - distributed.worker - INFO -           Worker name:                         10
2025-10-19 19:34:56,217 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33897
2025-10-19 19:34:56,217 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:34:56,217 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:34:56,217 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:34:56,217 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:34:56,217 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-duvjsoab
2025-10-19 19:34:56,217 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:34:56,219 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37849', name: 10, status: init, memory: 0, processing: 0>
2025-10-19 19:34:56,220 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37849
2025-10-19 19:34:56,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59784
2025-10-19 19:34:56,220 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:34:56,220 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:34:56,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:35:14,236 - distributed.utils_perf - INFO - full garbage collection released 316.46 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:35:16,711 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:16,826 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:41707 (pid=1500574) exceeded 95% memory budget. Restarting...
2025-10-19 19:35:16,907 - distributed.core - INFO - Connection to tcp://127.0.0.1:36208 has been closed.
2025-10-19 19:35:16,907 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41707', name: 9, status: running, memory: 9, processing: 1>
2025-10-19 19:35:16,907 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41707
2025-10-19 19:35:16,909 - distributed.nanny - INFO - Worker process 1500574 was killed by signal 15
2025-10-19 19:35:16,913 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34671 instead
  warnings.warn(
2025-10-19 19:35:17,563 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36653
2025-10-19 19:35:17,563 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36653
2025-10-19 19:35:17,563 - distributed.worker - INFO -           Worker name:                          9
2025-10-19 19:35:17,563 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34671
2025-10-19 19:35:17,563 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:35:17,563 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:35:17,563 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:35:17,563 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:35:17,563 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_4gta73k
2025-10-19 19:35:17,563 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:35:17,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36653', name: 9, status: init, memory: 0, processing: 0>
2025-10-19 19:35:17,566 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36653
2025-10-19 19:35:17,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36192
2025-10-19 19:35:17,566 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:35:17,566 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:35:17,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:35:45,625 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:45,734 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.94 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:45,734 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.94 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:46,402 - distributed.utils_perf - INFO - full garbage collection released 305.71 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:35:46,425 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:46,460 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:47,330 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:47,394 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:47,472 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.26 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:47,572 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.36 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:47,672 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 2.38 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:47,773 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:49,533 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.15 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:35:49,667 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:38,471 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:36:39,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:36:58,533 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:58,541 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.07 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:58,577 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:58,659 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:58,989 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:59,060 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:59,178 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:59,260 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:59,390 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:59,459 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:36:59,655 - distributed.utils_perf - INFO - full garbage collection released 316.82 MiB from 0 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:37:00,179 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.48 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:01,546 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:01,752 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.35 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:01,835 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.41 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:01,936 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:02,035 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:02,316 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:02,448 - distributed.worker.memory - WARNING - Worker is at 64% memory usage. Resuming worker. Process memory: 2.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:02,448 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:31,165 - distributed.utils_perf - INFO - full garbage collection released 316.70 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:37:37,191 - distributed.utils_perf - INFO - full garbage collection released 140.34 MiB from 113 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:37:37,191 - distributed.worker.memory - WARNING - gc.collect() took 1.510s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:37:37,316 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:37:38,345 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:38,627 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:49,748 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:49,928 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:49,968 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.20 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:50,193 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 2.32 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:50,512 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:50,614 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 2.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:50,655 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:37:50,711 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:38:24,636 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:38:28,870 - distributed.utils_perf - INFO - full garbage collection released 11.10 MiB from 93 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:38:52,612 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:38:52,711 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:38:52,811 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:38:52,912 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:05,182 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:06,970 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:07,114 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.18 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:07,212 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:07,242 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:07,310 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:07,318 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:07,402 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 2.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:07,412 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:07,512 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:35,438 - distributed.utils_perf - INFO - full garbage collection released 323.14 MiB from 45 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:39:39,098 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:40,106 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:39:40,379 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:52,219 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:52,726 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.62 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:56,392 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:56,425 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:57,402 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:57,532 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:57,849 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 2.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:57,849 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:58,965 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:59,001 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:59,025 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.10 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:59,125 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.27 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:59,224 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.51 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:39:59,324 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.75 GiB -- Worker memory limit: 3.73 GiB
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:415: FutureWarning: pivot_table dropped a column because it failed to aggregate. This behavior is deprecated and will raise in a future version of pandas. Select only the columns that can be aggregated.
  return pd.pivot_table(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:415: FutureWarning: pivot_table dropped a column because it failed to aggregate. This behavior is deprecated and will raise in a future version of pandas. Select only the columns that can be aggregated.
  return pd.pivot_table(
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/methods.py:334: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`
  df[name] = val
2025-10-19 19:40:48,518 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:41:07,216 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:07,489 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:07,551 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.81 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:21,603 - distributed.utils_perf - INFO - full garbage collection released 217.94 MiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:41:23,187 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:25,242 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:25,556 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:28,232 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:28,388 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:28,410 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:28,454 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:28,473 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:37387 (pid=1500305) exceeded 95% memory budget. Restarting...
2025-10-19 19:41:28,569 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.14 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:28,572 - distributed.core - INFO - Connection to tcp://127.0.0.1:49456 has been closed.
2025-10-19 19:41:28,572 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37387', name: 2, status: paused, memory: 8, processing: 1>
2025-10-19 19:41:28,572 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37387
2025-10-19 19:41:28,579 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:28,581 - distributed.nanny - INFO - Worker process 1500305 was killed by signal 15
2025-10-19 19:41:28,590 - distributed.nanny - WARNING - Restarting worker
Saved to data/weather_2018_wide.parquet

⏱️  Year 2018 processed in 182.5 seconds (3.0 minutes)

==================================================
Processing year 2019
==================================================
Loading weather data for year 2019...
Found 67 files for year 2019
Loaded data: 31,239,214 rows × 4 columns
Transforming to wide format...
Created wide format: 122,556 station-year-measurement combinations × 368 columns
Saved to data/weather_2019_wide.parquet

⏱️  Year 2019 processed in 121.4 seconds (2.0 minutes)

==================================================
Processing year 2020
==================================================
Loading weather data for year 2020...
Found 67 files for year 2020
Loaded data: 31,639,347 rows × 4 columns
Transforming to wide format...
Created wide format: 123,642 station-year-measurement combinations × 368 columns
Saved to data/weather_2020_wide.parquet

⏱️  Year 2020 processed in 128.5 seconds (2.1 minutes)

==================================================
Processing year 2021
==================================================
Loading weather data for year 2021...
Warning: No files found for year 2021
Loaded data: 0 rows × 4 columns
Transforming to wide format...
Created wide format: 0 station-year-measurement combinations × 368 columns
Saved to data/weather_2021_wide.parquet

⏱️  Year 2021 processed in 11.1 seconds (0.2 minutes)

==================================================
Processing year 2022
==================================================
Loading weather data for year 2022...
Found 68 files for year 2022
Loaded data: 32,496,340 rows × 4 columns
Transforming to wide format...
2025-10-19 19:41:28,681 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.27 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:28,788 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 2.60 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:41:28,880 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38535 instead
  warnings.warn(
2025-10-19 19:41:29,244 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34371
2025-10-19 19:41:29,244 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34371
2025-10-19 19:41:29,244 - distributed.worker - INFO -           Worker name:                          2
2025-10-19 19:41:29,244 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38535
2025-10-19 19:41:29,244 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:41:29,244 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:41:29,244 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:41:29,244 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:41:29,244 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l119yh3y
2025-10-19 19:41:29,244 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:41:29,246 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34371', name: 2, status: init, memory: 0, processing: 0>
2025-10-19 19:41:29,246 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34371
2025-10-19 19:41:29,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41772
2025-10-19 19:41:29,247 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:41:29,247 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:41:29,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:41:40,072 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:41:55,103 - distributed.utils_perf - INFO - full garbage collection released 11.66 MiB from 86 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:42:01,350 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:01,372 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:01,471 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:01,872 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:01,935 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:04,342 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.20 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:04,433 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.26 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:04,471 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:04,572 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:04,671 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:04,772 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 2.56 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:04,873 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.85 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:33,389 - distributed.utils_perf - INFO - full garbage collection released 321.28 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:42:36,485 - distributed.utils_perf - INFO - full garbage collection released 329.32 MiB from 26 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:42:44,080 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:44,229 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:44,329 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:48,480 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.18 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:48,844 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:48,845 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.84 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:49,693 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:50,023 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:50,024 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:54,505 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:54,539 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:54,640 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:54,739 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:54,840 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:54,940 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:57,348 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 3.27 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:57,370 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.24 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:57,441 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.33 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:57,474 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.37 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:57,494 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:57,522 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:33109 (pid=1499302) exceeded 95% memory budget. Restarting...
2025-10-19 19:42:57,574 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.44 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:42:57,619 - distributed.core - INFO - Connection to tcp://127.0.0.1:56794 has been closed.
2025-10-19 19:42:57,619 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33109', name: 6, status: paused, memory: 8, processing: 1>
2025-10-19 19:42:57,619 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33109
2025-10-19 19:42:57,620 - distributed.nanny - INFO - Worker process 1499302 was killed by signal 15
2025-10-19 19:42:57,629 - distributed.nanny - WARNING - Restarting worker
Created wide format: 126,826 station-year-measurement combinations × 368 columns
2025-10-19 19:42:57,671 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:32975 (pid=1503207) exceeded 95% memory budget. Restarting...
2025-10-19 19:42:57,760 - distributed.core - INFO - Connection to tcp://127.0.0.1:37476 has been closed.
2025-10-19 19:42:57,760 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32975', name: 0, status: paused, memory: 8, processing: 1>
2025-10-19 19:42:57,760 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32975
2025-10-19 19:42:57,761 - distributed.nanny - INFO - Worker process 1503207 was killed by signal 15
2025-10-19 19:42:57,766 - distributed.nanny - WARNING - Restarting worker
2025-10-19 19:42:58,285 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45089
2025-10-19 19:42:58,285 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45089
2025-10-19 19:42:58,285 - distributed.worker - INFO -           Worker name:                          6
2025-10-19 19:42:58,285 - distributed.worker - INFO -          dashboard at:             127.0.0.1:8791
2025-10-19 19:42:58,285 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:42:58,285 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:42:58,285 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:42:58,285 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:42:58,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m0azj_bg
2025-10-19 19:42:58,285 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:42:58,287 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45089', name: 6, status: init, memory: 0, processing: 0>
2025-10-19 19:42:58,288 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45089
2025-10-19 19:42:58,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47684
2025-10-19 19:42:58,288 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:42:58,288 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:42:58,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39143 instead
  warnings.warn(
2025-10-19 19:42:58,422 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38185
2025-10-19 19:42:58,422 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38185
2025-10-19 19:42:58,425 - distributed.worker - INFO -           Worker name:                          0
2025-10-19 19:42:58,425 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39143
2025-10-19 19:42:58,425 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:42:58,425 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:42:58,425 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:42:58,426 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:42:58,426 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vje9xcg8
2025-10-19 19:42:58,426 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:42:58,428 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38185', name: 0, status: init, memory: 0, processing: 0>
2025-10-19 19:42:58,428 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38185
2025-10-19 19:42:58,428 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47688
2025-10-19 19:42:58,428 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:42:58,428 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:42:58,429 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:43:12,845 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:13,112 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.29 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:13,127 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.32 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:13,227 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.41 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:13,274 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44099 (pid=1498713) exceeded 95% memory budget. Restarting...
2025-10-19 19:43:13,377 - distributed.core - INFO - Connection to tcp://127.0.0.1:38046 has been closed.
2025-10-19 19:43:13,377 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44099', name: 16, status: paused, memory: 8, processing: 1>
2025-10-19 19:43:13,377 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44099
2025-10-19 19:43:13,378 - distributed.nanny - INFO - Worker process 1498713 was killed by signal 15
2025-10-19 19:43:13,387 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45823 instead
  warnings.warn(
2025-10-19 19:43:14,058 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37619
2025-10-19 19:43:14,058 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37619
2025-10-19 19:43:14,058 - distributed.worker - INFO -           Worker name:                         16
2025-10-19 19:43:14,058 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45823
2025-10-19 19:43:14,058 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:43:14,058 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:43:14,058 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:43:14,058 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:43:14,058 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mjdx1bf6
2025-10-19 19:43:14,058 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:43:14,061 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37619', name: 16, status: init, memory: 0, processing: 0>
2025-10-19 19:43:14,061 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37619
2025-10-19 19:43:14,061 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58846
2025-10-19 19:43:14,061 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:43:14,061 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:43:14,062 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:43:16,147 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.23 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:16,671 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.53 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:16,709 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.54 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:16,724 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:39151 (pid=1504987) exceeded 95% memory budget. Restarting...
2025-10-19 19:43:16,946 - distributed.core - INFO - Connection to tcp://127.0.0.1:57018 has been closed.
2025-10-19 19:43:16,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39151', name: 15, status: paused, memory: 9, processing: 1>
2025-10-19 19:43:16,946 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39151
2025-10-19 19:43:16,950 - distributed.nanny - INFO - Worker process 1504987 was killed by signal 15
2025-10-19 19:43:16,968 - distributed.nanny - WARNING - Restarting worker
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37631 instead
  warnings.warn(
2025-10-19 19:43:18,282 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34069
2025-10-19 19:43:18,282 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34069
2025-10-19 19:43:18,282 - distributed.worker - INFO -           Worker name:                         15
2025-10-19 19:43:18,282 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37631
2025-10-19 19:43:18,282 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:43:18,282 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:43:18,282 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:43:18,282 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:43:18,282 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rvfb_7lt
2025-10-19 19:43:18,282 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:43:18,290 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34069', name: 15, status: init, memory: 0, processing: 0>
2025-10-19 19:43:18,291 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34069
2025-10-19 19:43:18,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58866
2025-10-19 19:43:18,292 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:43:18,292 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:43:18,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:43:37,290 - distributed.utils_perf - INFO - full garbage collection released 10.46 MiB from 71 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:43:41,077 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:41,111 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:41,212 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:41,310 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:41,412 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:41,511 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:41,611 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:41,712 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.64 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:45,808 - distributed.utils_perf - INFO - full garbage collection released 33.30 MiB from 63 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:43:48,191 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:51,425 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:51,632 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:51,671 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.40 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:51,772 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:51,772 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:51,871 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.85 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:52,624 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:52,721 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.26 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:43:52,832 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 2.27 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:44:23,026 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:44:24,839 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:44:49,265 - distributed.utils_perf - INFO - full garbage collection released 14.33 MiB from 232 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:44:51,758 - distributed.utils_perf - INFO - full garbage collection released 318.38 MiB from 18 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:44:57,083 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:45:01,430 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:01,785 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:01,807 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:01,907 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:02,014 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.50 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:02,206 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:11,641 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:11,710 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:11,811 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:11,912 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:11,939 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:12,012 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:12,039 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:12,112 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:12,140 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.74 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:12,211 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:12,296 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:12,920 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.57 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:13,073 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:13,173 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:16,513 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:16,824 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:42179 (pid=1501571) exceeded 95% memory budget. Restarting...
2025-10-19 19:45:16,919 - distributed.core - INFO - Connection to tcp://127.0.0.1:52952 has been closed.
2025-10-19 19:45:16,919 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42179', name: 3, status: running, memory: 9, processing: 1>
2025-10-19 19:45:16,919 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42179
2025-10-19 19:45:16,922 - distributed.nanny - INFO - Worker process 1501571 was killed by signal 15
2025-10-19 19:45:16,930 - distributed.nanny - WARNING - Restarting worker
Saved to data/weather_2022_wide.parquet

⏱️  Year 2022 processed in 222.6 seconds (3.7 minutes)

==================================================
Processing year 2023
==================================================
Loading weather data for year 2023...
Found 69 files for year 2023
Loaded data: 32,596,272 rows × 4 columns
Transforming to wide format...
2025-10-19 19:45:17,134 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:17,421 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:17,421 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44803 instead
  warnings.warn(
2025-10-19 19:45:17,612 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38647
2025-10-19 19:45:17,612 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38647
2025-10-19 19:45:17,612 - distributed.worker - INFO -           Worker name:                          3
2025-10-19 19:45:17,612 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44803
2025-10-19 19:45:17,612 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:45:17,612 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:45:17,612 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:45:17,612 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:45:17,612 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y91jn11p
2025-10-19 19:45:17,612 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:45:17,614 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38647', name: 3, status: init, memory: 0, processing: 0>
2025-10-19 19:45:17,614 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38647
2025-10-19 19:45:17,614 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38536
2025-10-19 19:45:17,614 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:45:17,614 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:45:17,615 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:45:25,518 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.76 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:27,290 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:27,533 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.46 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:27,566 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:27,678 - distributed.worker.memory - WARNING - Worker is at 68% memory usage. Resuming worker. Process memory: 2.55 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:27,767 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:56,019 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:56,039 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:45:59,566 - distributed.utils_perf - INFO - full garbage collection released 334.88 MiB from 32 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:46:06,246 - distributed.utils_perf - INFO - full garbage collection released 0.92 GiB from 19 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:46:12,348 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.34 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:12,793 - distributed.utils_perf - INFO - full garbage collection released 329.71 MiB from 38 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:46:12,794 - distributed.worker.memory - WARNING - gc.collect() took 1.311s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:46:13,418 - distributed.utils_perf - INFO - full garbage collection released 202.72 MiB from 33 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:46:13,419 - distributed.worker.memory - WARNING - gc.collect() took 1.699s. This is usually a sign that some tasks handle too many Python objects at the same time. Rechunking the work into smaller tasks might help.
2025-10-19 19:46:16,630 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:46:21,093 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:21,235 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.39 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:21,259 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.45 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:21,358 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.65 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:21,374 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:32989 (pid=1457368) exceeded 95% memory budget. Restarting...
2025-10-19 19:46:21,476 - distributed.nanny - INFO - Worker process 1457368 was killed by signal 15
2025-10-19 19:46:21,483 - distributed.core - INFO - Connection to tcp://127.0.0.1:58034 has been closed.
2025-10-19 19:46:21,483 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32989', name: 13, status: paused, memory: 8, processing: 1>
2025-10-19 19:46:21,483 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32989
2025-10-19 19:46:21,489 - distributed.nanny - WARNING - Restarting worker
Created wide format: 125,897 station-year-measurement combinations × 368 columns
2025-10-19 19:46:21,694 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:21,694 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.00 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:21,729 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.01 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:21,834 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 2.36 GiB -- Worker memory limit: 3.73 GiB
/home/yfreund/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8791 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45467 instead
  warnings.warn(
2025-10-19 19:46:22,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35847
2025-10-19 19:46:22,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35847
2025-10-19 19:46:22,163 - distributed.worker - INFO -           Worker name:                         13
2025-10-19 19:46:22,163 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45467
2025-10-19 19:46:22,163 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45371
2025-10-19 19:46:22,163 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:46:22,163 - distributed.worker - INFO -               Threads:                          1
2025-10-19 19:46:22,163 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-19 19:46:22,163 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-npa3p2bh
2025-10-19 19:46:22,163 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:46:22,166 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35847', name: 13, status: init, memory: 0, processing: 0>
2025-10-19 19:46:22,166 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35847
2025-10-19 19:46:22,166 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50872
2025-10-19 19:46:22,166 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45371
2025-10-19 19:46:22,166 - distributed.worker - INFO - -------------------------------------------------
2025-10-19 19:46:22,167 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45371
2025-10-19 19:46:28,765 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.83 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:28,771 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.83 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:30,874 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:31,224 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.42 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:31,271 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.49 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:31,371 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.52 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:46:31,472 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:12,223 - distributed.utils_perf - INFO - full garbage collection released 15.27 MiB from 135 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:47:43,295 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:43,379 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:43,481 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:43,581 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:43,681 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:43,780 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:43,879 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.67 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:43,981 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.72 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:45,542 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:45,566 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:45,666 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:45,766 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:45,867 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.71 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:46,189 - distributed.utils_perf - INFO - full garbage collection released 11.45 MiB from 123 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:47:47,748 - distributed.utils_perf - INFO - full garbage collection released 269.42 MiB from 84 reference cycles (threshold: 9.54 MiB)
2025-10-19 19:47:51,135 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:51,219 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.16 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:51,266 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:51,366 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.25 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:51,467 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.42 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:51,505 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:51,579 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.20 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:51,598 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.44 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:47:51,661 - distributed.worker.memory - WARNING - Worker is at 63% memory usage. Resuming worker. Process memory: 2.36 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:24,773 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-10-19 19:48:29,699 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:44,984 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.17 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:45,021 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:45,021 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.06 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:45,070 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.22 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:45,072 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.13 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:45,148 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.30 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:45,210 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.47 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:45,247 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.43 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:45,271 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:45,347 - distributed.worker.memory - WARNING - Worker is at 65% memory usage. Resuming worker. Process memory: 2.44 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:48:45,448 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.68 GiB -- Worker memory limit: 3.73 GiB
2025-10-19 19:49:16,892 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
Saved to data/weather_2023_wide.parquet

⏱️  Year 2023 processed in 158.8 seconds (2.6 minutes)

==================================================
Processing year 2024
==================================================
Loading weather data for year 2024...
Found 69 files for year 2024
Loaded data: 31,800,551 rows × 4 columns
Transforming to wide format...
Created wide format: 125,563 station-year-measurement combinations × 368 columns
Saved to data/weather_2024_wide.parquet

⏱️  Year 2024 processed in 134.1 seconds (2.2 minutes)

==================================================
Processing year 2025
==================================================
Loading weather data for year 2025...
Found 47 files for year 2025
Loaded data: 21,432,921 rows × 4 columns
Transforming to wide format...
Created wide format: 115,398 station-year-measurement combinations × 368 columns
Saved to data/weather_2025_wide.parquet

⏱️  Year 2025 processed in 74.6 seconds (1.2 minutes)

==================================================
Concatenating and sorting all files
==================================================
Concatenating 74 files...
Reading data/weather_1950_wide.parquet...
Reading data/weather_1951_wide.parquet...
Reading data/weather_1952_wide.parquet...
Reading data/weather_1953_wide.parquet...
Reading data/weather_1954_wide.parquet...
Reading data/weather_1955_wide.parquet...
Reading data/weather_1956_wide.parquet...
Reading data/weather_1957_wide.parquet...
Reading data/weather_1958_wide.parquet...
Reading data/weather_1959_wide.parquet...
Reading data/weather_1960_wide.parquet...
Reading data/weather_1961_wide.parquet...
Reading data/weather_1962_wide.parquet...
Reading data/weather_1963_wide.parquet...
Reading data/weather_1964_wide.parquet...
Reading data/weather_1965_wide.parquet...
Reading data/weather_1966_wide.parquet...
Reading data/weather_1967_wide.parquet...
Reading data/weather_1968_wide.parquet...
Reading data/weather_1969_wide.parquet...
Reading data/weather_1970_wide.parquet...
Reading data/weather_1971_wide.parquet...
Reading data/weather_1972_wide.parquet...
Reading data/weather_1973_wide.parquet...
Reading data/weather_1974_wide.parquet...
Reading data/weather_1975_wide.parquet...
Reading data/weather_1976_wide.parquet...
Reading data/weather_1977_wide.parquet...
Reading data/weather_1978_wide.parquet...
Reading data/weather_1979_wide.parquet...
Reading data/weather_1980_wide.parquet...
Reading data/weather_1981_wide.parquet...
Reading data/weather_1982_wide.parquet...
Reading data/weather_1983_wide.parquet...
Reading data/weather_1984_wide.parquet...
Reading data/weather_1985_wide.parquet...
Reading data/weather_1986_wide.parquet...
Reading data/weather_1987_wide.parquet...
Reading data/weather_1988_wide.parquet...
Reading data/weather_1989_wide.parquet...
Reading data/weather_1990_wide.parquet...
Reading data/weather_1991_wide.parquet...
Reading data/weather_1992_wide.parquet...
Reading data/weather_1993_wide.parquet...
Reading data/weather_1994_wide.parquet...
Reading data/weather_1995_wide.parquet...
Reading data/weather_1996_wide.parquet...
Reading data/weather_1997_wide.parquet...
Reading data/weather_1998_wide.parquet...
Reading data/weather_1999_wide.parquet...
Reading data/weather_2000_wide.parquet...
Reading data/weather_2001_wide.parquet...
Reading data/weather_2002_wide.parquet...
Reading data/weather_2003_wide.parquet...
Reading data/weather_2004_wide.parquet...
Reading data/weather_2005_wide.parquet...
Reading data/weather_2006_wide.parquet...
Reading data/weather_2008_wide.parquet...
Reading data/weather_2010_wide.parquet...
Reading data/weather_2011_wide.parquet...
Reading data/weather_2012_wide.parquet...
Reading data/weather_2013_wide.parquet...
Reading data/weather_2014_wide.parquet...
Reading data/weather_2015_wide.parquet...
Reading data/weather_2016_wide.parquet...
Reading data/weather_2017_wide.parquet...
Reading data/weather_2018_wide.parquet...
Reading data/weather_2019_wide.parquet...
Reading data/weather_2020_wide.parquet...
Reading data/weather_2021_wide.parquet...
Reading data/weather_2022_wide.parquet...
Reading data/weather_2023_wide.parquet...
Reading data/weather_2024_wide.parquet...
Reading data/weather_2025_wide.parquet...
Ensuring consistent column structure...
  Standardizing columns for DataFrame 1...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 2...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 3...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 4...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 5...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 6...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 7...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 8...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 9...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 10...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 11...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 12...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 13...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 14...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 15...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 16...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 17...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 18...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 19...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 20...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 21...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 22...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 23...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 24...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 25...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 26...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 27...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 28...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 29...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 30...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 31...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 32...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 33...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 34...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 35...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 36...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 37...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 38...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 39...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 40...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 41...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 42...
2025-10-19 19:50:09,370 - distributed.scheduler - INFO - Remove client Client-1413f9b9-ad31-11f0-bc67-5811224a71df
2025-10-19 19:50:09,370 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58074; closing.
2025-10-19 19:50:09,370 - distributed.scheduler - INFO - Remove client Client-1413f9b9-ad31-11f0-bc67-5811224a71df
2025-10-19 19:50:09,370 - distributed.scheduler - INFO - Close client connection: Client-1413f9b9-ad31-11f0-bc67-5811224a71df
2025-10-19 19:50:09,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46835'. Reason: nanny-close
2025-10-19 19:50:09,371 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33923'. Reason: nanny-close
2025-10-19 19:50:09,371 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42495'. Reason: nanny-close
2025-10-19 19:50:09,371 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38185. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42487'. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42093. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37553'. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34371. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43919'. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38647. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33941'. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39025'. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41291. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,372 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,372 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34667. Reason: nanny-close
2025-10-19 19:50:09,373 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35029'. Reason: nanny-close
2025-10-19 19:50:09,373 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,373 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45089. Reason: nanny-close
2025-10-19 19:50:09,373 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,373 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,373 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41407. Reason: nanny-close
2025-10-19 19:50:09,373 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35677'. Reason: nanny-close
2025-10-19 19:50:09,373 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,373 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,373 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38765'. Reason: nanny-close
2025-10-19 19:50:09,373 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,373 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,373 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39445. Reason: nanny-close
2025-10-19 19:50:09,374 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43159'. Reason: nanny-close
2025-10-19 19:50:09,374 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36653. Reason: nanny-close
2025-10-19 19:50:09,374 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,374 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,374 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,374 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37849. Reason: nanny-close
2025-10-19 19:50:09,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44605'. Reason: nanny-close
2025-10-19 19:50:09,374 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,374 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43641. Reason: nanny-close
2025-10-19 19:50:09,375 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37019'. Reason: nanny-close
2025-10-19 19:50:09,375 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,375 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,375 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,375 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36151. Reason: nanny-close
2025-10-19 19:50:09,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34679'. Reason: nanny-close
2025-10-19 19:50:09,375 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,375 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32833'. Reason: nanny-close
2025-10-19 19:50:09,376 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35847. Reason: nanny-close
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40519'. Reason: nanny-close
2025-10-19 19:50:09,376 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,376 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44469. Reason: nanny-close
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,376 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44899'. Reason: nanny-close
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36485'. Reason: nanny-close
2025-10-19 19:50:09,376 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34069. Reason: nanny-close
2025-10-19 19:50:09,376 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43475'. Reason: nanny-close
2025-10-19 19:50:09,377 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2025-10-19 19:50:09,377 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46681. Reason: nanny-close
2025-10-19 19:50:09,377 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37619. Reason: nanny-close
2025-10-19 19:50:09,377 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44061. Reason: nanny-close
2025-10-19 19:50:09,377 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46619. Reason: nanny-close
2025-10-19 19:50:09,377 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,378 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,378 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,378 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,378 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,379 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,379 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,379 - distributed.core - INFO - Connection to tcp://127.0.0.1:45371 has been closed.
2025-10-19 19:50:09,380 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,380 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,381 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,381 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47688; closing.
2025-10-19 19:50:09,381 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54394; closing.
2025-10-19 19:50:09,381 - distributed.nanny - INFO - Worker closed
2025-10-19 19:50:09,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41772; closing.
2025-10-19 19:50:09,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38536; closing.
2025-10-19 19:50:09,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49494; closing.
2025-10-19 19:50:09,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35736; closing.
2025-10-19 19:50:09,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46606; closing.
2025-10-19 19:50:09,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47684; closing.
2025-10-19 19:50:09,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49906; closing.
2025-10-19 19:50:09,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59784; closing.
2025-10-19 19:50:09,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36192; closing.
2025-10-19 19:50:09,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42428; closing.
2025-10-19 19:50:09,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51432; closing.
2025-10-19 19:50:09,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50872; closing.
2025-10-19 19:50:09,384 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38185', name: 0, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,384 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38185
2025-10-19 19:50:09,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42093', name: 1, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,385 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42093
2025-10-19 19:50:09,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34371', name: 2, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,385 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34371
2025-10-19 19:50:09,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38647', name: 3, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,385 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38647
2025-10-19 19:50:09,386 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34667', name: 5, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,386 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34667
2025-10-19 19:50:09,386 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41291', name: 4, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,386 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41291
2025-10-19 19:50:09,386 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41407', name: 7, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,386 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41407
2025-10-19 19:50:09,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45089', name: 6, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,387 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45089
2025-10-19 19:50:09,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39445', name: 8, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,387 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39445
2025-10-19 19:50:09,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37849', name: 10, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,387 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37849
2025-10-19 19:50:09,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36653', name: 9, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,388 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36653
2025-10-19 19:50:09,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43641', name: 11, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,388 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43641
2025-10-19 19:50:09,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36151', name: 12, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,388 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36151
2025-10-19 19:50:09,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35847', name: 13, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,389 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35847
2025-10-19 19:50:09,389 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45220; closing.
2025-10-19 19:50:09,389 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58866; closing.
2025-10-19 19:50:09,389 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58846; closing.
2025-10-19 19:50:09,389 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49902; closing.
2025-10-19 19:50:09,390 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57908; closing.
2025-10-19 19:50:09,390 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49772; closing.
2025-10-19 19:50:09,391 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44469', name: 14, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,391 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44469
2025-10-19 19:50:09,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34069', name: 15, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,392 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34069
2025-10-19 19:50:09,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37619', name: 16, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,392 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37619
2025-10-19 19:50:09,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46681', name: 17, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,392 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46681
2025-10-19 19:50:09,392 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46619', name: 19, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,392 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46619
2025-10-19 19:50:09,393 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44061', name: 18, status: closing, memory: 0, processing: 0>
2025-10-19 19:50:09,393 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44061
2025-10-19 19:50:09,393 - distributed.scheduler - INFO - Lost all workers
2025-10-19 19:50:10,751 - distributed.scheduler - INFO - Scheduler closing...
2025-10-19 19:50:10,752 - distributed.scheduler - INFO - Scheduler closing all comms
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 43...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 44...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 45...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 46...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 47...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 48...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 49...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 50...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 51...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 52...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 53...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 54...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 55...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 56...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 57...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 58...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 59...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 60...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 61...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 62...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 63...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 64...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 65...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 66...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 67...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 68...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 69...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 70...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 71...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 72...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 73...
    ✓ Columns already match expected structure
  Standardizing columns for DataFrame 74...
    ✓ Columns already match expected structure
Concatenating standardized DataFrames...
Sorting by year, measurement, then station...
Error concatenating files: Dataframes only support sorting by named columns which must be passed as a string or a list of strings; multi-partition dataframes only support sorting by a single column.
You passed ['year', 'ELEMENT', 'ID']
⏱️  Failed after 4.6 seconds (0.1 minutes)
Closing Dask cluster...
✓ Cluster closed successfully

==================================================
PROCESSING COMPLETE
==================================================
Processed years: 1950 to 2025
Individual files saved in: data
Completed at: 2025-10-19 19:50:10
⏱️  Total processing time: 9139.4 seconds (152.3 minutes)
⏱️  Average time per year: 120.3 seconds (2.0 minutes)
