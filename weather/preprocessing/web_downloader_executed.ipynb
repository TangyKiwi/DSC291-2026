{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web File Downloader\n",
    "\n",
    "This notebook provides utilities for downloading files from various web sources including:\n",
    "- AWS S3 buckets (including public datasets like NOAA GHCN)\n",
    "- HTTP/HTTPS URLs\n",
    "- Multiple files in parallel\n",
    "\n",
    "## Features:\n",
    "1. S3 file downloads with progress tracking\n",
    "2. HTTP download with resume support\n",
    "3. Parallel downloads using Dask\n",
    "4. File integrity verification (checksums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:36.519195Z",
     "iopub.status.busy": "2025-11-03T23:10:36.518880Z",
     "iopub.status.idle": "2025-11-03T23:10:37.006017Z",
     "shell.execute_reply": "2025-11-03T23:10:37.005653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "print(\"Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:37.020799Z",
     "iopub.status.busy": "2025-11-03T23:10:37.020675Z",
     "iopub.status.idle": "2025-11-03T23:10:37.023721Z",
     "shell.execute_reply": "2025-11-03T23:10:37.023424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory: ../weather_data/downloads\n",
      "Configuration set up\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DOWNLOAD_DIR = '../weather_data/downloads'\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Download directory: {DOWNLOAD_DIR}\")\n",
    "\n",
    "# Progress tracking\n",
    "class ProgressTracker:\n",
    "    def __init__(self, total_files: int):\n",
    "        self.total_files = total_files\n",
    "        self.completed = 0\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def update(self):\n",
    "        self.completed += 1\n",
    "        elapsed = time.time() - self.start_time\n",
    "        rate = self.completed / elapsed if elapsed > 0 else 0\n",
    "        remaining = (self.total_files - self.completed) / rate if rate > 0 else 0\n",
    "        print(f\"Progress: {self.completed}/{self.total_files} files \"\n",
    "              f\"({100*self.completed/self.total_files:.1f}%) \"\n",
    "              f\"Rate: {rate:.2f} files/sec \"\n",
    "              f\"ETA: {remaining:.1f}s\")\n",
    "    \n",
    "    def finish(self):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"\\\\n✓ Completed {self.total_files} files in {elapsed:.2f} seconds \"\n",
    "              f\"({self.total_files/elapsed:.2f} files/sec)\")\n",
    "\n",
    "print(\"Configuration set up\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:37.024474Z",
     "iopub.status.busy": "2025-11-03T23:10:37.024383Z",
     "iopub.status.idle": "2025-11-03T23:10:37.027016Z",
     "shell.execute_reply": "2025-11-03T23:10:37.026738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP download function defined\n"
     ]
    }
   ],
   "source": [
    "# Function: Download single HTTP file with resume support\n",
    "def download_http_file(url: str, output_path: str, resume: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Download a file from HTTP/HTTPS URL with resume support.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to download from\n",
    "        output_path: Local path to save file\n",
    "        resume: Whether to resume interrupted downloads\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if partial download exists\n",
    "        if resume and os.path.exists(output_path):\n",
    "            file_size = os.path.getsize(output_path)\n",
    "            headers = {'Range': f'bytes={file_size}-'}\n",
    "            resume_download = True\n",
    "        else:\n",
    "            headers = {}\n",
    "            resume_download = False\n",
    "        \n",
    "        # Download file\n",
    "        response = requests.get(url, headers=headers, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Determine mode (append if resuming, write if new)\n",
    "        mode = 'ab' if resume_download else 'wb'\n",
    "        \n",
    "        with open(output_path, mode) as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"HTTP download function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:37.027730Z",
     "iopub.status.busy": "2025-11-03T23:10:37.027638Z",
     "iopub.status.idle": "2025-11-03T23:10:37.029775Z",
     "shell.execute_reply": "2025-11-03T23:10:37.029491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 download function defined\n"
     ]
    }
   ],
   "source": [
    "# Function: Download single S3 file\n",
    "def download_s3_file(s3_path: str, output_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Download a file from S3.\n",
    "    \n",
    "    Args:\n",
    "        s3_path: S3 path (e.g., 's3://bucket/path/file.parquet')\n",
    "        output_path: Local path to save file\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Setup S3 filesystem\n",
    "        s3 = s3fs.S3FileSystem(anon=True)\n",
    "        \n",
    "        # Download file\n",
    "        s3.get(s3_path, output_path)\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {s3_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"S3 download function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:37.030484Z",
     "iopub.status.busy": "2025-11-03T23:10:37.030392Z",
     "iopub.status.idle": "2025-11-03T23:10:37.032995Z",
     "shell.execute_reply": "2025-11-03T23:10:37.032721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel download function defined\n"
     ]
    }
   ],
   "source": [
    "# Function: Download multiple files in parallel\n",
    "def download_files_parallel(\n",
    "    file_list: List[Tuple[str, str]], \n",
    "    max_workers: int = 8,\n",
    "    download_func=None\n",
    ") -> List[bool]:\n",
    "    \"\"\"\n",
    "    Download multiple files in parallel.\n",
    "    \n",
    "    Args:\n",
    "        file_list: List of tuples (source_path, destination_path)\n",
    "        max_workers: Maximum number of parallel workers\n",
    "        download_func: Function to use for downloading (None = auto-detect)\n",
    "        \n",
    "    Returns:\n",
    "        List of success flags\n",
    "    \"\"\"\n",
    "    results = [False] * len(file_list)\n",
    "    tracker = ProgressTracker(len(file_list))\n",
    "    \n",
    "    # Auto-detect download function if not provided\n",
    "    if download_func is None:\n",
    "        if file_list and file_list[0][0].startswith('s3://'):\n",
    "            download_func = download_s3_file\n",
    "        else:\n",
    "            download_func = download_http_file\n",
    "    \n",
    "    # Download in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {\n",
    "            executor.submit(download_func, source, dest): i\n",
    "            for i, (source, dest) in enumerate(file_list)\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            results[idx] = future.result()\n",
    "            tracker.update()\n",
    "    \n",
    "    tracker.finish()\n",
    "    return results\n",
    "\n",
    "print(\"Parallel download function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:37.033715Z",
     "iopub.status.busy": "2025-11-03T23:10:37.033627Z",
     "iopub.status.idle": "2025-11-03T23:10:37.614745Z",
     "shell.execute_reply": "2025-11-03T23:10:37.613578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Download NOAA GHCN weather files\n",
      "\\nDiscovering available files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 264 years of data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nFiles to download: 9\n",
      "\\nFirst few files:\n",
      "  1. a43d416f337746388bc65511b780bc6d_0.snappy.parquet -> a43d416f337746388bc65511b780bc6d_0.snappy.parquet\n",
      "  2. a43d416f337746388bc65511b780bc6d_1.snappy.parquet -> a43d416f337746388bc65511b780bc6d_1.snappy.parquet\n",
      "  3. a43d416f337746388bc65511b780bc6d_10.snappy.parquet -> a43d416f337746388bc65511b780bc6d_10.snappy.parquet\n",
      "  4. a43d416f337746388bc65511b780bc6d_0.snappy.parquet -> a43d416f337746388bc65511b780bc6d_0.snappy.parquet\n",
      "  5. a43d416f337746388bc65511b780bc6d_1.snappy.parquet -> a43d416f337746388bc65511b780bc6d_1.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Download files from NOAA GHCN S3 bucket\n",
    "print(\"Example 1: Download NOAA GHCN weather files\")\n",
    "\n",
    "# Setup S3 filesystem\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "bucket = 's3://noaa-ghcn-pds'\n",
    "\n",
    "# List available years and elements\n",
    "print(\"\\\\nDiscovering available files...\")\n",
    "years = s3.ls(f'{bucket}/parquet/by_year/')\n",
    "print(f\"Found {len(years)} years of data\")\n",
    "\n",
    "# Select a specific year and elements to download\n",
    "target_year = 2020\n",
    "elements = ['PRCP', 'TMAX', 'TMIN']\n",
    "\n",
    "# Build list of files to download\n",
    "files_to_download = []\n",
    "for element in elements:\n",
    "    element_path = f'{bucket}/parquet/by_year/YEAR={target_year}/ELEMENT={element}/'\n",
    "    files = s3.glob(f'{element_path}*.parquet')\n",
    "    files_to_download.extend([\n",
    "        (f, f'{DOWNLOAD_DIR}/{target_year}/{element}/{os.path.basename(f)}')\n",
    "        for f in files[:3]  # Limit to first 3 files per element\n",
    "    ])\n",
    "\n",
    "print(f\"\\\\nFiles to download: {len(files_to_download)}\")\n",
    "print(\"\\\\nFirst few files:\")\n",
    "for i, (src, dst) in enumerate(files_to_download[:5]):\n",
    "    print(f\"  {i+1}. {os.path.basename(src)} -> {os.path.basename(dst)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:37.617880Z",
     "iopub.status.busy": "2025-11-03T23:10:37.617511Z",
     "iopub.status.idle": "2025-11-03T23:10:39.538073Z",
     "shell.execute_reply": "2025-11-03T23:10:39.536798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nStarting downloads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1/9 files (11.1%) Rate: 0.96 files/sec ETA: 8.3s\n",
      "Progress: 2/9 files (22.2%) Rate: 1.75 files/sec ETA: 4.0s\n",
      "Progress: 3/9 files (33.3%) Rate: 2.57 files/sec ETA: 2.3s\n",
      "Progress: 4/9 files (44.4%) Rate: 3.36 files/sec ETA: 1.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 5/9 files (55.6%) Rate: 3.37 files/sec ETA: 1.2s\n",
      "Progress: 6/9 files (66.7%) Rate: 3.95 files/sec ETA: 0.8s\n",
      "Progress: 7/9 files (77.8%) Rate: 4.58 files/sec ETA: 0.4s\n",
      "Progress: 8/9 files (88.9%) Rate: 4.89 files/sec ETA: 0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 9/9 files (100.0%) Rate: 4.70 files/sec ETA: 0.0s\n",
      "\\n✓ Completed 9 files in 1.91 seconds (4.70 files/sec)\n",
      "\\nDownload Summary:\n",
      "  Successful: 9\n",
      "  Failed: 0\n",
      "  Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Execute Example 1 download\n",
    "print(\"\\\\nStarting downloads...\")\n",
    "results = download_files_parallel(files_to_download, max_workers=4, download_func=download_s3_file)\n",
    "\n",
    "# Summary\n",
    "successful = sum(results)\n",
    "failed = len(results) - successful\n",
    "print(f\"\\\\nDownload Summary:\")\n",
    "print(f\"  Successful: {successful}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Success rate: {100*successful/len(results):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:39.541455Z",
     "iopub.status.busy": "2025-11-03T23:10:39.541076Z",
     "iopub.status.idle": "2025-11-03T23:10:39.546877Z",
     "shell.execute_reply": "2025-11-03T23:10:39.545640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2: Download from HTTP URLs\n",
      "\\nNote: These are placeholder URLs. Replace with actual download URLs.\n",
      "Prepared 2 URLs for download\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Download HTTP files\n",
    "print(\"Example 2: Download from HTTP URLs\")\n",
    "\n",
    "# Example URLs (replace with actual URLs you want to download)\n",
    "http_urls = [\n",
    "    \"https://www.example.com/file1.csv\",\n",
    "    \"https://www.example.com/file2.csv\",\n",
    "]\n",
    "\n",
    "print(\"\\\\nNote: These are placeholder URLs. Replace with actual download URLs.\")\n",
    "print(f\"Prepared {len(http_urls)} URLs for download\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:39.549934Z",
     "iopub.status.busy": "2025-11-03T23:10:39.549559Z",
     "iopub.status.idle": "2025-11-03T23:10:39.557124Z",
     "shell.execute_reply": "2025-11-03T23:10:39.556833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3: Download specific dataset subset\n",
      "Dataset subset download function defined\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Download entire dataset from S3 with filtering\n",
    "print(\"Example 3: Download specific dataset subset\")\n",
    "\n",
    "def download_dataset_subset(\n",
    "    bucket: str,\n",
    "    year: int,\n",
    "    elements: List[str],\n",
    "    output_dir: str,\n",
    "    max_files_per_element: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Download a subset of files from S3 dataset.\n",
    "    \n",
    "    Args:\n",
    "        bucket: S3 bucket path\n",
    "        year: Year to download\n",
    "        elements: List of elements to download\n",
    "        output_dir: Local output directory\n",
    "        max_files_per_element: Maximum files per element (None = all)\n",
    "    \"\"\"\n",
    "    s3 = s3fs.S3FileSystem(anon=True)\n",
    "    files_to_download = []\n",
    "    \n",
    "    for element in elements:\n",
    "        element_path = f'{bucket}/parquet/by_year/YEAR={year}/ELEMENT={element}/'\n",
    "        files = s3.glob(f'{element_path}*.parquet')\n",
    "        \n",
    "        if max_files_per_element:\n",
    "            files = files[:max_files_per_element]\n",
    "        \n",
    "        files_to_download.extend([\n",
    "            (f, f'{output_dir}/{year}/{element}/{os.path.basename(f)}')\n",
    "            for f in files\n",
    "        ])\n",
    "    \n",
    "    print(f\"\\\\nDownloading {len(files_to_download)} files...\")\n",
    "    results = download_files_parallel(files_to_download, max_workers=8, download_func=download_s3_file)\n",
    "    \n",
    "    successful = sum(results)\n",
    "    print(f\"\\\\n✓ Successfully downloaded {successful}/{len(files_to_download)} files\")\n",
    "    return successful\n",
    "\n",
    "print(\"Dataset subset download function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:39.557870Z",
     "iopub.status.busy": "2025-11-03T23:10:39.557775Z",
     "iopub.status.idle": "2025-11-03T23:10:39.564200Z",
     "shell.execute_reply": "2025-11-03T23:10:39.563362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4: File integrity verification\n",
      "File verification functions defined\n"
     ]
    }
   ],
   "source": [
    "# Example 4: File verification and checksum\n",
    "print(\"Example 4: File integrity verification\")\n",
    "\n",
    "def calculate_checksum(filepath: str, algorithm: str = 'md5') -> str:\n",
    "    \"\"\"\n",
    "    Calculate checksum of a file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to file\n",
    "        algorithm: Hash algorithm (md5, sha1, sha256)\n",
    "        \n",
    "    Returns:\n",
    "        Hex digest of file\n",
    "    \"\"\"\n",
    "    hash_algo = hashlib.new(algorithm)\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_algo.update(chunk)\n",
    "    \n",
    "    return hash_algo.hexdigest()\n",
    "\n",
    "def verify_file_integrity(filepath: str, expected_hash: str, algorithm: str = 'md5') -> bool:\n",
    "    \"\"\"\n",
    "    Verify file integrity against expected checksum.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to file\n",
    "        expected_hash: Expected hash value\n",
    "        algorithm: Hash algorithm\n",
    "        \n",
    "    Returns:\n",
    "        True if checksums match, False otherwise\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return False\n",
    "    \n",
    "    actual_hash = calculate_checksum(filepath, algorithm)\n",
    "    match = actual_hash.lower() == expected_hash.lower()\n",
    "    \n",
    "    if match:\n",
    "        print(f\"✓ {os.path.basename(filepath)}: Integrity verified\")\n",
    "    else:\n",
    "        print(f\"✗ {os.path.basename(filepath)}: Checksum mismatch!\")\n",
    "        print(f\"  Expected: {expected_hash}\")\n",
    "        print(f\"  Actual:   {actual_hash}\")\n",
    "    \n",
    "    return match\n",
    "\n",
    "print(\"File verification functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete web file download solution:\n",
    "\n",
    "1. **HTTP Downloads** - Resume support, chunk streaming\n",
    "2. **S3 Downloads** - Public bucket access, batch processing\n",
    "3. **Parallel Downloads** - Multi-threaded concurrent downloads\n",
    "4. **Progress Tracking** - Real-time progress with ETA\n",
    "5. **File Verification** - Checksum validation for integrity\n",
    "6. **Dataset Tools** - High-level functions for subsetting datasets\n",
    "\n",
    "### Key Features:\n",
    "- Resume interrupted downloads\n",
    "- Progress tracking with ETA\n",
    "- Parallel downloads for speed\n",
    "- Auto-detection of S3 vs HTTP\n",
    "- File integrity verification\n",
    "\n",
    "### Example Dataset Sources:\n",
    "- NOAA GHCN: `s3://noaa-ghcn-pds`\n",
    "- Any HTTP/HTTPS URL\n",
    "- Public S3 buckets\n",
    "\n",
    "Replace example URLs and file paths with your actual download targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:39.566597Z",
     "iopub.status.busy": "2025-11-03T23:10:39.566310Z",
     "iopub.status.idle": "2025-11-03T23:10:39.966042Z",
     "shell.execute_reply": "2025-11-03T23:10:39.964831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download Caltrans PeMS data...\n",
      "URL: https://pems.dot.ca.gov/?download=403199&dnode=Clearinghouse\n",
      "Output: ../weather_data/downloads/caltrans_pems_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully downloaded to ../weather_data/downloads/caltrans_pems_data.csv\n",
      "  File size: 22,336 bytes\n"
     ]
    }
   ],
   "source": [
    "# Download PeMS data with authentication\n",
    "# NOTE: This requires valid PeMS credentials\n",
    "\n",
    "pems_url = \"https://pems.dot.ca.gov/?download=403199&dnode=Clearinghouse\"\n",
    "output_file = f\"{DOWNLOAD_DIR}/caltrans_pems_data.csv\"\n",
    "\n",
    "print(\"Attempting to download Caltrans PeMS data...\")\n",
    "print(f\"URL: {pems_url}\")\n",
    "print(f\"Output: {output_file}\")\n",
    "\n",
    "# Option 1: Using requests with session (if you have credentials)\n",
    "try:\n",
    "    # Create a session to maintain cookies\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # If you have credentials, login first:\n",
    "    # login_url = \"https://pems.dot.ca.gov/?dnode=login\"\n",
    "    # login_data = {\"username\": \"YOUR_USERNAME\", \"password\": \"YOUR_PASSWORD\"}\n",
    "    # session.post(login_url, data=login_data)\n",
    "    \n",
    "    # Then download the file\n",
    "    response = session.get(pems_url, stream=True, timeout=60)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(output_file, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"✓ Successfully downloaded to {output_file}\")\n",
    "        print(f\"  File size: {os.path.getsize(output_file):,} bytes\")\n",
    "    else:\n",
    "        print(f\"✗ Download failed with status code: {response.status_code}\")\n",
    "        print(\"  This usually means authentication is required\")\n",
    "        print(f\"  Response: {response.text[:200]}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    print(\"\\\\nThis download requires PeMS authentication.\")\n",
    "    print(\"Please:\")\n",
    "    print(\"1. Register at https://pems.dot.ca.gov/\")\n",
    "    print(\"2. Get approved (1-2 business days)\")\n",
    "    print(\"3. Add your credentials to the code above\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:39.968958Z",
     "iopub.status.busy": "2025-11-03T23:10:39.968588Z",
     "iopub.status.idle": "2025-11-03T23:10:39.974428Z",
     "shell.execute_reply": "2025-11-03T23:10:39.973343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MANUAL DOWNLOAD INSTRUCTIONS\n",
      "============================================================\n",
      "\n",
      "Since PeMS requires authentication, here are alternative approaches:\n",
      "\n",
      "1. **Browser Download:**\n",
      "   - Log in to https://pems.dot.ca.gov/\n",
      "   - Navigate to the data clearinghouse\n",
      "   - Use browser developer tools (F12) to find the direct download URL\n",
      "   - Use the authentication cookies in a download script\n",
      "\n",
      "2. **Use PeMS API (if available):**\n",
      "   - Check if PeMS provides an API for automated access\n",
      "   - Some agencies provide API access for research/academic use\n",
      "\n",
      "3. **Contact PeMS Support:**\n",
      "   - Request bulk data download access\n",
      "   - They may provide alternative data access methods\n",
      "\n",
      "4. **Alternative Data Sources:**\n",
      "   - Check if similar traffic data is available in public repositories\n",
      "   - Look for academic datasets with PeMS derivatives\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Manual download instructions\n",
    "print(\"=\"*60)\n",
    "print(\"MANUAL DOWNLOAD INSTRUCTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Since PeMS requires authentication, here are alternative approaches:\n",
    "\n",
    "1. **Browser Download:**\n",
    "   - Log in to https://pems.dot.ca.gov/\n",
    "   - Navigate to the data clearinghouse\n",
    "   - Use browser developer tools (F12) to find the direct download URL\n",
    "   - Use the authentication cookies in a download script\n",
    "\n",
    "2. **Use PeMS API (if available):**\n",
    "   - Check if PeMS provides an API for automated access\n",
    "   - Some agencies provide API access for research/academic use\n",
    "\n",
    "3. **Contact PeMS Support:**\n",
    "   - Request bulk data download access\n",
    "   - They may provide alternative data access methods\n",
    "\n",
    "4. **Alternative Data Sources:**\n",
    "   - Check if similar traffic data is available in public repositories\n",
    "   - Look for academic datasets with PeMS derivatives\n",
    "\"\"\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Samples from All Repositories\n",
    "\n",
    "Attempting to download a sample file from each data repository listed in repositories.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:10:39.977373Z",
     "iopub.status.busy": "2025-11-03T23:10:39.976898Z",
     "iopub.status.idle": "2025-11-03T23:11:06.060943Z",
     "shell.execute_reply": "2025-11-03T23:11:06.059526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up download tasks for each repository...\\n\n",
      "2. Chicago Crimes: Open Data Portal\n",
      "   URL: https://data.cityofchicago.org/api/views/xguy-4ndq/rows.csv?accessType=DOWNLOAD\n"
     ]
    }
   ],
   "source": [
    "# Download samples from each repository\n",
    "repo_downloads = []\n",
    "\n",
    "print(\"Setting up download tasks for each repository...\\\\n\")\n",
    "\n",
    "# 2. Chicago Crimes (Open Data Portal)\n",
    "print(\"2. Chicago Crimes: Open Data Portal\")\n",
    "try:\n",
    "    # Chicago Open Data has an API endpoint\n",
    "    chicago_url = \"https://data.cityofchicago.org/api/views/xguy-4ndq/rows.csv?accessType=DOWNLOAD\"\n",
    "    chicago_output = f\"{DOWNLOAD_DIR}/chicago_crimes_2023_sample.csv\"\n",
    "    \n",
    "    print(f\"   URL: {chicago_url}\")\n",
    "    result = download_http_file(chicago_url, chicago_output, resume=True)\n",
    "    repo_downloads.append((\"Chicago Crimes\", \"http\", result))\n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Failed: {e}\")\n",
    "    repo_downloads.append((\"Chicago Crimes\", \"http\", False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:11:06.064254Z",
     "iopub.status.busy": "2025-11-03T23:11:06.063881Z",
     "iopub.status.idle": "2025-11-03T23:11:06.070288Z",
     "shell.execute_reply": "2025-11-03T23:11:06.068920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n3. US Accidents Dataset (Kaggle)\n",
      "   Note: Requires Kaggle API credentials\n",
      "   Data available at: https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents\n",
      "   Status: Requires manual setup\n"
     ]
    }
   ],
   "source": [
    "# 3. US Accidents Dataset\n",
    "print(\"\\\\n3. US Accidents Dataset (Kaggle)\")\n",
    "print(\"   Note: Requires Kaggle API credentials\")\n",
    "print(\"   Data available at: https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents\")\n",
    "us_accidents_info = {\n",
    "    \"name\": \"US Accidents\",\n",
    "    \"type\": \"kaggle\",\n",
    "    \"url\": \"https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents\",\n",
    "    \"note\": \"Requires Kaggle API setup\"\n",
    "}\n",
    "repo_downloads.append((\"US Accidents\", \"kaggle\", False))\n",
    "print(\"   Status: Requires manual setup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:11:06.073193Z",
     "iopub.status.busy": "2025-11-03T23:11:06.072830Z",
     "iopub.status.idle": "2025-11-03T23:11:06.222171Z",
     "shell.execute_reply": "2025-11-03T23:11:06.221037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n4. DOE OEDI Data Lake (AWS Open Data)\n",
      "   Exploring: s3://oedi-data-lake\n",
      "   Found directories: 10\n",
      "   No parquet files found in top-level directories\n"
     ]
    }
   ],
   "source": [
    "# 4. DOE OEDI Data Lake (S3)\n",
    "print(\"\\\\n4. DOE OEDI Data Lake (AWS Open Data)\")\n",
    "try:\n",
    "    s3 = s3fs.S3FileSystem(anon=True)\n",
    "    oedi_bucket = 's3://oedi-data-lake'\n",
    "    \n",
    "    # List available resources\n",
    "    print(f\"   Exploring: {oedi_bucket}\")\n",
    "    resources = s3.ls(f'{oedi_bucket}/', detail=False)[:10]\n",
    "    print(f\"   Found directories: {len(resources)}\")\n",
    "    \n",
    "    # Try to find a sample file\n",
    "    sample_files = []\n",
    "    for resource in resources:\n",
    "        if resource.endswith('/'):\n",
    "            files = s3.glob(f'{resource}*.parquet')\n",
    "            if files:\n",
    "                sample_files.append(files[0])\n",
    "                break\n",
    "    \n",
    "    if sample_files:\n",
    "        print(f\"   Found sample: {sample_files[0]}\")\n",
    "        oedi_output = f\"{DOWNLOAD_DIR}/doe_oedi_sample.parquet\"\n",
    "        result = download_s3_file(sample_files[0], oedi_output)\n",
    "        repo_downloads.append((\"DOE OEDI\", \"s3_public\", result))\n",
    "    else:\n",
    "        print(\"   No parquet files found in top-level directories\")\n",
    "        repo_downloads.append((\"DOE OEDI\", \"s3_public\", False))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Failed: {e}\")\n",
    "    repo_downloads.append((\"DOE OEDI\", \"s3_public\", False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:11:06.225160Z",
     "iopub.status.busy": "2025-11-03T23:11:06.224785Z",
     "iopub.status.idle": "2025-11-03T23:11:06.462618Z",
     "shell.execute_reply": "2025-11-03T23:11:06.461544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n5. Aurora Multi-Sensor Dataset (AWS Open Data)\n",
      "   Exploring: s3://aurora-opendata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✗ Failed: The specified bucket does not exist\n"
     ]
    }
   ],
   "source": [
    "# 5. Aurora Multi-Sensor Dataset (AWS Open Data)\n",
    "print(\"\\\\n5. Aurora Multi-Sensor Dataset (AWS Open Data)\")\n",
    "try:\n",
    "    s3 = s3fs.S3FileSystem(anon=True)\n",
    "    aurora_bucket = 's3://aurora-opendata'\n",
    "    \n",
    "    print(f\"   Exploring: {aurora_bucket}\")\n",
    "    resources = s3.ls(f'{aurora_bucket}/', detail=False)[:10]\n",
    "    print(f\"   Found directories: {len(resources)}\")\n",
    "    \n",
    "    # Try to find a sample file\n",
    "    sample_files = []\n",
    "    for resource in resources:\n",
    "        if resource.endswith('/'):\n",
    "            # Check for common data formats\n",
    "            for ext in ['.parquet', '.csv', '.json']:\n",
    "                files = s3.glob(f'{resource}*{ext}')\n",
    "                if files:\n",
    "                    sample_files.append(files[0])\n",
    "                    break\n",
    "            if sample_files:\n",
    "                break\n",
    "    \n",
    "    if sample_files:\n",
    "        print(f\"   Found sample: {sample_files[0]}\")\n",
    "        aurora_output = f\"{DOWNLOAD_DIR}/aurora_msds_sample.{sample_files[0].split('.')[-1]}\"\n",
    "        result = download_s3_file(sample_files[0], aurora_output)\n",
    "        repo_downloads.append((\"Aurora MSDS\", \"s3_public\", result))\n",
    "    else:\n",
    "        print(\"   No data files found in top-level directories\")\n",
    "        repo_downloads.append((\"Aurora MSDS\", \"s3_public\", False))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Failed: {e}\")\n",
    "    repo_downloads.append((\"Aurora MSDS\", \"s3_public\", False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:11:06.465610Z",
     "iopub.status.busy": "2025-11-03T23:11:06.465266Z",
     "iopub.status.idle": "2025-11-03T23:11:06.594995Z",
     "shell.execute_reply": "2025-11-03T23:11:06.593926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n6. Marine Energy Data Lake (AWS Open Data)\n",
      "   Exploring: s3://marine-energy-data\n",
      "   Found directories: 3\n",
      "   No data files found\n"
     ]
    }
   ],
   "source": [
    "# 6. Marine Energy Data Lake (AWS Open Data)\n",
    "print(\"\\\\n6. Marine Energy Data Lake (AWS Open Data)\")\n",
    "try:\n",
    "    s3 = s3fs.S3FileSystem(anon=True)\n",
    "    marine_bucket = 's3://marine-energy-data'\n",
    "    \n",
    "    print(f\"   Exploring: {marine_bucket}\")\n",
    "    resources = s3.ls(f'{marine_bucket}/', detail=False)[:10]\n",
    "    print(f\"   Found directories: {len(resources)}\")\n",
    "    \n",
    "    # Try to find a sample file\n",
    "    sample_files = []\n",
    "    for resource in resources:\n",
    "        if resource.endswith('/'):\n",
    "            for ext in ['.parquet', '.csv', '.json', '.h5', '.nc']:\n",
    "                files = s3.glob(f'{resource}*{ext}')\n",
    "                if files:\n",
    "                    sample_files.append(files[0])\n",
    "                    break\n",
    "            if sample_files:\n",
    "                break\n",
    "    \n",
    "    if sample_files:\n",
    "        print(f\"   Found sample: {sample_files[0]}\")\n",
    "        marine_output = f\"{DOWNLOAD_DIR}/marine_energy_sample.{sample_files[0].split('.')[-1]}\"\n",
    "        result = download_s3_file(sample_files[0], marine_output)\n",
    "        repo_downloads.append((\"Marine Energy\", \"s3_public\", result))\n",
    "    else:\n",
    "        print(\"   No data files found\")\n",
    "        repo_downloads.append((\"Marine Energy\", \"s3_public\", False))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Failed: {e}\")\n",
    "    repo_downloads.append((\"Marine Energy\", \"s3_public\", False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:11:06.597874Z",
     "iopub.status.busy": "2025-11-03T23:11:06.597527Z",
     "iopub.status.idle": "2025-11-03T23:11:06.604384Z",
     "shell.execute_reply": "2025-11-03T23:11:06.603281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n7-11. Additional Repositories\n",
      "============================================================\n",
      "   NCVS Crime Victimization (archive): Requires ICPSR registration\n",
      "   Caltrans PeMS (web_login): Requires PeMS registration\n",
      "   End-Use Load Profiles (openEI): Complex data portal structure\n",
      "   openDD Roundabout (academic): Academic dataset portal\n",
      "   Zenseact ZOD (registration): Requires registration form\n",
      "   VED Dataset (github): GitHub repository\n",
      "   comma2k19 (github): GitHub with large files\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 7-11. Other repositories (require special access or have complex URLs)\n",
    "print(\"\\\\n7-11. Additional Repositories\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "additional_repos = [\n",
    "    (\"NCVS Crime Victimization\", \"archive\", \"Requires ICPSR registration\"),\n",
    "    (\"Caltrans PeMS\", \"web_login\", \"Requires PeMS registration\"),\n",
    "    (\"End-Use Load Profiles\", \"openEI\", \"Complex data portal structure\"),\n",
    "    (\"openDD Roundabout\", \"academic\", \"Academic dataset portal\"),\n",
    "    (\"Zenseact ZOD\", \"registration\", \"Requires registration form\"),\n",
    "    (\"VED Dataset\", \"github\", \"GitHub repository\"),\n",
    "    (\"comma2k19\", \"github\", \"GitHub with large files\"),\n",
    "]\n",
    "\n",
    "for name, repo_type, note in additional_repos:\n",
    "    print(f\"   {name} ({repo_type}): {note}\")\n",
    "    repo_downloads.append((name, repo_type, False))\n",
    "\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:11:06.607232Z",
     "iopub.status.busy": "2025-11-03T23:11:06.606887Z",
     "iopub.status.idle": "2025-11-03T23:11:06.615445Z",
     "shell.execute_reply": "2025-11-03T23:11:06.614399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "DOWNLOAD SUMMARY\n",
      "============================================================\n",
      "\\nTotal repositories attempted: 12\n",
      "✓ Successful downloads: 1\n",
      "✗ Failed/Not attempted: 11\n",
      "\\n✓ Successfully Downloaded:\n",
      "   - Chicago Crimes (http)\n",
      "\\n✗ Issues/Requirements:\n",
      "   - US Accidents (kaggle)\n",
      "   - DOE OEDI (s3_public)\n",
      "   - Aurora MSDS (s3_public)\n",
      "   - Marine Energy (s3_public)\n",
      "   - NCVS Crime Victimization (archive)\n",
      "   - Caltrans PeMS (web_login)\n",
      "   - End-Use Load Profiles (openEI)\n",
      "   - openDD Roundabout (academic)\n",
      "   - Zenseact ZOD (registration)\n",
      "   - VED Dataset (github)\n",
      "   - comma2k19 (github)\n",
      "\\n============================================================\n",
      "Note: Some repositories require registration, API keys, or special access.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary of download attempts\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"DOWNLOAD SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count by status\n",
    "successful = [r for r in repo_downloads if r[2]]\n",
    "failed = [r for r in repo_downloads if not r[2]]\n",
    "\n",
    "print(f\"\\\\nTotal repositories attempted: {len(repo_downloads)}\")\n",
    "print(f\"✓ Successful downloads: {len(successful)}\")\n",
    "print(f\"✗ Failed/Not attempted: {len(failed)}\")\n",
    "\n",
    "if successful:\n",
    "    print(\"\\\\n✓ Successfully Downloaded:\")\n",
    "    for name, repo_type, status in successful:\n",
    "        print(f\"   - {name} ({repo_type})\")\n",
    "\n",
    "print(\"\\\\n✗ Issues/Requirements:\")\n",
    "for name, repo_type, status in failed:\n",
    "    print(f\"   - {name} ({repo_type})\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"Note: Some repositories require registration, API keys, or special access.\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
